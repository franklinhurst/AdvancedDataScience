{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "collapsed": true
            },
            "source": "### Extract Transform Load (ETL)\n\nETL is one of the first things which needs to be done in a data science project. The nature of this task highly depends on the type of data source. Whether it is relational or unstructured, enterprise data or internet data, persistent data or streaming data. This heavily influences the choice of architecture. Therefore, you must document your choice and thinking process in the Architectural Decision Document (ADD).\n\nThis task involves \u2013 as the name implies \u2013 accessing the data source, transforming it in a way it can be easily worked with and finally make it available to downstream analytics processes \u2013 either real-time streaming or batch ones.\n\nIn case of operational relational data, de-normalization usually needs to take place, for unstructured data, some feature extraction might already be appropriate and for real-time data, windows are usually created.\n\nPlease create an ETL process, document it and save this deliverable according to the naming convention of the process model."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "!pip install tensorflow\n!pip install python-mnist\n!pip install Pillow\n!pip install pyspark"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "%matplotlib inline\nimport matplotlib.pyplot as plt\nfrom matplotlib import image\nimport tensorflow as tf\nimport seaborn as sns\nfrom mnist import MNIST\nimport numpy as np\nimport PIL\nfrom PIL import Image\nimport os\nimport matplotlib.image as mping\nfrom pyspark import SparkContext, SparkConf\nfrom pyspark.sql import SparkSession\nfrom pyspark.ml.feature import StringIndexer\nfrom numpy import asarray\nimport pandas as pd"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "tf.__version__"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "PIL.__version__"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## First dataset\n\n#### 60,000 images and 10 classes, each image is 28 x 28 or represented by a 794 element array"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# need to download and gunzip each of the train and test images and labels\n\n!wget http://codh.rois.ac.jp/kmnist/dataset/kmnist/train-images-idx3-ubyte.gz?raw=True\n!mv train-images-idx3-ubyte.gz?raw=True train-images-idx3-ubyte.gz\n!gunzip train-images-idx3-ubyte.gz\n!ls -lahr train-images-idx3-ubyte\n\n!wget http://codh.rois.ac.jp/kmnist/dataset/kmnist/train-labels-idx1-ubyte.gz?raw=True\n!mv train-labels-idx1-ubyte.gz?raw=True train-labels-idx1-ubyte.gz\n!gunzip train-labels-idx1-ubyte.gz\n!ls -lahr train-labels-idx1-ubyte\n\n!wget http://codh.rois.ac.jp/kmnist/dataset/kmnist/t10k-images-idx3-ubyte.gz?raw=True\n!mv t10k-images-idx3-ubyte.gz?raw=True t10k-images-idx3-ubyte.gz\n!gunzip t10k-images-idx3-ubyte.gz\n!ls -lahr t10k-images-idx3-ubyte\n\n!wget http://codh.rois.ac.jp/kmnist/dataset/kmnist/t10k-labels-idx1-ubyte.gz?raw=True\n!mv t10k-labels-idx1-ubyte.gz?raw=True t10k-labels-idx1-ubyte.gz\n!gunzip t10k-labels-idx1-ubyte.gz\n!ls -lahr t10k-labels-idx1-ubyte"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# download the classmap for the first dataset, add the phonetics, and display it\n# the classmap shows the classes that each written kuzushiji can be classified into\n# and we add the phonetic sound that the characters makes when spoken\n\nurl = \"http://codh.rois.ac.jp/kmnist/dataset/kmnist/kmnist_classmap.csv\"\ndf_classmap = pd.read_csv(url)\nphonetic = ['o','ki','su','tsu','na','ha','ma','ya','re','wo']\ndf_classmap['phonetic'] = phonetic\ndf_classmap"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# create a directory and move these files into it\n# then show what is in the directory\n\n!mkdir kmnistdata\n!cp t10k-images-idx3-ubyte kmnistdata/t10k-images-idx3-ubyte\n!cp t10k-labels-idx1-ubyte kmnistdata/t10k-labels-idx1-ubyte\n!cp train-images-idx3-ubyte kmnistdata/train-images-idx3-ubyte\n!cp train-labels-idx1-ubyte kmnistdata/train-labels-idx1-ubyte\n!ls -al kmnistdata"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# load the data into arrays\n\ndata = MNIST('kmnistdata')\ntrain_images, train_labels = data.load_training()\ntest_images, test_labels = data.load_testing()"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "print(train_labels[0])\nprint(train_images[0])"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "type(train_labels)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "type(train_images)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# the data needs to be transformed into numpy arrays for use  in the model\n\ntrain_images = np.array(train_images)\ntrain_labels = np.array(train_labels)\ntest_images = np.array(test_images)\ntest_labels = np.array(test_labels)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "type(train_images[0])"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "type(train_images)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# the data as downloaded has two dimensions\n# there are 60000 images, each image is represented\n# in one dimension as 784 numbers\n\n# (see the notebook kuzushiji.data_exp for displayed images)\n\ntrain_images.shape"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# the train_images are now a numpy array of numbers between 0 and 255\nprint(train_images[0])"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# we need to normalize the numpy arrays so that each number in the numpy\n# array is between 0 and 1:\n\ntrain_images = train_images / 255\ntest_images = test_images / 255"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "train_images[0]"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Second dataset\n\n#### 232,365 images, 49 classes, each image is 28 x 28 or represented by a 794 element array"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# download the train and test labels and images, \n# which are in .npz format \n\n!wget http://codh.rois.ac.jp/kmnist/dataset/k49/k49-train-imgs.npz?raw=True\n!mv k49-train-imgs.npz?raw=True k49-train-imgs.npz\n!ls -lahr k49-train-imgs.npz\n\n!wget http://codh.rois.ac.jp/kmnist/dataset/k49/k49-train-labels.npz?raw=True\n!mv k49-train-labels.npz?raw=True k49-train-labels.npz\n!ls -lahr k49-train-labels.npz\n\n!wget http://codh.rois.ac.jp/kmnist/dataset/k49/k49-test-imgs.npz?raw=True\n!mv k49-test-imgs.npz?raw=True k49-test-imgs.npz\n!ls -lahr k49-test-imgs.npz\n\n!wget http://codh.rois.ac.jp/kmnist/dataset/k49/k49-test-labels.npz?raw=True\n!mv k49-test-labels.npz?raw=True k49-test-labels.npz\n!ls -lahr k49-test-labels.npz"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# we need to define a load function in order to extract the data\n# from the .npz format, and then use  it to extract:\n\ndef load(f):\n    return np.load(f)['arr_0']\n\nk49_train_images = load('k49-train-imgs.npz')\nk49_train_labels = load('k49-train-labels.npz')\nk49_test_images = load('k49-test-imgs.npz')\nk49_test_labels = load('k49-test-labels.npz')"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# next we download the classmap for this dataset\n# and add the phonetic, and display it:\n\nurl = \"http://codh.rois.ac.jp/kmnist/dataset/k49/k49_classmap.csv\"\ndf_k49_classmap = pd.read_csv(url)\n\nk49_phonetic = ['a', 'i',  'u',  'e',  'o',\\\n                'ka','ki', 'ku', 'ke', 'ko',\\\n                'sa','shi','su', 'se', 'so',\\\n                'ta','chi','tsu','te', 'to',\\\n                'na','ni', 'nu', 'ne', 'no',\\\n                'ha','hi', 'fu', 'he', 'ho',\\\n                'ma','mi', 'mu', 'me', 'mo',\\\n                'ya','yu', 'yo',\\\n                'ra','ri', 'ru', 're', 'ro',\\\n                'wa','wi', 'we ','wo', 'n','iteration_mark']\nprint(len(k49_phonetic))\ndf_k49_classmap['phonetic'] = k49_phonetic\ndf_k49_classmap"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# the data is already a numpy array\n# since we pulled it out of the .nps formatted structure\n\ntype(k49_train_images)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# need to check the dimensionality of the data:\n\nk49_train_images.shape"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "k49_train_labels.shape"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "k49_test_images.shape"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# this means we need to flatten the data\n# to two dimensions\n\nk49_train_images = np.reshape(k49_train_images, (232365, 784))\nk49_test_images = np.reshape(k49_test_images, (38547, 784))"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "k49_train_images.shape"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "k49_test_images.shape"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# the k49_train_images are now a numpy array of numbers between 0 and 255\nprint(k49_train_images[0])"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# we need to normalize the numpy arrays so that each number in the numpy\n# array is between 0 and 1:\n\nk49_train_images = k49_train_images / 255\nk49_test_images = k49_test_images / 255"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "print(k49_train_images[0])"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# this is the format we need for the machine learning model"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## The Third Dataset \n\n#### includes Kanji characters, this data has 3832 different classes and consists of 140,426 images, each image is 64 X 64 pixels\n#### This dataset is not as processed as the other two.  It is just a bunch of png images in a directory inside an archive file."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# we download the archive:\n\n!wget http://codh.rois.ac.jp/kmnist/dataset/kkanji/kkanji.tar?raw=True\n!mv kkanji.tar?raw=True kkanji.tar\n!ls -lahr kkanji.tar\n\n# list the contents of the archive\n# limit output to the first 70 files\n\n!tar -tf kkanji.tar | head -70"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# extract the archive:\n!tar -xf kkanji.tar\n\n# check whats in the current working directory:\n!ls -al\n\n# check the extracted png files in the newly extracted directory\n# List out the first 50 folder names in the directory kkanji2\n# the folder names are also the codepoint of each of the characters:\n\n!ls kkanji2 | head -50"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# put the codepoints in a file\n# let the first line of the file be name\n# of the column in the dataframe we are creating\n!echo codepoint > codepoints.csv\n!ls kkanji2 >> codepoints.csv\n!cat codepoints.csv | head -50"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# make sure the file codepoints.csv is in the current directory:\n\n!ls -al"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# verify the file's path:\n\nprint (os.path.abspath(\"codepoints.csv\"))"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# read all the lines of the file into pandas dataframe\n# including the column header which is already in the file\n# display new dataframe\n# here we can confirm that the data has 3832 classes\n\ndf_kanji_classmap = pd.read_csv(\"codepoints.csv\")\ndf_kanji_classmap"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# we'll need to read the images in to transform them\n#read the first listed image in the first folder and display it\nimg = mping.imread('kkanji2/U+5B87/72d56fcb33d10fe0.png')\nplt.imshow(img)\nplt.show()\n# note that this image in only \"first\" as listed from the tar arcive above\n# it is not \"first\" in terms of the codepoint listing we created directly above"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# create a pandas dataframe that contains the codepoint for each image, \n# and its full path in the os and display that dataframe\n\ndata = []\ndir = os.path.realpath('/home/wsuser/work/kkanji2')\nfor r, d, f in os.walk(dir):\n    for file in f:\n        if \".png\" in file:\n            data.append((r.split('/')[-1],os.path.join(r,file)))\ndf_kanji2 = pd.DataFrame(data, columns=['codepoint', 'image_file_path']).sort_values(by=['codepoint'], ignore_index = True)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# display the new dataframe\n# we will change the display width of the pandas \n# dataframe to be able to see the entire path\n\npd.set_option('max_colwidth', 1000)\ndf_kanji2"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# show the os path of the first image in the first folder\n# as listed in the dataframe\n# note that the dataframe list the images grouped by their codepoints\n# and the codepoints are sorted by their ASCII values\n\ndf_kanji2['image_file_path'][0]"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# read the image using Pillow and show some information about it:\n\npimage = Image.open(df_kanji2['image_file_path'][0])\nprint(pimage.format)\nprint(pimage.size)\nprint(pimage.mode)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# read the image using matplotlib and show it\nimg = image.imread(df_kanji2['image_file_path'][0])\nprint(img.dtype)\nplt.imshow(img)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# convert to a numpy array and verify we created a numpy array and it shape\nimg_nparray = asarray(img)\nprint(type(img_nparray))\nprint(img_nparray.shape)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# show the values of the array\n# notice the array is already normalized\nimg_nparray"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "img_nparray.shape"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# add a blank column to the dataframe with column name 'np_array'\n\ndf_kanji2['np_array'] = \"\"\ndf_kanji2"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "type(df_kanji2['np_array'])\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "type(df_kanji2['np_array'][0])"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "df_kanji2['np_array'].shape"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "df_kanji2['np_array'][0].shape"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# add a column containing and numpy array of the image indicated in the path in image_file_path\n# and at the same time, flatten each image from a 64 x 64 numpy array to a single\n# dimension 4096 element long numpy array\ndf_kanji2['np_array'] = df_kanji2['image_file_path'].apply(lambda x: np.asarray(Image.open(x))).apply(lambda y: np.reshape(y,(4096,)))\n\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "df_kanji2"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "type(df_kanji2['np_array'])"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "type(df_kanji2['np_array'][0])"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "df_kanji2['np_array'].shape"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "df_kanji2['np_array'][0].shape"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# let's normalize the numpy arrays:\ndf_kanji2['np_array'] = df_kanji2['np_array'] / 255"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": ""
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.7",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.7.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}