{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "collapsed": true
            },
            "source": "### Keras\n\nKeras is a popular high level deep learning library and framework written in Python.  We can quickly prototype deep learning models using the Keras API.  Keras has been open source since 2015.  It's documentation can be found at:\n\nhttps://keras.io/\n\nSource code can be found at:\n\ngithub.com/fchollet/keras\n\nKeras can also be seen as a debugging tool.  Why is it such a good debugging tool?\n\nThere is a very robust community around Keras.  It is a very popular libarary and has an active community, that might be able to help you with any questions that may come up.\n\ngroups.google.com/forum/#forum/keras-users\n\nkeras-slack-autojoin.herokuapp.com\n\nAlso, Keras has an intuitive, high-level API, leads to fast prototyping.\n\nIt has modular building blocks.  Easy to build new customer layers.\n\nExtensions:\n\ngithub.com/fchollet/keras-resources\n\ngithub.com/fchollet/keras/tree/master/examples\n\nModels:\n\ngithub.com/fchollet/deep-learning-models\n\nDatasets:\n\ngithub.com/fchollet/keras/tree/master/keras/datasets\n\nYou can view Keras as a deep learning front end, and you can use other backends.  Keras provides a high level entry point, but in the back are different engines that do the heavy lifting.\n\nOne choice is google tensorflow.  There is also theano and Microsoft CNTK.  We exclusively use Tensorflow here.\n\nYou can easily swap backends, and depending on your configuration, Keras runs seemlessly on CPU's and GPU's.\n\nLayers are the core abstraction for Keras.\n\nSequential layers have:\n\ninput\noutput\ninput_shape, and\noutput_shape\n\nCan get the weights as a list of numpy arrays:\n\nlayer.get_weights()\n\nCan set layer weights with:\n\nlayer.set_weights(weights)\n\nEach layer has a defining configuration:\n\nlayer.get_config()\n\nFirst, instantiate a sequential model.  Then, add layers to it.  Next compile the model with a mandatory loss function, a mandatory optimizer, and optional evaluation metrics.\n\nNext we use data to fit the model.\n\nNext we evaluate the model, persist or deploy the model, or start a new experiment, etc., depending on what you want to do at that point.\n\nThere are two options in specifying the loss function in the step of compiling the model:\n\n1.  Import from loss module (preferred), which looks something like this:\n\nfrom keras.losses import mean_squared_error\nmodel.compile(loss=mean_squared_error, optimizer=...)\n\n    (See the code example notebooks, since the import statement may be different based on the versions of tensorflow, etc) \n\n2.  Use strings, which looks like:\n\nmodel.compile(loss='mean_squared_error', optimizer=...)\n\nThe string approach is error-prone.\n\nTo define an optimizer, you have two different ways of doing this:\n\n1.  Load optimizir from a module (preferred):\n\nInstantiate an SGD object, an optimizer, which also lets you set some parameters:\n\nsgd = SGD(lr=0.01,          # need to set the learning rate >= 0\n          decay=1e-6,       # learning rate decay after updates\n          momentum=0.9)     # this is the momentum parameter used for the SGD optimizer\nThen:\n\nmodel.compile(loss=..., optimizer=sgd)\n\n2.  Again, can also pass a string for the optimizer value:\n\nmodel.compile(loss=..., optimizer='sgd')\n\nAgain, the second approach is more error-prone.  If you just pass a string, then the default optimizer parameters will be used.\n\nOnce you are done with compiling your model, you fit it.\nYou must specify the batch size, and the number of epochs you\nwant to train, and optionally you can specify the validation data:\n\nmodel.fit(x_train, y_train,\n          batch_size=32,\n          epochs=10,\n          validation_data=(x_val, y_val))\n\nThen evaluate the model on test data:\nevaluate(x_test, y_test, batch_size=32)\n\nThen predict on new data:\n\npredict(x_test, batch_size=32)\n\n\nMulti-layer perceptrons or MLP's can make up what's called densely connected networks.\n\nThis involves stacking dense layers on top of each other with activations.\n\nRegularization is achieved using dropout, and we can build Keras dropout layers.\n\nTo initialize a dense layer, we need to do a few things:\n\nfrom keras.layers import Dense\n\nDense(units,                    # Number of output neurons\n      activation=None,          # activation function my name: sigmoid, or whichever\n      use_bias=True,            # use a bias term or not: best not to change this\n      kernel_initalizer='glorot_uniform'  # leave these alone\n      bias_initializer='zeros')           # leave these alone\n      \nDropout layers are much easier to specify:     \n\nfrom keras.layers import Dropout\n\nDropout(rate,            # Fraction of units to drop in each forward pass: a value between 0 and 1\n        seed=None)       # Random seed for reproducibility\n        \n        \nfrom keras.datasets import mnist\nfrom keras.utils import to_categorical\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout\n\nbatch_size = 128\nnum_classes  = 10  # the number of classes is the number of categories\nepochs = 20  # train the network for 20 epochs in total\n\nWe use the mnist dataset of handwriten digits.  The mnist datasets consist of 60,000 train samples and 10,000 smepls for test.  Each individual sample is a 28 by 28 image which has handwriten digits on it.  The labels are just encoded as the actual digits 0 to 9.\n\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\n\n\nNext is data preprocessing:\nThe mnist images are 28 x 28 images and we need to flatten them to \ninstead be 784 long vectors, so this is how we do that:\n\nx_train = x_train.reshape(60000, 784)\nx_test = x_test.reshape(10000, 784)\n\nNext we make sure that they are of type float:\n\nx_train = x_train.astype('float32')\nx_test = x_test.astype('float32')\n\nAnd then divide each of them by 255, in order to make them\nnormalized, i.e. each will be a fraction of 1, between 0 and 1:\n\nx_train /= 255\ny_test /= 255\n\nAs the last step in preprocessing, we are one-hot encoding the labels of\nthe data and the labels are in the y_train and y_test.  We supply both\nthe data to be transformed and the number of categories:\n\ny_train = to_categorical(y_train, num_classes)\ny_test = to_categorical(y_test, num_classes\n\nNext, we actually start to put together the model:\n\nmodel = Sequential()\n\nThe first layer.  In the first layer, we also need to specify the input shape.\nThis is one of the images transformed as we did above.  It is 784 units long.\nShapes in layers after the first layer are inferred.\nIn the last layer we specify that number of output classes amd also the \nactivation function is softmas which is appropriate for classification.\n\n\nmodel.add(Dense(512, activation='relu', input_shape=(784,)))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(512, activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(num_classes, activation='softmax'))\n\n\nmodel.summary()\n\nThe model will be compiled with a loss function of categorical crossentropy\nand the optimizer will be stochastic gradient descent.  Our metric will be\naccuracy:\n\nmodel.compile(loss='categorical_crossentrophy',\n              optimizer='sgd',\n              metrics=['accuracy'])\n\nNext we fit our model with the training data:\n\nmodel.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data(x_test, y_test))\n\nAnd then we evaluate:\n\nscore = model.evaluate(x_test, y_test, verbose=0)\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])\n\nThe model.evalute gives back both test loss and accuracy, and you should get about 98% accuracy in this model.\n\n[end]"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": ""
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.7",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.7.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}