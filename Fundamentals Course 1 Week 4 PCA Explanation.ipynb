{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": "# Principal Component Analysis"
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": "# with PCA you take an n-dimensional, Euclidian vector space R^n\n# where each row can be seen as a point in that space and each\n# column is one coordinate in that space and transforms that dataset\n# by specifying the number of desired dimensions n such a way that the new\n# dataset represents the higher dimension points in fewer dimensions.\n# This process is also called projection.  The key property of this \n# transformation is that the proportionality of distances between points \n# is preserved.\n\n# This means that if you take the distance between two random points Pa and Pb\n# in the original data set and you divide it by the distance between two \n# other random points, Pc and Pd in the original data set, then you get the same\n# value as when you divided the distance between Pa and Pb in the new data set by\n# the distance between Pc and Pd in the new data set.\n\n# After the dataset has been transformed, the new points still explain the majority \n# of the variation in the dataset.  In other words the principal components are chosen\n# such that one gets rid of highly correlated dimensions in the source dataset since \n# the information content of such dimensions is low.\n\n# The new dimensions of hte data are cloned out of the dimentions in the data that\n# have hte lowest correlation with each other, these are called the principal\n# components.\n\n# The remaining components are orthagonal to each other."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# if k = 3 then plottable and distance ratios are preserved\n\n# there is always information loss when transforming from an n dimensional\n# vector space to a k dimensional vector space\n# however, PCA is smart in that it tries to minimize the information loss"
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.7",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.7.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}