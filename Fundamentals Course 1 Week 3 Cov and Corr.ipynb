{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Waiting for a Spark session to start...\nSpark Initialization Done! ApplicationId = app-20210430223106-0001\nKERNEL_ID = 943638e3-2ed5-4573-89c2-4a0dad989851\n"
                }
            ],
            "source": "# RDD's are the central first class citizen data structure in ApacheSpark\n# so using the RDD API is crucial to working with Apache Spark\n\n# RDD = Resilient Distributed Datasets\n# Thus RDD's are going to be the first class citizen data structure for \n# use when using Apache Spark for distributed data science processing\n# operations\n\n# RDD's are schemaless and lazy, which means essentially\n# that they only organize themselves in response to a specific operation\n# or query.  They do not have an inherent scheme \n\n# If we want to treat data more like a relational database then instead\n# of dealing with RDD's directly, we will want to use ApacheSparkSQL and\n# or the *Dataframe* API.  When the dataframe API and or ApacheSparkSQL is\n# used instead of dealing with the RDD's then the dataframe structure acts \n# as a structure built around the RDD.  This notebook only deals with RDD's \n# directly.  The dataframe we are talking about here should not be confused\n# a pandas dataframe."
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": "# generate two identical RDD's containing numbers running from  0 to 99: \n\nrddX = sc.parallelize(list(range(100)))\nrddY = sc.parallelize(list(range(100)))"
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": "# get the mean (average) of each:\n\nmeanX = rddX.sum()/rddX.count()\nmeanY = rddY.sum()/rddY.count()"
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "49.5\n49.5\n"
                }
            ],
            "source": "# print the mean of each\n\nprint(meanX)\nprint(meanY)"
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "[(0, 0),\n (1, 1),\n (2, 2),\n (3, 3),\n (4, 4),\n (5, 5),\n (6, 6),\n (7, 7),\n (8, 8),\n (9, 9)]"
                    },
                    "execution_count": 5,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "# We want to use these two rdd's at the same time, so\n# we can \"zip\" them together into a new RDD which combines the two\n# this is essentially an RDD where the elements are each tuples\n# show the first 10 tuples in the newly created RDD\n\nrddXY = rddX.zip(rddY)\nrddXY.take(10)"
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": "# This creates and RDD with tuple values made up of the values of each of the two RDD's above.\n# This kind of resembles the behavior of a table in a relational database (with the limitations)\n# on RDD's mentioned above\n\n# We can subtract the mean from each value of X and each value of Y\n# and then take the sum.\n# This gives us the Covariance of the two datasets X and Y:"
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "833.25"
                    },
                    "execution_count": 7,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "covXY = rddXY.map(lambda x__y : (x__y[0]-meanX)*(x__y[1]-meanY) ).sum() / rddXY.count()\ncovXY"
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "28.86607004772212\n28.86607004772212\n"
                }
            ],
            "source": "# Next, we need to get the standard deviation of each dataset\n# for that we use the number of elements in the zipped together\n# series (of tuples)\n\nfrom math import sqrt\nn = rddXY.count()\nsdX = sqrt(rddX.map(lambda x : pow(x-meanX,2)).sum()/n)\nsdY = sqrt(rddY.map(lambda y : pow(y-meanY,2)).sum()/n)\nprint(sdX)\nprint(sdY)"
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "1.0"
                    },
                    "execution_count": 9,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "# The standard deviation of the two is the same, since they are identical\n\n# Next we calculate the Pearson's correlation between the two datasets\n\n\ncorrXY = covXY / (sdX * sdY)\ncorrXY"
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [],
            "source": "# since these two sets of observations are identical, they have a \n# corr = +B1.0"
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "49.5\n49.5\n"
                }
            ],
            "source": "# here we reverse the second list\n# and keep everything else the same\n\nrddX = sc.parallelize(list(range(100)))\nrddY = sc.parallelize(reversed(list(range(100))))\n\nmeanX = rddX.sum()/rddX.count()\nmeanY = rddY.sum()/rddY.count()\n\nprint(meanX)\nprint(meanY)"
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "-1.0"
                    },
                    "execution_count": 12,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "# We want to use these two rdd's at the same time, so\n# we can \"zip\" them together\n\nrddXY = rddX.zip(rddY)\n\ncovXY = rddXY.map(lambda x__y : (x__y[0]-meanX)*(x__y[1]-meanY) ).sum() / rddXY.count()\n\nsdX = sqrt(rddX.map(lambda x : pow(x-meanX,2)).sum()/n)\nsdY = sqrt(rddY.map(lambda y : pow(y-meanY,2)).sum()/n)\n\ncorrXY = covXY / (sdX * sdY)\ncorrXY"
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [],
            "source": "# since one list is reversed, then the two lists\n# are perfectly negatively correlated"
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "-0.131989198919892"
                    },
                    "execution_count": 14,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "# next, we use two random lists:\n\nimport random\n\nrddX = sc.parallelize(list(random.sample(range(100),100)))\nrddY = sc.parallelize(list(random.sample(range(100),100)))\n\nmeanX = rddX.sum()/rddX.count()\nmeanY = rddY.sum()/rddY.count()\n\nrddXY = rddX.zip(rddY)\n\ncovXY = rddXY.map(lambda x__y : (x__y[0]-meanX)*(x__y[1]-meanY) ).sum() / rddXY.count()\n\nsdX = sqrt(rddX.map(lambda x : pow(x-meanX,2)).sum()/n)\nsdY = sqrt(rddY.map(lambda y : pow(y-meanY,2)).sum()/n)\n\ncorrXY = covXY / (sdX * sdY)\ncorrXY"
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [],
            "source": "# the correlation is close to zero\n# since the two datasets were randomly generated\n# there is little or no correlation between the two datasets"
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "-0.12519651965196518"
                    },
                    "execution_count": 16,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "# again, randomly:\n\nrddX = sc.parallelize(list(random.sample(range(100),100)))\nrddY = sc.parallelize(list(random.sample(range(100),100)))\n\nmeanX = rddX.sum()/rddX.count()\nmeanY = rddY.sum()/rddY.count()\n\nrddXY = rddX.zip(rddY)\n\ncovXY = rddXY.map(lambda x__y : (x__y[0]-meanX)*(x__y[1]-meanY) ).sum() / rddXY.count()\n\nsdX = sqrt(rddX.map(lambda x : pow(x-meanX,2)).sum()/n)\nsdY = sqrt(rddY.map(lambda y : pow(y-meanY,2)).sum()/n)\n\ncorrXY = covXY / (sdX * sdY)\ncorrXY"
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {},
            "outputs": [],
            "source": "# the correlation is still close to zero"
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {},
            "outputs": [],
            "source": "# next we will consider a correlation matrix\n# we create a number of rdd's in apache spark: column 1, etc."
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "metadata": {},
            "outputs": [],
            "source": "# Here, we can use mllib, which is from the distributed\n# machine learning library of apache spark\n# we are also using the RDD API at the same time\n# could just use the code above to generate every correlation\n# value in the matrix, but this is easier\n\nfrom pyspark.mllib.stat import Statistics\n\ncolumn1 = sc.parallelize(list(range(100)))\ncolumn2 = sc.parallelize(list(range(100,200)))\ncolumn3 = sc.parallelize(list(reversed(range(100))))\ncolumn4 = sc.parallelize(list(random.sample(range(100),100)))"
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "metadata": {},
            "outputs": [],
            "source": "# Above we have generated four different RDD's ,but the \n# library we have imported expects a more relational database\n# table-like structure\n# Therefore we use the zip function from the RDD API to\n# put them together\n# If we zip column1 and column2 together:"
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "[(0, 100),\n (1, 101),\n (2, 102),\n (3, 103),\n (4, 104),\n (5, 105),\n (6, 106),\n (7, 107),\n (8, 108),\n (9, 109),\n (10, 110),\n (11, 111),\n (12, 112),\n (13, 113),\n (14, 114)]"
                    },
                    "execution_count": 21,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "data1 = column1.zip(column2)\ndata1.take(15)"
        },
        {
            "cell_type": "code",
            "execution_count": 22,
            "metadata": {},
            "outputs": [],
            "source": "# if we kind of \"stack\" the zipping together, this happens:"
        },
        {
            "cell_type": "code",
            "execution_count": 23,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "[((0, 100), 99),\n ((1, 101), 98),\n ((2, 102), 97),\n ((3, 103), 96),\n ((4, 104), 95),\n ((5, 105), 94),\n ((6, 106), 93),\n ((7, 107), 92),\n ((8, 108), 91),\n ((9, 109), 90),\n ((10, 110), 89),\n ((11, 111), 88),\n ((12, 112), 87),\n ((13, 113), 86),\n ((14, 114), 85)]"
                    },
                    "execution_count": 23,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "data2 = column1.zip(column2).zip(column3)\ndata2.take(15)"
        },
        {
            "cell_type": "code",
            "execution_count": 24,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "[(((0, 100), 99), 63),\n (((1, 101), 98), 62),\n (((2, 102), 97), 51),\n (((3, 103), 96), 0),\n (((4, 104), 95), 94),\n (((5, 105), 94), 4),\n (((6, 106), 93), 30),\n (((7, 107), 92), 50),\n (((8, 108), 91), 14),\n (((9, 109), 90), 66),\n (((10, 110), 89), 88),\n (((11, 111), 88), 7),\n (((12, 112), 87), 24),\n (((13, 113), 86), 92),\n (((14, 114), 85), 39)]"
                    },
                    "execution_count": 24,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "# we end up with a kind of nested tuple structure\n# this looks like it would be a problem, but let's keep going:\n\ndata3 = column1.zip(column2).zip(column3).zip(column4)\ndata3.take(15)"
        },
        {
            "cell_type": "code",
            "execution_count": 25,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "[(0, 100, 99, 63),\n (1, 101, 98, 62),\n (2, 102, 97, 51),\n (3, 103, 96, 0),\n (4, 104, 95, 94),\n (5, 105, 94, 4),\n (6, 106, 93, 30),\n (7, 107, 92, 50),\n (8, 108, 91, 14),\n (9, 109, 90, 66),\n (10, 110, 89, 88),\n (11, 111, 88, 7),\n (12, 112, 87, 24),\n (13, 113, 86, 92),\n (14, 114, 85, 39)]"
                    },
                    "execution_count": 25,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "#  Next we need to use a lambda function to take this nested tuple structure and turn it into \n#  a list of tuples containing four elements each\n#  The course was written in Python 2.* so we have to create the Python 3.* code ourselves here:\n\n\ndata4 = column1.zip(column2).zip(column3).zip(column4).map(lambda a__b__c__d : (a__b__c__d[0][0][0],a__b__c__d[0][0][1],a__b__c__d[0][1],a__b__c__d[1]) )\ndata4.take(15)"
        },
        {
            "cell_type": "code",
            "execution_count": 26,
            "metadata": {},
            "outputs": [],
            "source": "# So amazingly, that worked\n\n# Next we need to figure out out to tranform this structure that contains these 4 datasets into a correlation matrix between\n# the four datasets.\n\n# but we still need one more transform:\n# We need each of the tuples to be arrays instead, so that we an rdd of arrays, with 4 elements in each array:"
        },
        {
            "cell_type": "code",
            "execution_count": 27,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "[[0, 100, 99, 63],\n [1, 101, 98, 62],\n [2, 102, 97, 51],\n [3, 103, 96, 0],\n [4, 104, 95, 94],\n [5, 105, 94, 4],\n [6, 106, 93, 30],\n [7, 107, 92, 50],\n [8, 108, 91, 14],\n [9, 109, 90, 66],\n [10, 110, 89, 88],\n [11, 111, 88, 7],\n [12, 112, 87, 24],\n [13, 113, 86, 92],\n [14, 114, 85, 39]]"
                    },
                    "execution_count": 27,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "data = column1.zip(column2).zip(column3).zip(column4).map(lambda a__b__c__d : (a__b__c__d[0][0][0],a__b__c__d[0][0][1],a__b__c__d[0][1],a__b__c__d[1]) ).map(lambda abcd:[abcd[0],abcd[1],abcd[2],abcd[3]])\ndata.take(15)"
        },
        {
            "cell_type": "code",
            "execution_count": 28,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "array([[ 1.        ,  1.        , -1.        , -0.03714371],\n       [ 1.        ,  1.        , -1.        , -0.03714371],\n       [-1.        , -1.        ,  1.        ,  0.03714371],\n       [-0.03714371, -0.03714371,  0.03714371,  1.        ]])"
                    },
                    "execution_count": 28,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "# Now we really have something that looks like a relational database table.\n\n#   *** Data preparation is more than 80% of the work involved in data science.  ***\n\n# Once we have the data in this particular format, the generation of the correlation matrix\n# is relatively simple:\n\nStatistics.corr(data)"
        },
        {
            "cell_type": "code",
            "execution_count": 29,
            "metadata": {},
            "outputs": [],
            "source": "# There is redundant information in this as in all correlation matrixes, \n# since the top left to bottom right diagonal is all correlation of \n# datasets with themselves, which is always 1.  All of hte interesting\n# information is contained in the upper rectangular matrix not including\n# the diagonal:\n\n# 1          -1            0.0874\n#            -1            0.0874\n#                         -0.0874\n\n# Both Covariance and correlation determines the degree of linear relationship\n# between two datasets.  Correlation is a normalized and unitless measure of \n# the linearity whereas Covariance is not normalized and also has units."
        },
        {
            "cell_type": "code",
            "execution_count": 30,
            "metadata": {},
            "outputs": [],
            "source": "# [end of notebook on covariance and correlation - FFH]"
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.7 with Spark",
            "language": "python3",
            "name": "python37"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.7.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}