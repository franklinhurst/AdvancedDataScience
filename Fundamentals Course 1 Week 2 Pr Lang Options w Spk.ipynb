{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "ename": "Compile Error",
                    "evalue": "<console>:1: error: illegal start of definition\n       # in Scala  the 'val' keyword tells Scala that a constant variable is defined\n       ^\n<console>:1: error: unclosed character literal (or use \" for string literal \"val\")\n       # in Scala  the 'val' keyword tells Scala that a constant variable is defined\n                           ^\n",
                    "output_type": "error",
                    "traceback": []
                }
            ],
            "source": "# in Scala  the 'val' keyword tells Scala that a constant variable is defined"
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "rdd = ParallelCollectionRDD[0] at parallelize at <console>:27\n"
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/plain": "ParallelCollectionRDD[0] at parallelize at <console>:27"
                    },
                    "execution_count": 2,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "val rdd = sc.parallelize(List.range(0,100))"
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "ename": "Compile Error",
                    "evalue": "<console>:1: error: illegal start of definition\n       # in Scala in Spark, 'rdd' is used in the same way as the 'sc' object is used in Python.\n       ^\n<console>:1: error: unclosed character literal (or use \" for string literal \"rdd\")\n       # in Scala in Spark, 'rdd' is used in the same way as the 'sc' object is used in Python.\n                                ^\n<console>:1: error: unclosed character literal (or use \" for string literal \"sc\")\n       # in Scala in Spark, 'rdd' is used in the same way as the 'sc' object is used in Python.\n                                                                    ^\n<console>:2: error: unclosed character literal (or use \" for string literal \"sc\")\n       # 'sc' stands for spark context and it is used to create an RDD from an array or other\n            ^\n",
                    "output_type": "error",
                    "traceback": []
                }
            ],
            "source": "# in Scala in Spark, 'rdd' is used in the same way as the 'sc' object is used in Python.  \n# 'sc' stands for spark context and it is used to create an RDD from an array or other\n# supported data source.  This is the main syntatic difference between python and scala\n# the difference of an array ranging from 0 to 100 looks a bit different than in Python\n# spark also uses lazy evaluation with scala like in Python"
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "ename": "Compile Error",
                    "evalue": "<console>:1: error: illegal start of definition\n       # we count the number of elements next\n       ^\n",
                    "output_type": "error",
                    "traceback": []
                }
            ],
            "source": "# we count the number of elements next"
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "[Stage 0:>                                                          (0 + 2) / 2]\r"
                },
                {
                    "data": {
                        "text/plain": "100"
                    },
                    "execution_count": 5,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "rdd.count()"
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "Array(0, 1, 2, 3, 4, 5, 6, 7, 8, 9)"
                    },
                    "execution_count": 6,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "rdd.take(10)"
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "ename": "Compile Error",
                    "evalue": "<console>:1: error: illegal start of definition\n       # copy the entire contents of this RDD to the Spark driver JVM\n       ^\n",
                    "output_type": "error",
                    "traceback": []
                }
            ],
            "source": "# copy the entire contents of this RDD to the Spark driver JVM"
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "Array(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99)"
                    },
                    "execution_count": 8,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "rdd.collect"
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "ename": "Compile Error",
                    "evalue": "<console>:1: error: illegal start of definition\n       # everything here is similar to what we saw in ApacheSpark using Python\n       ^\n",
                    "output_type": "error",
                    "traceback": []
                }
            ],
            "source": "# everything here is similar to what we saw in ApacheSpark using Python\n# when we use external libraries, the difference in the two languages becomes more noticable\n# for example, numpy provides Python with powerful access to matrix and vector operations"
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [
                {
                    "ename": "Compile Error",
                    "evalue": "<console>:1: error: illegal start of definition\n       # Java:\n       ^\n",
                    "output_type": "error",
                    "traceback": []
                }
            ],
            "source": "# Java:\n#  Not a Data Science programming language\n#  Compete API set is available to use ApacheSpark\n#  Java is the defacto standard in Enterprise IT, \n#  so you have to learn it in general, and it can \n#  be used for data science\n#  Java is the language of Hadoop, which was the \n#  standard *before* ApacheSpark"
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [
                {
                    "ename": "Compile Error",
                    "evalue": "<console>:1: error: illegal start of definition\n       # R:\n       ^\n",
                    "output_type": "error",
                    "traceback": []
                }
            ],
            "source": "# R:\n#  R is THE Data Science Programming Language\n#  However, ONLY a subset of ApacheSpark API is available\n#  R is the defacto standard in academic research\n#  There are more than 8,000 add-on packages available for R\n#  R has awesome plotting and charting libraries\n#  R is one of the slowest programming languages\n#  As long as you are only using R to execute programs on \n#  Apache spark, then this won't be a problem\n#  But as soon as you mix and match local and parallel\n#  versions of R you will notice the limitations of the language.\n\n#  There is a notebook created to demo R in this project - FH"
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Scala 2.12 with Spark",
            "language": "scala",
            "name": "scala"
        },
        "language_info": {
            "codemirror_mode": "text/x-scala",
            "file_extension": ".scala",
            "mimetype": "text/x-scala",
            "name": "scala",
            "pygments_lexer": "scala",
            "version": "2.12.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}