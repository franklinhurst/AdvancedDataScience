{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": "from IPython.display import Markdown, display\ndef printmd(string):\n    display(Markdown('# <span style=\"color:red\">'+string+'</span>'))\n\n\nif ('sc' in locals() or 'sc' in globals()):\n    printmd('<<<<<!!!!! It seems that you are running in a IBM Watson Studio Apache Spark Notebook. Please run it in an IBM Watson Studio Default Runtime (without Apache Spark) !!!!!>>>>>')"
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "/opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages/secretstorage/dhcrypto.py:16: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n  from cryptography.utils import int_from_bytes\n/opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages/secretstorage/util.py:25: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n  from cryptography.utils import int_from_bytes\nRequirement already satisfied: pyspark==2.4.5 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (2.4.5)\nRequirement already satisfied: py4j==0.10.7 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from pyspark==2.4.5) (0.10.7)\n"
                }
            ],
            "source": "!pip install pyspark==2.4.5"
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": "try:\n    from pyspark import SparkContext, SparkConf\n    from pyspark.sql import SparkSession\nexcept ImportError as e:\n    printmd('<<<<<!!!!! Please restart your kernel after installing Apache Spark !!!!!>>>>>')"
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": "sc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\"))\n\nspark = SparkSession \\\n    .builder \\\n    .getOrCreate()"
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "--2021-05-06 14:13:58--  https://github.com/IBM/coursera/raw/master/hmp.parquet\nResolving github.com (github.com)... 140.82.113.4\nConnecting to github.com (github.com)|140.82.113.4|:443... connected.\nHTTP request sent, awaiting response... 301 Moved Permanently\nLocation: https://github.com/IBM/skillsnetwork/raw/master/hmp.parquet [following]\n--2021-05-06 14:13:58--  https://github.com/IBM/skillsnetwork/raw/master/hmp.parquet\nReusing existing connection to github.com:443.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://raw.githubusercontent.com/IBM/skillsnetwork/master/hmp.parquet [following]\n--2021-05-06 14:13:58--  https://raw.githubusercontent.com/IBM/skillsnetwork/master/hmp.parquet\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.110.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 932997 (911K) [application/octet-stream]\nSaving to: \u2018hmp.parquet\u2019\n\nhmp.parquet         100%[===================>] 911.13K  --.-KB/s    in 0.02s   \n\n2021-05-06 14:13:58 (45.1 MB/s) - \u2018hmp.parquet\u2019 saved [932997/932997]\n\n"
                }
            ],
            "source": "# delete files from previous runs\n!rm -f hmp.parquet*\n\n# download the file containing the data in PARQUET format\n!wget https://github.com/IBM/coursera/raw/master/hmp.parquet\n    \n# create a dataframe out of it\ndf = spark.read.parquet('hmp.parquet')\n\n# register a corresponding query table\n# this creates a temporary query view in order to write SQL queries against the dataframe\ndf.createOrReplaceTempView('df')"
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "+------------------+--------------+\n|             label|         class|\n+------------------+--------------+\n| 8959.680239829991| Use_telephone|\n| 9737.511232342687| Standup_chair|\n| 12542.96539897962|      Eat_meat|\n|13225.945637269193|     Getup_bed|\n|15003.269043778426|   Drink_glass|\n|14454.885091207056|    Pour_water|\n|10616.408809008817|     Comb_hair|\n|16537.370891408344|          Walk|\n|11082.626493751379|  Climb_stairs|\n|10261.338314274606| Sitdown_chair|\n|6783.4063714331605|   Liedown_bed|\n| 7173.493500380411|Descend_stairs|\n| 11785.39634462923|   Brush_teeth|\n| 6071.460120926432|      Eat_soup|\n+------------------+--------------+\n\n"
                }
            ],
            "source": "# setting this up to do linear regression, we are adding an emergy column\n# the SQL statement computes the energy for us, the column is labeled \"label\"\n\ndf_energy = spark.sql(\"\"\"\nselect sqrt(sum(x*x)+sum(y*y)+sum(z*z)) as label, class from df group by class\n\"\"\").show()      "
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [],
            "source": "df_energy = spark.sql(\"\"\"\nselect sqrt(sum(x*x)+sum(y*y)+sum(z*z)) as label, class from df group by class\n\"\"\")\n\ndf_energy.createOrReplaceTempView('df_energy')"
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [],
            "source": "df_join = spark.sql('select * from df inner join df_energy on df.class=df_energy.class')"
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "+---+---+---+--------------------+-----------+-----------------+-----------+\n|  x|  y|  z|              source|      class|            label|      class|\n+---+---+---+--------------------+-----------+-----------------+-----------+\n| 22| 49| 35|Accelerometer-201...|Brush_teeth|11785.39634462923|Brush_teeth|\n| 22| 49| 35|Accelerometer-201...|Brush_teeth|11785.39634462923|Brush_teeth|\n| 22| 52| 35|Accelerometer-201...|Brush_teeth|11785.39634462923|Brush_teeth|\n| 22| 52| 35|Accelerometer-201...|Brush_teeth|11785.39634462923|Brush_teeth|\n| 21| 52| 34|Accelerometer-201...|Brush_teeth|11785.39634462923|Brush_teeth|\n| 22| 51| 34|Accelerometer-201...|Brush_teeth|11785.39634462923|Brush_teeth|\n| 20| 50| 35|Accelerometer-201...|Brush_teeth|11785.39634462923|Brush_teeth|\n| 22| 52| 34|Accelerometer-201...|Brush_teeth|11785.39634462923|Brush_teeth|\n| 22| 50| 34|Accelerometer-201...|Brush_teeth|11785.39634462923|Brush_teeth|\n| 22| 51| 35|Accelerometer-201...|Brush_teeth|11785.39634462923|Brush_teeth|\n| 21| 51| 33|Accelerometer-201...|Brush_teeth|11785.39634462923|Brush_teeth|\n| 20| 50| 34|Accelerometer-201...|Brush_teeth|11785.39634462923|Brush_teeth|\n| 21| 49| 33|Accelerometer-201...|Brush_teeth|11785.39634462923|Brush_teeth|\n| 21| 49| 33|Accelerometer-201...|Brush_teeth|11785.39634462923|Brush_teeth|\n| 20| 51| 35|Accelerometer-201...|Brush_teeth|11785.39634462923|Brush_teeth|\n| 18| 49| 34|Accelerometer-201...|Brush_teeth|11785.39634462923|Brush_teeth|\n| 19| 48| 34|Accelerometer-201...|Brush_teeth|11785.39634462923|Brush_teeth|\n| 16| 53| 34|Accelerometer-201...|Brush_teeth|11785.39634462923|Brush_teeth|\n| 18| 52| 35|Accelerometer-201...|Brush_teeth|11785.39634462923|Brush_teeth|\n| 18| 51| 32|Accelerometer-201...|Brush_teeth|11785.39634462923|Brush_teeth|\n+---+---+---+--------------------+-----------+-----------------+-----------+\nonly showing top 20 rows\n\n"
                }
            ],
            "source": "df_join.show()"
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [],
            "source": "# the energy column is now there and it is something that we can try \n# to predict using the sensor data\n\n# we import and construct the vector assember and the normalizer\n# configuring the object to the appropriate input and output columns\n\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.feature import Normalizer\n\nvectorAssembler = VectorAssembler(inputCols=[\"x\",\"y\",\"z\"],\n                                  outputCol=\"features\")\nnormalizer = Normalizer(inputCol=\"features\", outputCol=\"features_norm\", p=1.0)"
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [],
            "source": "# we import linear regression and construct the instance of the \n# LR model called 'lr' with three parameters, maxIter, regParam, \n# and elasticNetParam\n\nfrom pyspark.ml.regression import LinearRegression\nlr = LinearRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)"
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [],
            "source": "# import the pipeline and create a new pipeline\n# we set up the pipeline to have three stages:\n# the vector Assembler, the normalizer, and the\n# linear regression\n\nfrom pyspark.ml import Pipeline\npipeline = Pipeline(stages=[vectorAssembler, normalizer, lr])"
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [],
            "source": "# we execute the pipeline on the dataframe called df_join\n# that we created above to fit the data\n\nmodel = pipeline.fit(df_join)"
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "+---+---+---+--------------------+-----------+-----------------+-----------+----------------+--------------------+------------------+\n|  x|  y|  z|              source|      class|            label|      class|        features|       features_norm|        prediction|\n+---+---+---+--------------------+-----------+-----------------+-----------+----------------+--------------------+------------------+\n| 22| 49| 35|Accelerometer-201...|Brush_teeth|11785.39634462923|Brush_teeth|[22.0,49.0,35.0]|[0.20754716981132...|12586.729735016828|\n| 22| 49| 35|Accelerometer-201...|Brush_teeth|11785.39634462923|Brush_teeth|[22.0,49.0,35.0]|[0.20754716981132...|12586.729735016828|\n| 22| 52| 35|Accelerometer-201...|Brush_teeth|11785.39634462923|Brush_teeth|[22.0,52.0,35.0]|[0.20183486238532...|12542.703337345756|\n| 22| 52| 35|Accelerometer-201...|Brush_teeth|11785.39634462923|Brush_teeth|[22.0,52.0,35.0]|[0.20183486238532...|12542.703337345756|\n| 21| 52| 34|Accelerometer-201...|Brush_teeth|11785.39634462923|Brush_teeth|[21.0,52.0,34.0]|[0.19626168224299...|12573.865911821156|\n| 22| 51| 34|Accelerometer-201...|Brush_teeth|11785.39634462923|Brush_teeth|[22.0,51.0,34.0]|[0.20560747663551...|12541.076564471234|\n| 20| 50| 35|Accelerometer-201...|Brush_teeth|11785.39634462923|Brush_teeth|[20.0,50.0,35.0]|[0.19047619047619...|12666.983895607029|\n| 22| 52| 34|Accelerometer-201...|Brush_teeth|11785.39634462923|Brush_teeth|[22.0,52.0,34.0]|[0.20370370370370...|12526.401098580878|\n| 22| 50| 34|Accelerometer-201...|Brush_teeth|11785.39634462923|Brush_teeth|[22.0,50.0,34.0]|[0.20754716981132...|12555.752030361591|\n| 22| 51| 35|Accelerometer-201...|Brush_teeth|11785.39634462923|Brush_teeth|[22.0,51.0,35.0]|[0.20370370370370...|12557.378803236114|\n| 21| 51| 33|Accelerometer-201...|Brush_teeth|11785.39634462923|Brush_teeth|[21.0,51.0,33.0]|[0.2,0.4857142857...|12572.239138946636|\n| 20| 50| 34|Accelerometer-201...|Brush_teeth|11785.39634462923|Brush_teeth|[20.0,50.0,34.0]|[0.19230769230769...| 12650.68165684215|\n| 21| 49| 33|Accelerometer-201...|Brush_teeth|11785.39634462923|Brush_teeth|[21.0,49.0,33.0]|[0.20388349514563...|12601.590070727349|\n| 21| 49| 33|Accelerometer-201...|Brush_teeth|11785.39634462923|Brush_teeth|[21.0,49.0,33.0]|[0.20388349514563...|12601.590070727349|\n| 20| 51| 35|Accelerometer-201...|Brush_teeth|11785.39634462923|Brush_teeth|[20.0,51.0,35.0]|[0.18867924528301...|12652.308429716672|\n| 18| 49| 34|Accelerometer-201...|Brush_teeth|11785.39634462923|Brush_teeth|[18.0,49.0,34.0]|[0.17821782178217...|12760.286749213064|\n| 19| 48| 34|Accelerometer-201...|Brush_teeth|11785.39634462923|Brush_teeth|[19.0,48.0,34.0]|[0.18811881188118...|12727.497401863142|\n| 16| 53| 34|Accelerometer-201...|Brush_teeth|11785.39634462923|Brush_teeth|[16.0,53.0,34.0]|[0.15533980582524...|12796.514512132195|\n| 18| 52| 35|Accelerometer-201...|Brush_teeth|11785.39634462923|Brush_teeth|[18.0,52.0,35.0]|[0.17142857142857...|12732.562590306872|\n| 18| 51| 32|Accelerometer-201...|Brush_teeth|11785.39634462923|Brush_teeth|[18.0,51.0,32.0]|[0.17821782178217...|12698.331339902592|\n+---+---+---+--------------------+-----------+-----------------+-----------+----------------+--------------------+------------------+\nonly showing top 20 rows\n\n"
                }
            ],
            "source": "# next, the prediction:  the prediction column is based on x, y, and z\n# we are trying to predict the label column using the prediction column\n\nprediction = model.transform(df_join)\nprediction.show()"
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "0.03259100556263628"
                    },
                    "execution_count": 16,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "# we obtain the linear regression model from the stages, and hte lr is stage 2\n# (we count from 0, 1, 2) and there are three stages\n# we want the r-squared from the linear regression\n\nmodel.stages[2].summary.r2"
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {},
            "outputs": [],
            "source": "# this is not the greatest linear regression r-squared\n# but this is essentially just rigged data, since we made up\n# the column to be predicted using a function"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": ""
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.7",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.7.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}