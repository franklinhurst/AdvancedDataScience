{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "collapsed": true
            },
            "source": "### Crossvalidation\n\nThe problem of overfitting is solved by dividing your data into something like 80% training data and 20% testing data.  Then you fit your model on the 80% training data and you test your model on the testing data.  But there is only one problem with this approach, which is that your model cannot be trained on your testing data.  The way to solve this is crossvalidation.  With crossvalidation, different folds of the data are created. You could for example have five folds of the data.  The first fold would take the first 20% as the testing data and the rest as training data.  The second fold would have the second 20% be testing data and the rest be training data, and so on, so that in each fold, the training data is a different part of the data.  Then, since you have a number of different folds for the data, you create a model for each fold.  Then you just combine all your models.  In this way,  the resulting model has been trained on all the data.  In the case of classification, the combining of the models happens by voting.  In the case of regression, you just take the average.  Crossvalidation lets you get the maximum amount of training out of your data."
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### Hyper-parameter tuning using Gridsearch\n\n### Precision and recall\n\n### F1 Score\n\n### Decisions Trees"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### Bootstrap Aggregation (Bagging) and RandomForest\n\n\nBootstrapping refers to taking samples from the training data B1, B2, etc. and you will use those for Random Forest.  Random forest refers to the practice of creating a decision tree based on each sample of hte data.  We take the average of the outcomes of the decision trees to come up with our prediction.  Decisions trees are weak models that are going to give an answer to a binary question.\n\nhttp://www.wilsonmongwe.co.za/wp-content/uploads/2017/09/random_forest_diagram_complete.png\n\nRandomForest compared to just using one decision tree is that there is improved model performance, and they are more resistant to overfitting."
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### Boosting and Gradient Boosted Trees\n\nBoosting is also called \"Stagewise additive Modeling\".  In contrast to RandomForst where all the decision trees are created in parallel, in boosting, each model is trained based on output from previous models.  Like RandomForest, this is an ensemble of weak models.  \n\nLet h_i(x) be a weak model.\n\nThen we can define a strong learner F(x) as:\n\nF(x) = Summation i=1 to M [gamma_i h_i(x)]\n\nThis give us F(x) as the sum of all the weak models weighted by gamma_i for each model, where there are i models.\n\nWe can also look at it like this:\n\nF_m(x) = F_(m-1)(x) + gamma_i h_m(x)\n\nThis last notation reflects that every F(x) will be the summation of the previous F(x) and the latest weak model added in.\n\nEach gamma_i is that gamma_i which minimizes the sum of the squared error, or the residuals, which is hte difference between the correct value and the predicted value.\n\nSo this is how boosting works.\n\n### Gradient Boosted Trees\n\nStart with a weak learner:\n\nF_0(X) = h_0 = mean(y)\n\nerror_1 = y - F_0(X)\n\nThe new training dataset consists of the errors from the previous model.\n\nThe new weak learner is h_1:\n\nF_1(X) = F_0(X) + h_1(error_1)\n\nerror_2 = y - F_1(X)\n\nThe next training data set consists of the error_2's.\n\nWe use this to train a new weak learner h_2:\n\nF_2(X) = F_1(X) + h_2(error_2)\n\nWe continue until convergence at some acceptable level.\n\nGradient Boosted Trees 1.) outpeform RandomForest 2.) are computationally expensive because of their sequential nature.\n\nThere is a specific implementation of Gradient Boosted Trees called XGBoost which is very fast, and it has a slightly different implementation than the one discussed here, which makes it very resistant to overfitting.\n\nIn summary, boosting is different than bootstrapping because it takes the previous step's error into account.\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": ""
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.7",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.7.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}