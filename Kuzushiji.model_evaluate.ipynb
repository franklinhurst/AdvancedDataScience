{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "collapsed": true
            },
            "source": "### Model Evaluation\n\nModel evaluation is a critical task in data science. This is one of the few measures business stakeholders are interested in. Model performance heavily influences business impact of a data science project. Therefore, it is important to take some time apart in an independent task in the process model. \n\nSo how are models evaluated? In supervised machine learning this is relatively straightforward since you can always create a ground truth and compare your results against ground truth.\n\nSo, we are either splitting data into training-, test- and validation-sets to assess model performance on the test set or we use cross validation. This all is explained in the following coursera course https://www.coursera.org/learn/advanced-machine-learning-signal-processing/ Week 2.\n\nIn case we know what data set we can use as ground truth in supervised learning (classification and regression) we need to define a different measure for evaluation than in unsupervised learning (clustering). Since it depends on the type of model we create, the following none exhaustive lists can be used as a starting point for further research:\n\n#### Classification:\n\n- Confusion Matrix\n\n- Accuracy\n\n- Precision\n\n- Recall\n\n- Specificity\n\n- True positive rate\n\n- True negative rate\n\n- False positive rate\n\n- False negative rate\n\n- F1-score\n\n- Gain and Lift\n\n- Kolomogorov Smirnov\n\n- Area Under ROC\n\n- Gini Coefficient\n\n- Concordant \u2013 Discordant ratio\n\n#### Regression:\n\n- Root Mean Squared Error (RMSE)\n\n- Mean Squared Error\n\n- Mean Absolute Error (MAE)\n\n- R-Squared\n\n- Relative Squared Error\n\n- Relative Absolute Error\n\n- Sum of Differences\n\n- ACF plot of residuals\n\n- Histogram of residuals\n\n- Residual plots against predictors\n\n- Residual plots against fitted values\n\nClustering:\n\n- Adjusted Rand index\n\n- Mutual Information\n\n- Homogeneity completeness\n\n- V-measure\n\n- Fowlkes-Mallows\n\n- Silhouette Coefficient Calinski-Harabaz\u00b6\n\nReferences:\n\nhttp://scikit-learn.org/stable/modules/clustering.html#clustering-performance-evaluation\n\nPlease choose at least one appropriate model performance measure, justify why you\u2019ve used it and document how iterative changes in the feature creation task influence it."
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Requirement already satisfied: tensorflow in /opt/conda/envs/Python-3.7-CUDA/lib/python3.7/site-packages (2.4.1)\nRequirement already satisfied: gast==0.3.3 in /opt/conda/envs/Python-3.7-CUDA/lib/python3.7/site-packages (from tensorflow) (0.3.3)\nRequirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0 in /opt/conda/envs/Python-3.7-CUDA/lib/python3.7/site-packages (from tensorflow) (2.4.0)\nCollecting opt-einsum~=3.3.0\n  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 65 kB 5.7 MB/s  eta 0:00:01\n\u001b[?25hRequirement already satisfied: tensorboard~=2.4 in /opt/conda/envs/Python-3.7-CUDA/lib/python3.7/site-packages (from tensorflow) (2.4.1)\nCollecting flatbuffers~=1.12.0\n  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\nRequirement already satisfied: google-pasta~=0.2 in /opt/conda/envs/Python-3.7-CUDA/lib/python3.7/site-packages (from tensorflow) (0.2.0)\nRequirement already satisfied: termcolor~=1.1.0 in /opt/conda/envs/Python-3.7-CUDA/lib/python3.7/site-packages (from tensorflow) (1.1.0)\nRequirement already satisfied: wrapt~=1.12.1 in /opt/conda/envs/Python-3.7-CUDA/lib/python3.7/site-packages (from tensorflow) (1.12.1)\nRequirement already satisfied: typing-extensions~=3.7.4 in /opt/conda/envs/Python-3.7-CUDA/lib/python3.7/site-packages (from tensorflow) (3.7.4.2)\nRequirement already satisfied: six~=1.15.0 in /opt/conda/envs/Python-3.7-CUDA/lib/python3.7/site-packages (from tensorflow) (1.15.0)\nCollecting keras-preprocessing~=1.1.2\n  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 42 kB 3.0 MB/s  eta 0:00:01\n\u001b[?25hRequirement already satisfied: astunparse~=1.6.3 in /opt/conda/envs/Python-3.7-CUDA/lib/python3.7/site-packages (from tensorflow) (1.6.3)\nCollecting grpcio~=1.32.0\n  Downloading grpcio-1.32.0-cp37-cp37m-manylinux2014_x86_64.whl (3.8 MB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3.8 MB 28.1 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: h5py~=2.10.0 in /opt/conda/envs/Python-3.7-CUDA/lib/python3.7/site-packages (from tensorflow) (2.10.0)\nRequirement already satisfied: wheel~=0.35 in /opt/conda/envs/Python-3.7-CUDA/lib/python3.7/site-packages (from tensorflow) (0.35.1)\nRequirement already satisfied: absl-py~=0.10 in /opt/conda/envs/Python-3.7-CUDA/lib/python3.7/site-packages (from tensorflow) (0.10.0)\nRequirement already satisfied: numpy~=1.19.2 in /opt/conda/envs/Python-3.7-CUDA/lib/python3.7/site-packages (from tensorflow) (1.19.2)\nRequirement already satisfied: protobuf>=3.9.2 in /opt/conda/envs/Python-3.7-CUDA/lib/python3.7/site-packages (from tensorflow) (3.11.2)\nRequirement already satisfied: setuptools>=41.0.0 in /opt/conda/envs/Python-3.7-CUDA/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow) (47.3.1.post20200622)\nRequirement already satisfied: werkzeug>=0.11.15 in /opt/conda/envs/Python-3.7-CUDA/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow) (0.16.0)\nRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/envs/Python-3.7-CUDA/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow) (2.22.0)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/envs/Python-3.7-CUDA/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow) (3.1.1)\nRequirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/envs/Python-3.7-CUDA/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow) (1.6.0)\nRequirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/envs/Python-3.7-CUDA/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow) (0.4.1)\nRequirement already satisfied: google-auth<2,>=1.6.3 in /opt/conda/envs/Python-3.7-CUDA/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow) (1.23.0)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/envs/Python-3.7-CUDA/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (1.25.9)\nRequirement already satisfied: idna<2.9,>=2.5 in /opt/conda/envs/Python-3.7-CUDA/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (2.8)\nRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/envs/Python-3.7-CUDA/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (3.0.4)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/Python-3.7-CUDA/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (2021.5.30)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/envs/Python-3.7-CUDA/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow) (1.3.0)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/envs/Python-3.7-CUDA/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (0.2.8)\nRequirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/envs/Python-3.7-CUDA/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (4.1.1)\nRequirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.5\" in /opt/conda/envs/Python-3.7-CUDA/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (4.6)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/envs/Python-3.7-CUDA/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow) (3.1.0)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/envs/Python-3.7-CUDA/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (0.4.8)\nInstalling collected packages: opt-einsum, flatbuffers, keras-preprocessing, grpcio\n  Attempting uninstall: opt-einsum\n    Found existing installation: opt-einsum 3.1.0\n    Uninstalling opt-einsum-3.1.0:\n      Successfully uninstalled opt-einsum-3.1.0\n  Attempting uninstall: flatbuffers\n    Found existing installation: flatbuffers 20210226132247\n    Uninstalling flatbuffers-20210226132247:\n      Successfully uninstalled flatbuffers-20210226132247\n  Attempting uninstall: keras-preprocessing\n    Found existing installation: Keras-Preprocessing 1.1.0\n    Uninstalling Keras-Preprocessing-1.1.0:\n      Successfully uninstalled Keras-Preprocessing-1.1.0\n  Attempting uninstall: grpcio\n    Found existing installation: grpcio 1.31.0\n    Uninstalling grpcio-1.31.0:\n      Successfully uninstalled grpcio-1.31.0\nSuccessfully installed flatbuffers-1.12 grpcio-1.32.0 keras-preprocessing-1.1.2 opt-einsum-3.3.0\nCollecting python-mnist\n  Downloading python_mnist-0.7-py2.py3-none-any.whl (9.6 kB)\nInstalling collected packages: python-mnist\nSuccessfully installed python-mnist-0.7\nRequirement already satisfied: Pillow in /opt/conda/envs/Python-3.7-CUDA/lib/python3.7/site-packages (8.2.0)\nCollecting pyspark\n  Downloading pyspark-3.1.2.tar.gz (212.4 MB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 212.4 MB 123 kB/s  eta 0:00:011\n\u001b[?25hCollecting py4j==0.10.9\n  Downloading py4j-0.10.9-py2.py3-none-any.whl (198 kB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 198 kB 53.3 MB/s eta 0:00:01\n\u001b[?25hBuilding wheels for collected packages: pyspark\n  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.1.2-py2.py3-none-any.whl size=212880769 sha256=c839abfe930120e4c33498925c50de11d8386bfeda4ee26e4d583e85db775311\n  Stored in directory: /tmp/wsuser/.cache/pip/wheels/a5/0a/c1/9561f6fecb759579a7d863dcd846daaa95f598744e71b02c77\nSuccessfully built pyspark\nInstalling collected packages: py4j, pyspark\nSuccessfully installed py4j-0.10.9 pyspark-3.1.2\nRequirement already satisfied: scikit-learn in /opt/conda/envs/Python-3.7-CUDA/lib/python3.7/site-packages (0.23.2)\nRequirement already satisfied: scipy>=0.19.1 in /opt/conda/envs/Python-3.7-CUDA/lib/python3.7/site-packages (from scikit-learn) (1.4.1)\nRequirement already satisfied: joblib>=0.11 in /opt/conda/envs/Python-3.7-CUDA/lib/python3.7/site-packages (from scikit-learn) (0.16.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/envs/Python-3.7-CUDA/lib/python3.7/site-packages (from scikit-learn) (2.1.0)\nRequirement already satisfied: numpy>=1.13.3 in /opt/conda/envs/Python-3.7-CUDA/lib/python3.7/site-packages (from scikit-learn) (1.19.2)\n"
                }
            ],
            "source": "!pip install tensorflow\n!pip install python-mnist\n!pip install Pillow\n!pip install pyspark\n!pip install scikit-learn"
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "'2.4.1'"
                    },
                    "execution_count": 2,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "%matplotlib inline\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom matplotlib import image\nimport tensorflow as tf\nimport seaborn as sns\nfrom mnist import MNIST\nimport numpy as np\nimport PIL\nfrom PIL import Image\nimport os\nimport matplotlib.image as mping\nfrom pyspark import SparkContext, SparkConf\nfrom pyspark.sql import SparkSession\nfrom pyspark.ml.feature import StringIndexer\nfrom numpy import asarray\nimport pandas as pd\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout\nfrom tensorflow.keras import backend as K\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.decomposition import PCA\n\ntf.__version__"
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "'8.2.0'"
                    },
                    "execution_count": 3,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "PIL.__version__"
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": "# fire up the spark session\n# comment this cell out for a spark server\nsc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\"))\n\nspark = SparkSession \\\n    .builder \\\n    .getOrCreate()"
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": "# enable arrow which lets us transfrom a pandas dataframe into a pyspark dataframe\nspark.conf.set(\"spark.sql.execution.arrow.enabled\",\"true\")"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## First dataset\n\n#### 60,000 images and 10 classes, each image is 28 x 28 or represented by a 794 element array"
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "--2021-07-06 21:29:26--  http://codh.rois.ac.jp/kmnist/dataset/kmnist/train-images-idx3-ubyte.gz?raw=True\nResolving codh.rois.ac.jp (codh.rois.ac.jp)... 136.187.88.58\nConnecting to codh.rois.ac.jp (codh.rois.ac.jp)|136.187.88.58|:80... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 18165135 (17M)\nSaving to: \u2018train-images-idx3-ubyte.gz?raw=True\u2019\n\ntrain-images-idx3-u 100%[===================>]  17.32M  7.15MB/s    in 2.4s    \n\n2021-07-06 21:29:29 (7.15 MB/s) - \u2018train-images-idx3-ubyte.gz?raw=True\u2019 saved [18165135/18165135]\n\n-rw-rw---- 1 wsuser watsonstudio 45M Feb  4  2019 train-images-idx3-ubyte\n--2021-07-06 21:29:31--  http://codh.rois.ac.jp/kmnist/dataset/kmnist/train-labels-idx1-ubyte.gz?raw=True\nResolving codh.rois.ac.jp (codh.rois.ac.jp)... 136.187.88.58\nConnecting to codh.rois.ac.jp (codh.rois.ac.jp)|136.187.88.58|:80... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 29497 (29K)\nSaving to: \u2018train-labels-idx1-ubyte.gz?raw=True\u2019\n\ntrain-labels-idx1-u 100%[===================>]  28.81K  --.-KB/s    in 0.1s    \n\n2021-07-06 21:29:32 (206 KB/s) - \u2018train-labels-idx1-ubyte.gz?raw=True\u2019 saved [29497/29497]\n\n-rw-rw---- 1 wsuser watsonstudio 59K Feb  4  2019 train-labels-idx1-ubyte\n--2021-07-06 21:29:34--  http://codh.rois.ac.jp/kmnist/dataset/kmnist/t10k-images-idx3-ubyte.gz?raw=True\nResolving codh.rois.ac.jp (codh.rois.ac.jp)... 136.187.88.58\nConnecting to codh.rois.ac.jp (codh.rois.ac.jp)|136.187.88.58|:80... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 3041136 (2.9M)\nSaving to: \u2018t10k-images-idx3-ubyte.gz?raw=True\u2019\n\nt10k-images-idx3-ub 100%[===================>]   2.90M  2.51MB/s    in 1.2s    \n\n2021-07-06 21:29:36 (2.51 MB/s) - \u2018t10k-images-idx3-ubyte.gz?raw=True\u2019 saved [3041136/3041136]\n\n-rw-rw---- 1 wsuser watsonstudio 7.5M Feb  4  2019 t10k-images-idx3-ubyte\n--2021-07-06 21:29:38--  http://codh.rois.ac.jp/kmnist/dataset/kmnist/t10k-labels-idx1-ubyte.gz?raw=True\nResolving codh.rois.ac.jp (codh.rois.ac.jp)... 136.187.88.58\nConnecting to codh.rois.ac.jp (codh.rois.ac.jp)|136.187.88.58|:80... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 5120 (5.0K)\nSaving to: \u2018t10k-labels-idx1-ubyte.gz?raw=True\u2019\n\nt10k-labels-idx1-ub 100%[===================>]   5.00K  --.-KB/s    in 0s      \n\n2021-07-06 21:29:39 (305 MB/s) - \u2018t10k-labels-idx1-ubyte.gz?raw=True\u2019 saved [5120/5120]\n\n-rw-rw---- 1 wsuser watsonstudio 9.8K Feb  4  2019 t10k-labels-idx1-ubyte\n"
                }
            ],
            "source": "!wget http://codh.rois.ac.jp/kmnist/dataset/kmnist/train-images-idx3-ubyte.gz?raw=True\n!mv train-images-idx3-ubyte.gz?raw=True train-images-idx3-ubyte.gz\n!gunzip train-images-idx3-ubyte.gz\n!ls -lahr train-images-idx3-ubyte\n\n!wget http://codh.rois.ac.jp/kmnist/dataset/kmnist/train-labels-idx1-ubyte.gz?raw=True\n!mv train-labels-idx1-ubyte.gz?raw=True train-labels-idx1-ubyte.gz\n!gunzip train-labels-idx1-ubyte.gz\n!ls -lahr train-labels-idx1-ubyte\n\n!wget http://codh.rois.ac.jp/kmnist/dataset/kmnist/t10k-images-idx3-ubyte.gz?raw=True\n!mv t10k-images-idx3-ubyte.gz?raw=True t10k-images-idx3-ubyte.gz\n!gunzip t10k-images-idx3-ubyte.gz\n!ls -lahr t10k-images-idx3-ubyte\n\n!wget http://codh.rois.ac.jp/kmnist/dataset/kmnist/t10k-labels-idx1-ubyte.gz?raw=True\n!mv t10k-labels-idx1-ubyte.gz?raw=True t10k-labels-idx1-ubyte.gz\n!gunzip t10k-labels-idx1-ubyte.gz\n!ls -lahr t10k-labels-idx1-ubyte"
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "total 53680\r\ndrwxrwx--- 2 wsuser watsonstudio     4096 Jul  6 21:29 .\r\ndrwxr-x--- 4 wsuser watsonstudio     4096 Jul  6 21:29 ..\r\n-rw-rw---- 1 wsuser watsonstudio  7840016 Jul  6 21:29 t10k-images-idx3-ubyte\r\n-rw-rw---- 1 wsuser watsonstudio    10008 Jul  6 21:29 t10k-labels-idx1-ubyte\r\n-rw-rw---- 1 wsuser watsonstudio 47040016 Jul  6 21:29 train-images-idx3-ubyte\r\n-rw-rw---- 1 wsuser watsonstudio    60008 Jul  6 21:29 train-labels-idx1-ubyte\r\n"
                }
            ],
            "source": "!mkdir kmnistdata\n!cp t10k-images-idx3-ubyte kmnistdata/t10k-images-idx3-ubyte\n!cp t10k-labels-idx1-ubyte kmnistdata/t10k-labels-idx1-ubyte\n!cp train-images-idx3-ubyte kmnistdata/train-images-idx3-ubyte\n!cp train-labels-idx1-ubyte kmnistdata/train-labels-idx1-ubyte\n!ls -al kmnistdata\n\ndata = MNIST('kmnistdata')\ntrain_images, train_labels = data.load_training()\ntest_images, test_labels = data.load_testing()\n\ntrain_images = np.array(train_images)\ntrain_labels = np.array(train_labels)\ntest_images = np.array(test_images)\ntest_labels = np.array(test_labels)\n\ntrain_images, val_images, train_labels, val_labels = train_test_split(train_images, train_labels, test_size = 0.2, random_state=39)\n\ntrain_images = train_images / 255\nval_images = val_images / 255\ntest_images = test_images / 255"
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [],
            "source": "# model_1 def\n\nmodel_1 = tf.keras.Sequential([\n    tf.keras.layers.Dense(128, activation=tf.nn.relu),\n    tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n])"
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [],
            "source": "# model_1 compile\nmodel_1.compile(optimizer='adam',\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])"
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Epoch 1/20\n1500/1500 [==============================] - 5s 3ms/step - loss: 0.6649 - accuracy: 0.7991 - val_loss: 0.2637 - val_accuracy: 0.9208\nEpoch 2/20\n1500/1500 [==============================] - 4s 2ms/step - loss: 0.2289 - accuracy: 0.9338 - val_loss: 0.2120 - val_accuracy: 0.9366\nEpoch 3/20\n1500/1500 [==============================] - 4s 2ms/step - loss: 0.1456 - accuracy: 0.9581 - val_loss: 0.1906 - val_accuracy: 0.9412\nEpoch 4/20\n1500/1500 [==============================] - 4s 2ms/step - loss: 0.1082 - accuracy: 0.9695 - val_loss: 0.1754 - val_accuracy: 0.9475\nEpoch 5/20\n1500/1500 [==============================] - 4s 2ms/step - loss: 0.0808 - accuracy: 0.9780 - val_loss: 0.1725 - val_accuracy: 0.9503\nEpoch 6/20\n1500/1500 [==============================] - 4s 3ms/step - loss: 0.0598 - accuracy: 0.9837 - val_loss: 0.1786 - val_accuracy: 0.9484\nEpoch 7/20\n1500/1500 [==============================] - 4s 2ms/step - loss: 0.0428 - accuracy: 0.9892 - val_loss: 0.1921 - val_accuracy: 0.9463\nEpoch 8/20\n1500/1500 [==============================] - 4s 2ms/step - loss: 0.0346 - accuracy: 0.9910 - val_loss: 0.1863 - val_accuracy: 0.9513\nEpoch 9/20\n1500/1500 [==============================] - 4s 2ms/step - loss: 0.0294 - accuracy: 0.9922 - val_loss: 0.2064 - val_accuracy: 0.9477\nEpoch 10/20\n1500/1500 [==============================] - 4s 2ms/step - loss: 0.0219 - accuracy: 0.9945 - val_loss: 0.2091 - val_accuracy: 0.9504\nEpoch 11/20\n1500/1500 [==============================] - 4s 2ms/step - loss: 0.0192 - accuracy: 0.9946 - val_loss: 0.2378 - val_accuracy: 0.9453\nEpoch 12/20\n1500/1500 [==============================] - 4s 2ms/step - loss: 0.0145 - accuracy: 0.9963 - val_loss: 0.2271 - val_accuracy: 0.9500\nEpoch 13/20\n1500/1500 [==============================] - 4s 2ms/step - loss: 0.0168 - accuracy: 0.9951 - val_loss: 0.2339 - val_accuracy: 0.9508\nEpoch 14/20\n1500/1500 [==============================] - 4s 2ms/step - loss: 0.0112 - accuracy: 0.9968 - val_loss: 0.2574 - val_accuracy: 0.9494\nEpoch 15/20\n1500/1500 [==============================] - 4s 2ms/step - loss: 0.0139 - accuracy: 0.9957 - val_loss: 0.2444 - val_accuracy: 0.9505\nEpoch 16/20\n1500/1500 [==============================] - 4s 2ms/step - loss: 0.0085 - accuracy: 0.9977 - val_loss: 0.2600 - val_accuracy: 0.9488\nEpoch 17/20\n1500/1500 [==============================] - 4s 2ms/step - loss: 0.0093 - accuracy: 0.9975 - val_loss: 0.2693 - val_accuracy: 0.9526\nEpoch 18/20\n1500/1500 [==============================] - 4s 2ms/step - loss: 0.0109 - accuracy: 0.9966 - val_loss: 0.2563 - val_accuracy: 0.9532\nEpoch 19/20\n1500/1500 [==============================] - 4s 2ms/step - loss: 0.0080 - accuracy: 0.9981 - val_loss: 0.2756 - val_accuracy: 0.9523\nEpoch 20/20\n1500/1500 [==============================] - 4s 2ms/step - loss: 0.0084 - accuracy: 0.9975 - val_loss: 0.2730 - val_accuracy: 0.9514\n"
                },
                {
                    "data": {
                        "text/plain": "<tensorflow.python.keras.callbacks.History at 0x7f8bec0e8c10>"
                    },
                    "execution_count": 10,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "# model_1 fit\n\n# began by using 5 epochs here but observed that \n# the validation accuracy is increasing even as we\n# stopped training, so consider using more epochs here next\n\nmodel_1.fit(train_images, train_labels, epochs=20, verbose=1, validation_data=(val_images, val_labels))"
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "model_1 test loss: 0.7422628998756409\nmodel_1 test accuracy 0.8898000121116638\n"
                }
            ],
            "source": "score = model_1.evaluate(test_images, test_labels, verbose=0)\nprint('model_1 test loss:', score[0])\nprint('model_1 test accuracy', score[1])"
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [],
            "source": "# model_1_1 def\n\nmodel_1_1 = tf.keras.Sequential([\n    tf.keras.layers.Dense(128, activation=tf.nn.relu),\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Dense(128, activation=tf.nn.relu),    \n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n])"
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [],
            "source": "# model_1_1 compile\nmodel_1_1.compile(optimizer='adam',\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])"
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Epoch 1/20\n1500/1500 [==============================] - 5s 3ms/step - loss: 0.7687 - accuracy: 0.7554 - val_loss: 0.2541 - val_accuracy: 0.9207\nEpoch 2/20\n1500/1500 [==============================] - 4s 3ms/step - loss: 0.2931 - accuracy: 0.9109 - val_loss: 0.2060 - val_accuracy: 0.9377\nEpoch 3/20\n1500/1500 [==============================] - 4s 3ms/step - loss: 0.2234 - accuracy: 0.9312 - val_loss: 0.1899 - val_accuracy: 0.9438\nEpoch 4/20\n1500/1500 [==============================] - 4s 3ms/step - loss: 0.1942 - accuracy: 0.9399 - val_loss: 0.1709 - val_accuracy: 0.9500\nEpoch 5/20\n1500/1500 [==============================] - 4s 3ms/step - loss: 0.1658 - accuracy: 0.9476 - val_loss: 0.1640 - val_accuracy: 0.9516\nEpoch 6/20\n1500/1500 [==============================] - 4s 3ms/step - loss: 0.1471 - accuracy: 0.9530 - val_loss: 0.1629 - val_accuracy: 0.9506\nEpoch 7/20\n1500/1500 [==============================] - 4s 3ms/step - loss: 0.1370 - accuracy: 0.9550 - val_loss: 0.1594 - val_accuracy: 0.9523\nEpoch 8/20\n1500/1500 [==============================] - 4s 3ms/step - loss: 0.1305 - accuracy: 0.9573 - val_loss: 0.1560 - val_accuracy: 0.9546\nEpoch 9/20\n1500/1500 [==============================] - 4s 3ms/step - loss: 0.1147 - accuracy: 0.9626 - val_loss: 0.1555 - val_accuracy: 0.9557\nEpoch 10/20\n1500/1500 [==============================] - 4s 3ms/step - loss: 0.1148 - accuracy: 0.9625 - val_loss: 0.1587 - val_accuracy: 0.9564\nEpoch 11/20\n1500/1500 [==============================] - 4s 3ms/step - loss: 0.1077 - accuracy: 0.9652 - val_loss: 0.1589 - val_accuracy: 0.9567\nEpoch 12/20\n1500/1500 [==============================] - 4s 3ms/step - loss: 0.0994 - accuracy: 0.9674 - val_loss: 0.1562 - val_accuracy: 0.9567\nEpoch 13/20\n1500/1500 [==============================] - 4s 3ms/step - loss: 0.0948 - accuracy: 0.9696 - val_loss: 0.1579 - val_accuracy: 0.9603\nEpoch 14/20\n1500/1500 [==============================] - 4s 3ms/step - loss: 0.0900 - accuracy: 0.9706 - val_loss: 0.1641 - val_accuracy: 0.9566\nEpoch 15/20\n1500/1500 [==============================] - 4s 3ms/step - loss: 0.0876 - accuracy: 0.9702 - val_loss: 0.1564 - val_accuracy: 0.9577\nEpoch 16/20\n1500/1500 [==============================] - 4s 3ms/step - loss: 0.0833 - accuracy: 0.9730 - val_loss: 0.1640 - val_accuracy: 0.9553\nEpoch 17/20\n1500/1500 [==============================] - 4s 3ms/step - loss: 0.0836 - accuracy: 0.9725 - val_loss: 0.1719 - val_accuracy: 0.9576\nEpoch 18/20\n1500/1500 [==============================] - 4s 3ms/step - loss: 0.0854 - accuracy: 0.9731 - val_loss: 0.1657 - val_accuracy: 0.9561\nEpoch 19/20\n1500/1500 [==============================] - 4s 3ms/step - loss: 0.0787 - accuracy: 0.9752 - val_loss: 0.1733 - val_accuracy: 0.9575\nEpoch 20/20\n1500/1500 [==============================] - 4s 3ms/step - loss: 0.0762 - accuracy: 0.9746 - val_loss: 0.1756 - val_accuracy: 0.9564\n"
                },
                {
                    "data": {
                        "text/plain": "<tensorflow.python.keras.callbacks.History at 0x7f8bec01ae10>"
                    },
                    "execution_count": 14,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "# model_1_1 fit\nmodel_1_1.fit(train_images, train_labels, epochs=20, verbose=1, validation_data=(val_images, val_labels))"
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Model: \"sequential_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense_2 (Dense)              (32, 128)                 100480    \n_________________________________________________________________\ndropout (Dropout)            (32, 128)                 0         \n_________________________________________________________________\ndense_3 (Dense)              (32, 128)                 16512     \n_________________________________________________________________\ndropout_1 (Dropout)          (32, 128)                 0         \n_________________________________________________________________\ndense_4 (Dense)              (32, 10)                  1290      \n=================================================================\nTotal params: 118,282\nTrainable params: 118,282\nNon-trainable params: 0\n_________________________________________________________________\n"
                }
            ],
            "source": "model_1_1.summary()"
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "model_1_1 test loss: 0.4759407341480255\nmodel_1_1 test accuracy 0.892300009727478\n"
                }
            ],
            "source": "score = model_1_1.evaluate(test_images, test_labels, verbose=0)\nprint('model_1_1 test loss:', score[0])\nprint('model_1_1 test accuracy', score[1])"
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {},
            "outputs": [],
            "source": "# model_1_2 def\n\nmodel_1_2 = tf.keras.Sequential([\n    tf.keras.layers.Dense(128, activation=tf.nn.relu),\n    tf.keras.layers.Dense(128, activation=tf.nn.relu),    \n    tf.keras.layers.Dense(128, activation=tf.nn.relu),\n    tf.keras.layers.Dense(128, activation=tf.nn.relu),    \n    tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n])"
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {},
            "outputs": [],
            "source": "# model_1_2 compile\nmodel_1_2.compile(optimizer='adam',\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])"
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Epoch 1/20\n1500/1500 [==============================] - 5s 3ms/step - loss: 0.6492 - accuracy: 0.7954 - val_loss: 0.2544 - val_accuracy: 0.9208\nEpoch 2/20\n1500/1500 [==============================] - 4s 3ms/step - loss: 0.1985 - accuracy: 0.9398 - val_loss: 0.1897 - val_accuracy: 0.9441\nEpoch 3/20\n1500/1500 [==============================] - 4s 3ms/step - loss: 0.1380 - accuracy: 0.9580 - val_loss: 0.1827 - val_accuracy: 0.9473\nEpoch 4/20\n1500/1500 [==============================] - 4s 3ms/step - loss: 0.1004 - accuracy: 0.9685 - val_loss: 0.1801 - val_accuracy: 0.9492\nEpoch 5/20\n1500/1500 [==============================] - 4s 3ms/step - loss: 0.0796 - accuracy: 0.9746 - val_loss: 0.2108 - val_accuracy: 0.9409\nEpoch 6/20\n1500/1500 [==============================] - 4s 3ms/step - loss: 0.0682 - accuracy: 0.9793 - val_loss: 0.1901 - val_accuracy: 0.9522\nEpoch 7/20\n1500/1500 [==============================] - 4s 3ms/step - loss: 0.0528 - accuracy: 0.9840 - val_loss: 0.1893 - val_accuracy: 0.9535\nEpoch 8/20\n1500/1500 [==============================] - 4s 3ms/step - loss: 0.0491 - accuracy: 0.9842 - val_loss: 0.2154 - val_accuracy: 0.9435\nEpoch 9/20\n1500/1500 [==============================] - 4s 3ms/step - loss: 0.0423 - accuracy: 0.9860 - val_loss: 0.2152 - val_accuracy: 0.9531\nEpoch 10/20\n1500/1500 [==============================] - 4s 3ms/step - loss: 0.0387 - accuracy: 0.9878 - val_loss: 0.2100 - val_accuracy: 0.9503\nEpoch 11/20\n1500/1500 [==============================] - 4s 3ms/step - loss: 0.0322 - accuracy: 0.9893 - val_loss: 0.1890 - val_accuracy: 0.9572\nEpoch 12/20\n1500/1500 [==============================] - 4s 3ms/step - loss: 0.0261 - accuracy: 0.9920 - val_loss: 0.2301 - val_accuracy: 0.9498\nEpoch 13/20\n1500/1500 [==============================] - 5s 3ms/step - loss: 0.0298 - accuracy: 0.9909 - val_loss: 0.1963 - val_accuracy: 0.9559\nEpoch 14/20\n1500/1500 [==============================] - 4s 3ms/step - loss: 0.0222 - accuracy: 0.9931 - val_loss: 0.2217 - val_accuracy: 0.9537\nEpoch 15/20\n1500/1500 [==============================] - 4s 3ms/step - loss: 0.0288 - accuracy: 0.9916 - val_loss: 0.2165 - val_accuracy: 0.9560\nEpoch 16/20\n1500/1500 [==============================] - 4s 3ms/step - loss: 0.0243 - accuracy: 0.9927 - val_loss: 0.2193 - val_accuracy: 0.9582\nEpoch 17/20\n1500/1500 [==============================] - 4s 3ms/step - loss: 0.0197 - accuracy: 0.9943 - val_loss: 0.2345 - val_accuracy: 0.9562\nEpoch 18/20\n1500/1500 [==============================] - 4s 3ms/step - loss: 0.0225 - accuracy: 0.9937 - val_loss: 0.2160 - val_accuracy: 0.9562\nEpoch 19/20\n1500/1500 [==============================] - 4s 3ms/step - loss: 0.0169 - accuracy: 0.9947 - val_loss: 0.2409 - val_accuracy: 0.9540\nEpoch 20/20\n1500/1500 [==============================] - 5s 3ms/step - loss: 0.0242 - accuracy: 0.9928 - val_loss: 0.2686 - val_accuracy: 0.9561\n"
                },
                {
                    "data": {
                        "text/plain": "<tensorflow.python.keras.callbacks.History at 0x7f8b4c61ee50>"
                    },
                    "execution_count": 19,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "# model_1_2 fit\nmodel_1_2.fit(train_images, train_labels, epochs=20, verbose=1, validation_data=(val_images, val_labels))"
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Model: \"sequential_2\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense_5 (Dense)              (32, 128)                 100480    \n_________________________________________________________________\ndense_6 (Dense)              (32, 128)                 16512     \n_________________________________________________________________\ndense_7 (Dense)              (32, 128)                 16512     \n_________________________________________________________________\ndense_8 (Dense)              (32, 128)                 16512     \n_________________________________________________________________\ndense_9 (Dense)              (32, 10)                  1290      \n=================================================================\nTotal params: 151,306\nTrainable params: 151,306\nNon-trainable params: 0\n_________________________________________________________________\n"
                }
            ],
            "source": "model_1_2.summary()"
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "model_1_2 test loss: 0.755105197429657\nmodel_1_2 test accuracy 0.8855000138282776\n"
                }
            ],
            "source": "score = model_1_2.evaluate(test_images, test_labels, verbose=0)\nprint('model_1_2 test loss:', score[0])\nprint('model_1_2 test accuracy', score[1])"
        },
        {
            "cell_type": "code",
            "execution_count": 22,
            "metadata": {},
            "outputs": [],
            "source": "# model_1_3 def\n\nmodel_1_3 = tf.keras.Sequential([\n    tf.keras.layers.Dense(128, activation=tf.nn.relu),\n    tf.keras.layers.Dense(128, activation=tf.nn.relu),    \n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Dense(128, activation=tf.nn.relu),\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Dense(128, activation=tf.nn.relu),    \n    tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n])"
        },
        {
            "cell_type": "code",
            "execution_count": 23,
            "metadata": {},
            "outputs": [],
            "source": "# model_1_3 compile\nmodel_1_3.compile(optimizer='adam',\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])"
        },
        {
            "cell_type": "code",
            "execution_count": 24,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Epoch 1/20\n1500/1500 [==============================] - 6s 3ms/step - loss: 0.7879 - accuracy: 0.7441 - val_loss: 0.2740 - val_accuracy: 0.9162\nEpoch 2/20\n1500/1500 [==============================] - 5s 3ms/step - loss: 0.2543 - accuracy: 0.9245 - val_loss: 0.2046 - val_accuracy: 0.9371\nEpoch 3/20\n1500/1500 [==============================] - 5s 3ms/step - loss: 0.1856 - accuracy: 0.9452 - val_loss: 0.1845 - val_accuracy: 0.9461\nEpoch 4/20\n1500/1500 [==============================] - 5s 3ms/step - loss: 0.1473 - accuracy: 0.9556 - val_loss: 0.1837 - val_accuracy: 0.9461\nEpoch 5/20\n1500/1500 [==============================] - 5s 3ms/step - loss: 0.1192 - accuracy: 0.9650 - val_loss: 0.2088 - val_accuracy: 0.9425\nEpoch 6/20\n1500/1500 [==============================] - 5s 3ms/step - loss: 0.1028 - accuracy: 0.9686 - val_loss: 0.1925 - val_accuracy: 0.9503\nEpoch 7/20\n1500/1500 [==============================] - 5s 3ms/step - loss: 0.0899 - accuracy: 0.9716 - val_loss: 0.1894 - val_accuracy: 0.9501\nEpoch 8/20\n1500/1500 [==============================] - 5s 3ms/step - loss: 0.0830 - accuracy: 0.9753 - val_loss: 0.1921 - val_accuracy: 0.9518\nEpoch 9/20\n1500/1500 [==============================] - 5s 3ms/step - loss: 0.0715 - accuracy: 0.9787 - val_loss: 0.1761 - val_accuracy: 0.9563\nEpoch 10/20\n1500/1500 [==============================] - 5s 3ms/step - loss: 0.0605 - accuracy: 0.9815 - val_loss: 0.1854 - val_accuracy: 0.9558\nEpoch 11/20\n1500/1500 [==============================] - 5s 3ms/step - loss: 0.0552 - accuracy: 0.9832 - val_loss: 0.1928 - val_accuracy: 0.9537\nEpoch 12/20\n1500/1500 [==============================] - 5s 3ms/step - loss: 0.0486 - accuracy: 0.9843 - val_loss: 0.1858 - val_accuracy: 0.9558\nEpoch 13/20\n1500/1500 [==============================] - 5s 3ms/step - loss: 0.0456 - accuracy: 0.9864 - val_loss: 0.2080 - val_accuracy: 0.9542\nEpoch 14/20\n1500/1500 [==============================] - 5s 3ms/step - loss: 0.0433 - accuracy: 0.9871 - val_loss: 0.2111 - val_accuracy: 0.9553\nEpoch 15/20\n1500/1500 [==============================] - 5s 3ms/step - loss: 0.0406 - accuracy: 0.9881 - val_loss: 0.2001 - val_accuracy: 0.9544\nEpoch 16/20\n1500/1500 [==============================] - 5s 3ms/step - loss: 0.0402 - accuracy: 0.9878 - val_loss: 0.2085 - val_accuracy: 0.9553\nEpoch 17/20\n1500/1500 [==============================] - 5s 3ms/step - loss: 0.0392 - accuracy: 0.9889 - val_loss: 0.2073 - val_accuracy: 0.9555\nEpoch 18/20\n1500/1500 [==============================] - 5s 3ms/step - loss: 0.0375 - accuracy: 0.9892 - val_loss: 0.2270 - val_accuracy: 0.9530\nEpoch 19/20\n1500/1500 [==============================] - 5s 3ms/step - loss: 0.0348 - accuracy: 0.9898 - val_loss: 0.2253 - val_accuracy: 0.9553\nEpoch 20/20\n1500/1500 [==============================] - 5s 3ms/step - loss: 0.0333 - accuracy: 0.9902 - val_loss: 0.2291 - val_accuracy: 0.9526\n"
                },
                {
                    "data": {
                        "text/plain": "<tensorflow.python.keras.callbacks.History at 0x7f8b4c485550>"
                    },
                    "execution_count": 24,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "# model_1_3 fit\nmodel_1_3.fit(train_images, train_labels, epochs=20, verbose=1, validation_data=(val_images, val_labels))"
        },
        {
            "cell_type": "code",
            "execution_count": 25,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "model_1_3 test loss: 0.5735206007957458\nmodel_1_3 test accuracy 0.8925999999046326\n"
                }
            ],
            "source": "score = model_1_3.evaluate(test_images, test_labels, verbose=0)\nprint('model_1_3 test loss:', score[0])\nprint('model_1_3 test accuracy', score[1])"
        },
        {
            "cell_type": "code",
            "execution_count": 26,
            "metadata": {},
            "outputs": [],
            "source": "# model_1_4 def\n\nmodel_1_4 = tf.keras.Sequential([\n    tf.keras.layers.Dense(128, activation=tf.nn.relu),\n    tf.keras.layers.Dense(128, activation=tf.nn.relu),    \n    tf.keras.layers.Dense(128, activation=tf.nn.relu),\n    tf.keras.layers.Dense(128, activation=tf.nn.relu),    \n    tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n])"
        },
        {
            "cell_type": "code",
            "execution_count": 27,
            "metadata": {},
            "outputs": [],
            "source": "# model_1_4 compile\n\nmodel_1_4.compile(optimizer=tf.keras.optimizers.Adadelta(),\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])"
        },
        {
            "cell_type": "code",
            "execution_count": 28,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Epoch 1/20\n1500/1500 [==============================] - 5s 3ms/step - loss: 2.2875 - accuracy: 0.1197 - val_loss: 2.2531 - val_accuracy: 0.2028\nEpoch 2/20\n1500/1500 [==============================] - 5s 3ms/step - loss: 2.2394 - accuracy: 0.2291 - val_loss: 2.2018 - val_accuracy: 0.2755\nEpoch 3/20\n1500/1500 [==============================] - 5s 3ms/step - loss: 2.1866 - accuracy: 0.2825 - val_loss: 2.1404 - val_accuracy: 0.3061\nEpoch 4/20\n1500/1500 [==============================] - 5s 3ms/step - loss: 2.1215 - accuracy: 0.3168 - val_loss: 2.0678 - val_accuracy: 0.3363\nEpoch 5/20\n1500/1500 [==============================] - 5s 3ms/step - loss: 2.0486 - accuracy: 0.3438 - val_loss: 1.9851 - val_accuracy: 0.3720\nEpoch 6/20\n1500/1500 [==============================] - 5s 3ms/step - loss: 1.9689 - accuracy: 0.3777 - val_loss: 1.8948 - val_accuracy: 0.4141\nEpoch 7/20\n1500/1500 [==============================] - 5s 3ms/step - loss: 1.8720 - accuracy: 0.4223 - val_loss: 1.7987 - val_accuracy: 0.4588\nEpoch 8/20\n1500/1500 [==============================] - 5s 3ms/step - loss: 1.7782 - accuracy: 0.4630 - val_loss: 1.7000 - val_accuracy: 0.5012\nEpoch 9/20\n1500/1500 [==============================] - 5s 3ms/step - loss: 1.6823 - accuracy: 0.5089 - val_loss: 1.6020 - val_accuracy: 0.5485\nEpoch 10/20\n1500/1500 [==============================] - 5s 3ms/step - loss: 1.5855 - accuracy: 0.5533 - val_loss: 1.5082 - val_accuracy: 0.5868\nEpoch 11/20\n1500/1500 [==============================] - 5s 3ms/step - loss: 1.4898 - accuracy: 0.5959 - val_loss: 1.4217 - val_accuracy: 0.6248\nEpoch 12/20\n1500/1500 [==============================] - 5s 3ms/step - loss: 1.4048 - accuracy: 0.6307 - val_loss: 1.3444 - val_accuracy: 0.6525\nEpoch 13/20\n1500/1500 [==============================] - 5s 3ms/step - loss: 1.3348 - accuracy: 0.6530 - val_loss: 1.2760 - val_accuracy: 0.6718\nEpoch 14/20\n1500/1500 [==============================] - 5s 3ms/step - loss: 1.2635 - accuracy: 0.6716 - val_loss: 1.2155 - val_accuracy: 0.6873\nEpoch 15/20\n1500/1500 [==============================] - 5s 3ms/step - loss: 1.2143 - accuracy: 0.6811 - val_loss: 1.1620 - val_accuracy: 0.6962\nEpoch 16/20\n1500/1500 [==============================] - 5s 3ms/step - loss: 1.1566 - accuracy: 0.6945 - val_loss: 1.1148 - val_accuracy: 0.7038\nEpoch 17/20\n1500/1500 [==============================] - 5s 3ms/step - loss: 1.1092 - accuracy: 0.7017 - val_loss: 1.0730 - val_accuracy: 0.7105\nEpoch 18/20\n1500/1500 [==============================] - 5s 3ms/step - loss: 1.0700 - accuracy: 0.7093 - val_loss: 1.0355 - val_accuracy: 0.7184\nEpoch 19/20\n1500/1500 [==============================] - 5s 3ms/step - loss: 1.0329 - accuracy: 0.7139 - val_loss: 1.0019 - val_accuracy: 0.7244\nEpoch 20/20\n1500/1500 [==============================] - 5s 3ms/step - loss: 1.0010 - accuracy: 0.7211 - val_loss: 0.9715 - val_accuracy: 0.7300\n"
                },
                {
                    "data": {
                        "text/plain": "<tensorflow.python.keras.callbacks.History at 0x7f8b4c32f110>"
                    },
                    "execution_count": 28,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "# model_1_4 fit\n\nmodel_1_4.fit(train_images, train_labels, epochs=20, verbose=1, validation_data=(val_images, val_labels))"
        },
        {
            "cell_type": "code",
            "execution_count": 29,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "model_1_4 test loss: 1.3908430337905884\nmodel_1_4 test accuracy 0.569100022315979\n"
                }
            ],
            "source": "score = model_1_4.evaluate(test_images, test_labels, verbose=0)\nprint('model_1_4 test loss:', score[0])\nprint('model_1_4 test accuracy', score[1])"
        },
        {
            "cell_type": "code",
            "execution_count": 30,
            "metadata": {},
            "outputs": [],
            "source": "# model_1_4_1 def (using the dropouts with adadelta)\n\nmodel_1_4_1 = tf.keras.Sequential([\n    tf.keras.layers.Dense(128, activation=tf.nn.relu),\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Dense(128, activation=tf.nn.relu),    \n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n])"
        },
        {
            "cell_type": "code",
            "execution_count": 31,
            "metadata": {},
            "outputs": [],
            "source": "# model_1_4_1 compile\n\nmodel_1_4_1.compile(optimizer=tf.keras.optimizers.Adadelta(learning_rate = 1.0),\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])"
        },
        {
            "cell_type": "code",
            "execution_count": 32,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Epoch 1/20\n1500/1500 [==============================] - 5s 3ms/step - loss: 0.7651 - accuracy: 0.7566 - val_loss: 0.2681 - val_accuracy: 0.9169\nEpoch 2/20\n1500/1500 [==============================] - 4s 3ms/step - loss: 0.3004 - accuracy: 0.9087 - val_loss: 0.2053 - val_accuracy: 0.9390\nEpoch 3/20\n1500/1500 [==============================] - 4s 3ms/step - loss: 0.2459 - accuracy: 0.9257 - val_loss: 0.1834 - val_accuracy: 0.9461\nEpoch 4/20\n1500/1500 [==============================] - 4s 3ms/step - loss: 0.1993 - accuracy: 0.9396 - val_loss: 0.1820 - val_accuracy: 0.9464\nEpoch 5/20\n1500/1500 [==============================] - 4s 3ms/step - loss: 0.1822 - accuracy: 0.9451 - val_loss: 0.1738 - val_accuracy: 0.9496\nEpoch 6/20\n1500/1500 [==============================] - 4s 3ms/step - loss: 0.1671 - accuracy: 0.9487 - val_loss: 0.1756 - val_accuracy: 0.9494\nEpoch 7/20\n1500/1500 [==============================] - 4s 3ms/step - loss: 0.1532 - accuracy: 0.9530 - val_loss: 0.1665 - val_accuracy: 0.9517\nEpoch 8/20\n1500/1500 [==============================] - 4s 3ms/step - loss: 0.1395 - accuracy: 0.9578 - val_loss: 0.1655 - val_accuracy: 0.9517\nEpoch 9/20\n1500/1500 [==============================] - 4s 3ms/step - loss: 0.1326 - accuracy: 0.9595 - val_loss: 0.1688 - val_accuracy: 0.9527\nEpoch 10/20\n1500/1500 [==============================] - 4s 3ms/step - loss: 0.1244 - accuracy: 0.9616 - val_loss: 0.1643 - val_accuracy: 0.9542\nEpoch 11/20\n1500/1500 [==============================] - 4s 3ms/step - loss: 0.1136 - accuracy: 0.9651 - val_loss: 0.1665 - val_accuracy: 0.9553\nEpoch 12/20\n1500/1500 [==============================] - 4s 3ms/step - loss: 0.1093 - accuracy: 0.9673 - val_loss: 0.1695 - val_accuracy: 0.9551\nEpoch 13/20\n1500/1500 [==============================] - 4s 3ms/step - loss: 0.1061 - accuracy: 0.9664 - val_loss: 0.1715 - val_accuracy: 0.9564\nEpoch 14/20\n1500/1500 [==============================] - 4s 3ms/step - loss: 0.0990 - accuracy: 0.9698 - val_loss: 0.1793 - val_accuracy: 0.9538\nEpoch 15/20\n1500/1500 [==============================] - 4s 3ms/step - loss: 0.1026 - accuracy: 0.9695 - val_loss: 0.1760 - val_accuracy: 0.9546\nEpoch 16/20\n1500/1500 [==============================] - 4s 3ms/step - loss: 0.0984 - accuracy: 0.9705 - val_loss: 0.1815 - val_accuracy: 0.9545\nEpoch 17/20\n1500/1500 [==============================] - 4s 3ms/step - loss: 0.0881 - accuracy: 0.9723 - val_loss: 0.1808 - val_accuracy: 0.9554\nEpoch 18/20\n1500/1500 [==============================] - 4s 3ms/step - loss: 0.0852 - accuracy: 0.9732 - val_loss: 0.1832 - val_accuracy: 0.9548\nEpoch 19/20\n1500/1500 [==============================] - 4s 3ms/step - loss: 0.0854 - accuracy: 0.9746 - val_loss: 0.1895 - val_accuracy: 0.9542\nEpoch 20/20\n1500/1500 [==============================] - 4s 3ms/step - loss: 0.0889 - accuracy: 0.9737 - val_loss: 0.1910 - val_accuracy: 0.9547\n"
                },
                {
                    "data": {
                        "text/plain": "<tensorflow.python.keras.callbacks.History at 0x7f8b4c106e90>"
                    },
                    "execution_count": 32,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "# model_1_4_1 fit\n\nmodel_1_4_1.fit(train_images, train_labels, epochs=20, verbose=1, validation_data=(val_images, val_labels))"
        },
        {
            "cell_type": "code",
            "execution_count": 33,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "model_1_4_1 test loss: 0.5012156367301941\nmodel_1_4_1 test accuracy 0.8932999968528748\n"
                }
            ],
            "source": "score = model_1_4_1.evaluate(test_images, test_labels, verbose=0)\nprint('model_1_4_1 test loss:', score[0])\nprint('model_1_4_1 test accuracy', score[1])"
        },
        {
            "cell_type": "code",
            "execution_count": 34,
            "metadata": {},
            "outputs": [],
            "source": "# model_1_5 def\n\nmodel_1_5 = tf.keras.Sequential([\n    tf.keras.layers.Dense(128, activation=tf.nn.relu),\n    tf.keras.layers.Dense(128, activation=tf.nn.relu),    \n    tf.keras.layers.Dense(128, activation=tf.nn.relu),\n    tf.keras.layers.Dense(128, activation=tf.nn.relu),    \n    tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n])"
        },
        {
            "cell_type": "code",
            "execution_count": 35,
            "metadata": {},
            "outputs": [],
            "source": "# model_1_5 compile\n\nmodel_1_5.compile(optimizer='sgd',\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])"
        },
        {
            "cell_type": "code",
            "execution_count": 36,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Epoch 1/20\n1500/1500 [==============================] - 5s 3ms/step - loss: 1.6107 - accuracy: 0.4708 - val_loss: 0.5670 - val_accuracy: 0.8272\nEpoch 2/20\n1500/1500 [==============================] - 4s 3ms/step - loss: 0.5150 - accuracy: 0.8446 - val_loss: 0.3987 - val_accuracy: 0.8789\nEpoch 3/20\n1500/1500 [==============================] - 4s 3ms/step - loss: 0.3817 - accuracy: 0.8859 - val_loss: 0.3519 - val_accuracy: 0.8922\nEpoch 4/20\n1500/1500 [==============================] - 4s 3ms/step - loss: 0.3098 - accuracy: 0.9067 - val_loss: 0.2914 - val_accuracy: 0.9107\nEpoch 5/20\n1500/1500 [==============================] - 4s 3ms/step - loss: 0.2620 - accuracy: 0.9237 - val_loss: 0.2657 - val_accuracy: 0.9185\nEpoch 6/20\n1500/1500 [==============================] - 4s 3ms/step - loss: 0.2283 - accuracy: 0.9322 - val_loss: 0.2466 - val_accuracy: 0.9245\nEpoch 7/20\n1500/1500 [==============================] - 4s 3ms/step - loss: 0.1901 - accuracy: 0.9446 - val_loss: 0.2382 - val_accuracy: 0.9275\nEpoch 8/20\n1500/1500 [==============================] - 4s 3ms/step - loss: 0.1706 - accuracy: 0.9514 - val_loss: 0.2213 - val_accuracy: 0.9302\nEpoch 9/20\n1500/1500 [==============================] - 4s 3ms/step - loss: 0.1519 - accuracy: 0.9572 - val_loss: 0.2184 - val_accuracy: 0.9318\nEpoch 10/20\n1500/1500 [==============================] - 4s 3ms/step - loss: 0.1377 - accuracy: 0.9610 - val_loss: 0.2061 - val_accuracy: 0.9353\nEpoch 11/20\n1500/1500 [==============================] - 4s 3ms/step - loss: 0.1185 - accuracy: 0.9659 - val_loss: 0.2008 - val_accuracy: 0.9383\nEpoch 12/20\n1500/1500 [==============================] - 4s 3ms/step - loss: 0.1036 - accuracy: 0.9706 - val_loss: 0.2023 - val_accuracy: 0.9395\nEpoch 13/20\n1500/1500 [==============================] - 4s 3ms/step - loss: 0.0887 - accuracy: 0.9758 - val_loss: 0.2140 - val_accuracy: 0.9344\nEpoch 14/20\n1500/1500 [==============================] - 4s 3ms/step - loss: 0.0825 - accuracy: 0.9783 - val_loss: 0.1995 - val_accuracy: 0.9416\nEpoch 15/20\n1500/1500 [==============================] - 4s 3ms/step - loss: 0.0717 - accuracy: 0.9808 - val_loss: 0.2024 - val_accuracy: 0.9433\nEpoch 16/20\n1500/1500 [==============================] - 4s 3ms/step - loss: 0.0629 - accuracy: 0.9829 - val_loss: 0.2023 - val_accuracy: 0.9423\nEpoch 17/20\n1500/1500 [==============================] - 4s 3ms/step - loss: 0.0561 - accuracy: 0.9855 - val_loss: 0.2019 - val_accuracy: 0.9432\nEpoch 18/20\n1500/1500 [==============================] - 4s 3ms/step - loss: 0.0490 - accuracy: 0.9880 - val_loss: 0.2270 - val_accuracy: 0.9368\nEpoch 19/20\n1500/1500 [==============================] - 4s 3ms/step - loss: 0.0420 - accuracy: 0.9895 - val_loss: 0.2091 - val_accuracy: 0.9430\nEpoch 20/20\n1500/1500 [==============================] - 4s 3ms/step - loss: 0.0331 - accuracy: 0.9927 - val_loss: 0.2169 - val_accuracy: 0.9427\n"
                },
                {
                    "data": {
                        "text/plain": "<tensorflow.python.keras.callbacks.History at 0x7f8b2876a8d0>"
                    },
                    "execution_count": 36,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "# model_1_5 fit\n\nmodel_1_5.fit(train_images, train_labels, epochs=20, verbose=1, validation_data=(val_images, val_labels))"
        },
        {
            "cell_type": "code",
            "execution_count": 37,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "model_1_5 test loss: 0.5508348941802979\nmodel_1_5 test accuracy 0.8748999834060669\n"
                }
            ],
            "source": "score = model_1_5.evaluate(test_images, test_labels, verbose=0)\nprint('model_1_5 test loss:', score[0])\nprint('model_1_5 test accuracy', score[1])"
        },
        {
            "cell_type": "code",
            "execution_count": 38,
            "metadata": {},
            "outputs": [],
            "source": "# model_1_5_1 def\n\nmodel_1_5_1 = tf.keras.Sequential([\n    tf.keras.layers.Dense(128, activation=tf.nn.relu),\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Dense(128, activation=tf.nn.relu),    \n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n])"
        },
        {
            "cell_type": "code",
            "execution_count": 39,
            "metadata": {},
            "outputs": [],
            "source": "# model_1_5_1 compile\n\nmodel_1_5_1.compile(optimizer='sgd',\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])"
        },
        {
            "cell_type": "code",
            "execution_count": 40,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Epoch 1/20\n1500/1500 [==============================] - 4s 3ms/step - loss: 1.5708 - accuracy: 0.4783 - val_loss: 0.6132 - val_accuracy: 0.8180\nEpoch 2/20\n1500/1500 [==============================] - 4s 3ms/step - loss: 0.7001 - accuracy: 0.7861 - val_loss: 0.4537 - val_accuracy: 0.8627\nEpoch 3/20\n1500/1500 [==============================] - 4s 3ms/step - loss: 0.5592 - accuracy: 0.8266 - val_loss: 0.3842 - val_accuracy: 0.8829\nEpoch 4/20\n1500/1500 [==============================] - 4s 3ms/step - loss: 0.4809 - accuracy: 0.8525 - val_loss: 0.3433 - val_accuracy: 0.8952\nEpoch 5/20\n1500/1500 [==============================] - 4s 3ms/step - loss: 0.4172 - accuracy: 0.8721 - val_loss: 0.3153 - val_accuracy: 0.9029\nEpoch 6/20\n1500/1500 [==============================] - 4s 3ms/step - loss: 0.3860 - accuracy: 0.8821 - val_loss: 0.2926 - val_accuracy: 0.9091\nEpoch 7/20\n1500/1500 [==============================] - 4s 3ms/step - loss: 0.3527 - accuracy: 0.8936 - val_loss: 0.2757 - val_accuracy: 0.9158\nEpoch 8/20\n1500/1500 [==============================] - 4s 3ms/step - loss: 0.3319 - accuracy: 0.8997 - val_loss: 0.2604 - val_accuracy: 0.9197\nEpoch 9/20\n1500/1500 [==============================] - 4s 3ms/step - loss: 0.3137 - accuracy: 0.9049 - val_loss: 0.2462 - val_accuracy: 0.9251\nEpoch 10/20\n1500/1500 [==============================] - 4s 3ms/step - loss: 0.3033 - accuracy: 0.9078 - val_loss: 0.2358 - val_accuracy: 0.9291\nEpoch 11/20\n1500/1500 [==============================] - 4s 3ms/step - loss: 0.2798 - accuracy: 0.9153 - val_loss: 0.2280 - val_accuracy: 0.9304\nEpoch 12/20\n1500/1500 [==============================] - 4s 3ms/step - loss: 0.2696 - accuracy: 0.9188 - val_loss: 0.2217 - val_accuracy: 0.9326\nEpoch 13/20\n1500/1500 [==============================] - 4s 3ms/step - loss: 0.2602 - accuracy: 0.9202 - val_loss: 0.2127 - val_accuracy: 0.9349\nEpoch 14/20\n1500/1500 [==============================] - 4s 3ms/step - loss: 0.2503 - accuracy: 0.9234 - val_loss: 0.2073 - val_accuracy: 0.9367\nEpoch 15/20\n1500/1500 [==============================] - 4s 3ms/step - loss: 0.2397 - accuracy: 0.9271 - val_loss: 0.2032 - val_accuracy: 0.9379\nEpoch 16/20\n1500/1500 [==============================] - 4s 3ms/step - loss: 0.2297 - accuracy: 0.9307 - val_loss: 0.1981 - val_accuracy: 0.9394\nEpoch 17/20\n1500/1500 [==============================] - 4s 3ms/step - loss: 0.2239 - accuracy: 0.9303 - val_loss: 0.1954 - val_accuracy: 0.9402\nEpoch 18/20\n1500/1500 [==============================] - 4s 3ms/step - loss: 0.2167 - accuracy: 0.9338 - val_loss: 0.1913 - val_accuracy: 0.9423\nEpoch 19/20\n1500/1500 [==============================] - 4s 3ms/step - loss: 0.2091 - accuracy: 0.9370 - val_loss: 0.1870 - val_accuracy: 0.9436\nEpoch 20/20\n1500/1500 [==============================] - 4s 3ms/step - loss: 0.1988 - accuracy: 0.9381 - val_loss: 0.1849 - val_accuracy: 0.9438\n"
                },
                {
                    "data": {
                        "text/plain": "<tensorflow.python.keras.callbacks.History at 0x7f8b28665f90>"
                    },
                    "execution_count": 40,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "# model_1_5_1 fit\n\nmodel_1_5_1.fit(train_images, train_labels, epochs=20, verbose=1, validation_data=(val_images, val_labels))"
        },
        {
            "cell_type": "code",
            "execution_count": 41,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "model_1_5_1 test loss: 0.4522767663002014\nmodel_1_5_1 test accuracy 0.8659999966621399\n"
                }
            ],
            "source": "score = model_1_5_1.evaluate(test_images, test_labels, verbose=0)\nprint('model_1_5_1 test loss:', score[0])\nprint('model_1_5_1 test accuracy', score[1])"
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [],
            "source": "# model_2 def\n\n# optimizer is adam and loss is sparse_categorical_crossentropy\n\nbatch_size = 128\nnum_classes = 10\nepochs = 12\n\n# input image dimensions\nimg_rows, img_cols = 28, 28\n\nx_train = train_images\nx_val = val_images\nx_test = test_images\n\ny_train = train_labels\ny_val = val_labels\ny_test = test_labels\n\nif K.image_data_format() == 'channels_first':\n    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n    x_val = x_val.reshape(x_val.shape[0], 1, img_rows, img_cols)\n    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n    input_shape = (1, img_rows, img_cols)\nelse:\n    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n    x_val = x_val.reshape(x_val.shape[0], img_rows, img_cols, 1)\n    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n    input_shape = (img_rows, img_cols, 1)\n\nmodel_2 = tf.keras.Sequential()\nmodel_2.add(Conv2D(32, kernel_size=(3, 3),\n                 activation='relu',\n                 input_shape=input_shape))\nmodel_2.add(Conv2D(64, (3, 3), activation='relu'))\nmodel_2.add(MaxPooling2D(pool_size=(2, 2)))\nmodel_2.add(Dropout(0.25))\nmodel_2.add(tf.keras.layers.Flatten())\nmodel_2.add(tf.keras.layers.Dense(128, activation='relu'))\nmodel_2.add(Dropout(0.5))\nmodel_2.add(tf.keras.layers.Dense(num_classes, activation='softmax'))\n\n# model_2 compile\n\nmodel_2.compile(optimizer='adam', \n          loss='sparse_categorical_crossentropy',\n          metrics=['accuracy'])"
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Epoch 1/12\n375/375 [==============================] - 16s 23ms/step - loss: 0.7630 - accuracy: 0.7604 - val_loss: 0.1374 - val_accuracy: 0.9601\nEpoch 2/12\n375/375 [==============================] - 5s 13ms/step - loss: 0.2055 - accuracy: 0.9382 - val_loss: 0.0967 - val_accuracy: 0.9712\nEpoch 3/12\n375/375 [==============================] - 5s 13ms/step - loss: 0.1581 - accuracy: 0.9510 - val_loss: 0.0806 - val_accuracy: 0.9747\nEpoch 4/12\n375/375 [==============================] - 5s 13ms/step - loss: 0.1146 - accuracy: 0.9639 - val_loss: 0.0723 - val_accuracy: 0.9786\nEpoch 5/12\n375/375 [==============================] - 5s 13ms/step - loss: 0.0949 - accuracy: 0.9693 - val_loss: 0.0701 - val_accuracy: 0.9797\nEpoch 6/12\n375/375 [==============================] - 5s 13ms/step - loss: 0.0815 - accuracy: 0.9751 - val_loss: 0.0671 - val_accuracy: 0.9811\nEpoch 7/12\n375/375 [==============================] - 5s 13ms/step - loss: 0.0727 - accuracy: 0.9766 - val_loss: 0.0636 - val_accuracy: 0.9830\nEpoch 8/12\n375/375 [==============================] - 5s 13ms/step - loss: 0.0641 - accuracy: 0.9793 - val_loss: 0.0630 - val_accuracy: 0.9824\nEpoch 9/12\n375/375 [==============================] - 5s 13ms/step - loss: 0.0562 - accuracy: 0.9806 - val_loss: 0.0614 - val_accuracy: 0.9831\nEpoch 10/12\n375/375 [==============================] - 5s 13ms/step - loss: 0.0523 - accuracy: 0.9825 - val_loss: 0.0664 - val_accuracy: 0.9828\nEpoch 11/12\n375/375 [==============================] - 5s 13ms/step - loss: 0.0479 - accuracy: 0.9826 - val_loss: 0.0608 - val_accuracy: 0.9849\nEpoch 12/12\n375/375 [==============================] - 5s 13ms/step - loss: 0.0425 - accuracy: 0.9862 - val_loss: 0.0585 - val_accuracy: 0.9850\n"
                },
                {
                    "data": {
                        "text/plain": "<tensorflow.python.keras.callbacks.History at 0x7f3cc3dff550>"
                    },
                    "execution_count": 9,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "# fit model\n\nmodel_2.fit(x_train, y_train,\n         batch_size=batch_size,\n         epochs=epochs,\n         verbose=1,\n         validation_data=(x_val, y_val))"
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "model_2 test loss: 0.217491015791893\nmodel_2 test accuracy 0.9496999979019165\n"
                }
            ],
            "source": "score = model_2.evaluate(x_test, y_test, verbose=0)\nprint('model_2 test loss:', score[0])\nprint('model_2 test accuracy', score[1])"
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [],
            "source": "# model_2_1 def : change of optimizer to Adadelta and compile\n\nmodel_2_1 = tf.keras.Sequential()\nmodel_2_1.add(Conv2D(32, kernel_size=(3, 3),\n                 activation='relu',\n                 input_shape=input_shape))\nmodel_2_1.add(Conv2D(64, (3, 3), activation='relu'))\nmodel_2_1.add(MaxPooling2D(pool_size=(2, 2)))\nmodel_2_1.add(Dropout(0.25))\nmodel_2_1.add(tf.keras.layers.Flatten())\nmodel_2_1.add(tf.keras.layers.Dense(128, activation='relu'))\nmodel_2_1.add(Dropout(0.5))\nmodel_2_1.add(tf.keras.layers.Dense(num_classes, activation='softmax'))\n\nmodel_2_1.compile(loss='sparse_categorical_crossentropy',\n              optimizer=tf.keras.optimizers.Adadelta(learning_rate=1.0),\n              metrics=['accuracy'])"
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Epoch 1/12\n375/375 [==============================] - 6s 14ms/step - loss: 0.8693 - accuracy: 0.7214 - val_loss: 0.1403 - val_accuracy: 0.9569\nEpoch 2/12\n375/375 [==============================] - 5s 13ms/step - loss: 0.2073 - accuracy: 0.9380 - val_loss: 0.0987 - val_accuracy: 0.9693\nEpoch 3/12\n375/375 [==============================] - 5s 13ms/step - loss: 0.1466 - accuracy: 0.9563 - val_loss: 0.0820 - val_accuracy: 0.9747\nEpoch 4/12\n375/375 [==============================] - 5s 13ms/step - loss: 0.1164 - accuracy: 0.9649 - val_loss: 0.0755 - val_accuracy: 0.9781\nEpoch 5/12\n375/375 [==============================] - 5s 13ms/step - loss: 0.0967 - accuracy: 0.9706 - val_loss: 0.0708 - val_accuracy: 0.9788\nEpoch 6/12\n375/375 [==============================] - 5s 13ms/step - loss: 0.0799 - accuracy: 0.9756 - val_loss: 0.0677 - val_accuracy: 0.9806\nEpoch 7/12\n375/375 [==============================] - 5s 13ms/step - loss: 0.0741 - accuracy: 0.9764 - val_loss: 0.0664 - val_accuracy: 0.9814\nEpoch 8/12\n375/375 [==============================] - 5s 13ms/step - loss: 0.0633 - accuracy: 0.9799 - val_loss: 0.0649 - val_accuracy: 0.9821\nEpoch 9/12\n375/375 [==============================] - 5s 13ms/step - loss: 0.0636 - accuracy: 0.9813 - val_loss: 0.0618 - val_accuracy: 0.9822\nEpoch 10/12\n375/375 [==============================] - 5s 13ms/step - loss: 0.0598 - accuracy: 0.9813 - val_loss: 0.0633 - val_accuracy: 0.9824\nEpoch 11/12\n375/375 [==============================] - 5s 13ms/step - loss: 0.0527 - accuracy: 0.9823 - val_loss: 0.0609 - val_accuracy: 0.9834\nEpoch 12/12\n375/375 [==============================] - 5s 13ms/step - loss: 0.0544 - accuracy: 0.9821 - val_loss: 0.0626 - val_accuracy: 0.9834\n"
                },
                {
                    "data": {
                        "text/plain": "<tensorflow.python.keras.callbacks.History at 0x7f3ca401d8d0>"
                    },
                    "execution_count": 12,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "model_2_1.fit(x_train, y_train,\n           batch_size=batch_size,\n           epochs=epochs,\n           verbose=1,\n           validation_data=(x_val, y_val))"
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "model_2_1 test loss: 0.2383953332901001\nmodel_2_1 test accuracy 0.9462000131607056\n"
                }
            ],
            "source": "score = model_2_1.evaluate(x_test, y_test, verbose=0)\nprint('model_2_1 test loss:', score[0])\nprint('model_2_1 test accuracy', score[1])"
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [],
            "source": "# optimizer = Adamax\n\n# model_2_2: change of optimizer to Adamax and compile\n\nmodel_2_2 = tf.keras.Sequential()\nmodel_2_2.add(Conv2D(32, kernel_size=(3, 3),\n                 activation='relu',\n                 input_shape=input_shape))\nmodel_2_2.add(Conv2D(64, (3, 3), activation='relu'))\nmodel_2_2.add(MaxPooling2D(pool_size=(2, 2)))\nmodel_2_2.add(Dropout(0.25))\nmodel_2_2.add(tf.keras.layers.Flatten())\nmodel_2_2.add(tf.keras.layers.Dense(128, activation='relu'))\nmodel_2_2.add(Dropout(0.5))\nmodel_2_2.add(tf.keras.layers.Dense(num_classes, activation='softmax'))\n\nmodel_2_2.compile(loss='sparse_categorical_crossentropy',\n              optimizer='Adamax',\n              metrics=['accuracy'])"
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Epoch 1/12\n375/375 [==============================] - 5s 13ms/step - loss: 0.9304 - accuracy: 0.7049 - val_loss: 0.2315 - val_accuracy: 0.9316\nEpoch 2/12\n375/375 [==============================] - 5s 13ms/step - loss: 0.3261 - accuracy: 0.9033 - val_loss: 0.1543 - val_accuracy: 0.9538\nEpoch 3/12\n375/375 [==============================] - 5s 13ms/step - loss: 0.2308 - accuracy: 0.9309 - val_loss: 0.1251 - val_accuracy: 0.9623\nEpoch 4/12\n375/375 [==============================] - 5s 13ms/step - loss: 0.1893 - accuracy: 0.9443 - val_loss: 0.1105 - val_accuracy: 0.9669\nEpoch 5/12\n375/375 [==============================] - 5s 13ms/step - loss: 0.1597 - accuracy: 0.9523 - val_loss: 0.0961 - val_accuracy: 0.9718\nEpoch 6/12\n375/375 [==============================] - 5s 13ms/step - loss: 0.1382 - accuracy: 0.9576 - val_loss: 0.0914 - val_accuracy: 0.9728\nEpoch 7/12\n375/375 [==============================] - 5s 13ms/step - loss: 0.1256 - accuracy: 0.9622 - val_loss: 0.0833 - val_accuracy: 0.9763\nEpoch 8/12\n375/375 [==============================] - 5s 13ms/step - loss: 0.1088 - accuracy: 0.9667 - val_loss: 0.0805 - val_accuracy: 0.9763\nEpoch 9/12\n375/375 [==============================] - 5s 13ms/step - loss: 0.1038 - accuracy: 0.9672 - val_loss: 0.0769 - val_accuracy: 0.9778\nEpoch 10/12\n375/375 [==============================] - 5s 13ms/step - loss: 0.0949 - accuracy: 0.9713 - val_loss: 0.0741 - val_accuracy: 0.9782\nEpoch 11/12\n375/375 [==============================] - 5s 13ms/step - loss: 0.0892 - accuracy: 0.9729 - val_loss: 0.0721 - val_accuracy: 0.9796\nEpoch 12/12\n375/375 [==============================] - 5s 13ms/step - loss: 0.0802 - accuracy: 0.9743 - val_loss: 0.0711 - val_accuracy: 0.9798\n"
                },
                {
                    "data": {
                        "text/plain": "<tensorflow.python.keras.callbacks.History at 0x7f3c44187ed0>"
                    },
                    "execution_count": 15,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "model_2_2.fit(x_train, y_train,\n           batch_size=batch_size,\n           epochs=epochs,\n           verbose=1,\n           validation_data=(x_val, y_val))"
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "model_2_2 test loss: 0.2569980025291443\nmodel_2_2 test accuracy 0.9320999979972839\n"
                }
            ],
            "source": "score = model_2_2.evaluate(x_test, y_test, verbose=0)\nprint('model_2_2 test loss:', score[0])\nprint('model_2_2 test accuracy', score[1])"
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {},
            "outputs": [],
            "source": "# optimizer = Adagrad\n\n# model_2_3: change of optimizer to Adagrad and compile\n\nmodel_2_3 = tf.keras.Sequential()\nmodel_2_3.add(Conv2D(32, kernel_size=(3, 3),\n                 activation='relu',\n                 input_shape=input_shape))\nmodel_2_3.add(Conv2D(64, (3, 3), activation='relu'))\nmodel_2_3.add(MaxPooling2D(pool_size=(2, 2)))\nmodel_2_3.add(Dropout(0.25))\nmodel_2_3.add(tf.keras.layers.Flatten())\nmodel_2_3.add(tf.keras.layers.Dense(128, activation='relu'))\nmodel_2_3.add(Dropout(0.5))\nmodel_2_3.add(tf.keras.layers.Dense(num_classes, activation='softmax'))\n\nmodel_2_3.compile(loss='sparse_categorical_crossentropy',\n              optimizer='Adagrad',\n              metrics=['accuracy'])"
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Epoch 1/12\n375/375 [==============================] - 6s 13ms/step - loss: 2.2664 - accuracy: 0.1622 - val_loss: 1.8694 - val_accuracy: 0.5909\nEpoch 2/12\n375/375 [==============================] - 5s 13ms/step - loss: 1.6909 - accuracy: 0.4923 - val_loss: 0.9881 - val_accuracy: 0.7302\nEpoch 3/12\n375/375 [==============================] - 5s 13ms/step - loss: 1.1256 - accuracy: 0.6548 - val_loss: 0.7463 - val_accuracy: 0.7890\nEpoch 4/12\n375/375 [==============================] - 5s 13ms/step - loss: 0.9289 - accuracy: 0.7101 - val_loss: 0.6383 - val_accuracy: 0.8159\nEpoch 5/12\n375/375 [==============================] - 5s 13ms/step - loss: 0.8352 - accuracy: 0.7401 - val_loss: 0.5752 - val_accuracy: 0.8342\nEpoch 6/12\n375/375 [==============================] - 5s 13ms/step - loss: 0.7717 - accuracy: 0.7611 - val_loss: 0.5279 - val_accuracy: 0.8455\nEpoch 7/12\n375/375 [==============================] - 5s 13ms/step - loss: 0.7197 - accuracy: 0.7749 - val_loss: 0.4957 - val_accuracy: 0.8547\nEpoch 8/12\n375/375 [==============================] - 5s 13ms/step - loss: 0.6883 - accuracy: 0.7877 - val_loss: 0.4721 - val_accuracy: 0.8609\nEpoch 9/12\n375/375 [==============================] - 5s 13ms/step - loss: 0.6643 - accuracy: 0.7962 - val_loss: 0.4466 - val_accuracy: 0.8695\nEpoch 10/12\n375/375 [==============================] - 5s 13ms/step - loss: 0.6375 - accuracy: 0.8054 - val_loss: 0.4322 - val_accuracy: 0.8746\nEpoch 11/12\n375/375 [==============================] - 5s 13ms/step - loss: 0.6138 - accuracy: 0.8116 - val_loss: 0.4192 - val_accuracy: 0.8789\nEpoch 12/12\n375/375 [==============================] - 5s 13ms/step - loss: 0.6025 - accuracy: 0.8152 - val_loss: 0.4055 - val_accuracy: 0.8813\n"
                },
                {
                    "data": {
                        "text/plain": "<tensorflow.python.keras.callbacks.History at 0x7f3c187f1150>"
                    },
                    "execution_count": 18,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "model_2_3.fit(x_train, y_train,\n           batch_size=batch_size,\n           epochs=epochs,\n           verbose=1,\n           validation_data=(x_val, y_val))"
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "model_2_3 test loss: 0.7908464074134827\nmodel_2_3 test accuracy 0.7494999766349792\n"
                }
            ],
            "source": "score = model_2_3.evaluate(x_test, y_test, verbose=0)\nprint('model_2_3 test loss:', score[0])\nprint('model_2_3 test accuracy', score[1])"
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "metadata": {},
            "outputs": [],
            "source": "# optimizer = Nadam\n\n# model_2_4: change of optimizer to Nadam and compile\n\nmodel_2_4 = tf.keras.Sequential()\nmodel_2_4.add(Conv2D(32, kernel_size=(3, 3),\n                 activation='relu',\n                 input_shape=input_shape))\nmodel_2_4.add(Conv2D(64, (3, 3), activation='relu'))\nmodel_2_4.add(MaxPooling2D(pool_size=(2, 2)))\nmodel_2_4.add(Dropout(0.25))\nmodel_2_4.add(tf.keras.layers.Flatten())\nmodel_2_4.add(tf.keras.layers.Dense(128, activation='relu'))\nmodel_2_4.add(Dropout(0.5))\nmodel_2_4.add(tf.keras.layers.Dense(num_classes, activation='softmax'))\n\nmodel_2_4.compile(loss='sparse_categorical_crossentropy',\n              optimizer='Nadam',\n              metrics=['accuracy'])"
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Epoch 1/12\n375/375 [==============================] - 7s 16ms/step - loss: 0.7594 - accuracy: 0.7553 - val_loss: 0.1427 - val_accuracy: 0.9590\nEpoch 2/12\n375/375 [==============================] - 6s 16ms/step - loss: 0.2047 - accuracy: 0.9372 - val_loss: 0.0969 - val_accuracy: 0.9709\nEpoch 3/12\n375/375 [==============================] - 6s 16ms/step - loss: 0.1399 - accuracy: 0.9568 - val_loss: 0.0825 - val_accuracy: 0.9762\nEpoch 4/12\n375/375 [==============================] - 6s 16ms/step - loss: 0.1125 - accuracy: 0.9639 - val_loss: 0.0719 - val_accuracy: 0.9778\nEpoch 5/12\n375/375 [==============================] - 6s 16ms/step - loss: 0.0928 - accuracy: 0.9709 - val_loss: 0.0645 - val_accuracy: 0.9797\nEpoch 6/12\n375/375 [==============================] - 6s 16ms/step - loss: 0.0764 - accuracy: 0.9752 - val_loss: 0.0628 - val_accuracy: 0.9813\nEpoch 7/12\n375/375 [==============================] - 6s 16ms/step - loss: 0.0717 - accuracy: 0.9767 - val_loss: 0.0618 - val_accuracy: 0.9822\nEpoch 8/12\n375/375 [==============================] - 6s 16ms/step - loss: 0.0572 - accuracy: 0.9814 - val_loss: 0.0639 - val_accuracy: 0.9820\nEpoch 9/12\n375/375 [==============================] - 6s 16ms/step - loss: 0.0532 - accuracy: 0.9819 - val_loss: 0.0606 - val_accuracy: 0.9833\nEpoch 10/12\n375/375 [==============================] - 6s 16ms/step - loss: 0.0472 - accuracy: 0.9835 - val_loss: 0.0618 - val_accuracy: 0.9843\nEpoch 11/12\n375/375 [==============================] - 6s 16ms/step - loss: 0.0415 - accuracy: 0.9853 - val_loss: 0.0607 - val_accuracy: 0.9841\nEpoch 12/12\n375/375 [==============================] - 6s 16ms/step - loss: 0.0427 - accuracy: 0.9862 - val_loss: 0.0594 - val_accuracy: 0.9845\n"
                },
                {
                    "data": {
                        "text/plain": "<tensorflow.python.keras.callbacks.History at 0x7f3c186dfc50>"
                    },
                    "execution_count": 21,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "model_2_4.fit(x_train, y_train,\n           batch_size=batch_size,\n           epochs=epochs,\n           verbose=1,\n           validation_data=(x_val, y_val))"
        },
        {
            "cell_type": "code",
            "execution_count": 22,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "model_2_4 test loss: 0.227501779794693\nmodel_2_4 test accuracy 0.947700023651123\n"
                }
            ],
            "source": "score = model_2_4.evaluate(x_test, y_test, verbose=0)\nprint('model_2_4 test loss:', score[0])\nprint('model_2_4 test accuracy', score[1])"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "#### This is the first dataset in .npz format"
        },
        {
            "cell_type": "code",
            "execution_count": 56,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "--2021-07-02 20:35:47--  http://codh.rois.ac.jp/kmnist/dataset/kmnist/kmnist-train-imgs.npz?raw=True\nResolving codh.rois.ac.jp (codh.rois.ac.jp)... 136.187.88.58\nConnecting to codh.rois.ac.jp (codh.rois.ac.jp)|136.187.88.58|:80... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 18384171 (18M)\nSaving to: \u2018kmnist-train-imgs.npz?raw=True\u2019\n\nkmnist-train-imgs.n 100%[===================>]  17.53M  7.55MB/s    in 2.3s    \n\n2021-07-02 20:35:50 (7.55 MB/s) - \u2018kmnist-train-imgs.npz?raw=True\u2019 saved [18384171/18384171]\n\n-rw-rw---- 1 wsuser watsonstudio 18M Feb  4  2019 kmnist-train-imgs.npz\n--2021-07-02 20:35:52--  http://codh.rois.ac.jp/kmnist/dataset/kmnist/kmnist-train-labels.npz?raw=True\nResolving codh.rois.ac.jp (codh.rois.ac.jp)... 136.187.88.58\nConnecting to codh.rois.ac.jp (codh.rois.ac.jp)|136.187.88.58|:80... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 29700 (29K)\nSaving to: \u2018kmnist-train-labels.npz?raw=True\u2019\n\nkmnist-train-labels 100%[===================>]  29.00K  --.-KB/s    in 0.1s    \n\n2021-07-02 20:35:53 (197 KB/s) - \u2018kmnist-train-labels.npz?raw=True\u2019 saved [29700/29700]\n\n-rw-rw---- 1 wsuser watsonstudio 30K Feb  4  2019 kmnist-train-labels.npz\n--2021-07-02 20:35:55--  http://codh.rois.ac.jp/kmnist/dataset/kmnist/kmnist-test-imgs.npz?raw=True\nResolving codh.rois.ac.jp (codh.rois.ac.jp)... 136.187.88.58\nConnecting to codh.rois.ac.jp (codh.rois.ac.jp)|136.187.88.58|:80... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 3079479 (2.9M)\nSaving to: \u2018kmnist-test-imgs.npz?raw=True\u2019\n\nkmnist-test-imgs.np 100%[===================>]   2.94M  2.74MB/s    in 1.1s    \n\n2021-07-02 20:35:56 (2.74 MB/s) - \u2018kmnist-test-imgs.npz?raw=True\u2019 saved [3079479/3079479]\n\n-rw-rw---- 1 wsuser watsonstudio 3.0M Feb  4  2019 kmnist-test-imgs.npz\n--2021-07-02 20:35:58--  http://codh.rois.ac.jp/kmnist/dataset/kmnist/kmnist-test-labels.npz?raw=True\nResolving codh.rois.ac.jp (codh.rois.ac.jp)... 136.187.88.58\nConnecting to codh.rois.ac.jp (codh.rois.ac.jp)|136.187.88.58|:80... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 5304 (5.2K)\nSaving to: \u2018kmnist-test-labels.npz?raw=True\u2019\n\nkmnist-test-labels. 100%[===================>]   5.18K  --.-KB/s    in 0s      \n\n2021-07-02 20:36:00 (288 MB/s) - \u2018kmnist-test-labels.npz?raw=True\u2019 saved [5304/5304]\n\n-rw-rw---- 1 wsuser watsonstudio 5.2K Feb  4  2019 kmnist-test-labels.npz\n"
                }
            ],
            "source": "!wget http://codh.rois.ac.jp/kmnist/dataset/kmnist/kmnist-train-imgs.npz?raw=True\n!mv kmnist-train-imgs.npz?raw=True kmnist-train-imgs.npz\n!ls -lahr kmnist-train-imgs.npz\n\n!wget http://codh.rois.ac.jp/kmnist/dataset/kmnist/kmnist-train-labels.npz?raw=True\n!mv kmnist-train-labels.npz?raw=True kmnist-train-labels.npz\n!ls -lahr kmnist-train-labels.npz\n\n!wget http://codh.rois.ac.jp/kmnist/dataset/kmnist/kmnist-test-imgs.npz?raw=True\n!mv kmnist-test-imgs.npz?raw=True kmnist-test-imgs.npz\n!ls -lahr kmnist-test-imgs.npz\n\n!wget http://codh.rois.ac.jp/kmnist/dataset/kmnist/kmnist-test-labels.npz?raw=True\n!mv kmnist-test-labels.npz?raw=True kmnist-test-labels.npz\n!ls -lahr kmnist-test-labels.npz"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### Model:  K Nearest Neighbors"
        },
        {
            "cell_type": "code",
            "execution_count": 57,
            "metadata": {},
            "outputs": [],
            "source": "def load(f):\n    return np.load(f)['arr_0']"
        },
        {
            "cell_type": "code",
            "execution_count": 58,
            "metadata": {},
            "outputs": [],
            "source": "x_train = load('kmnist-train-imgs.npz')\ny_train = load('kmnist-train-labels.npz')\nx_test = load('kmnist-test-imgs.npz')\ny_test = load('kmnist-test-labels.npz')"
        },
        {
            "cell_type": "code",
            "execution_count": 59,
            "metadata": {},
            "outputs": [],
            "source": "x_train = x_train.reshape(-1, 784)\nx_test = x_test.reshape(-1, 784)"
        },
        {
            "cell_type": "code",
            "execution_count": 60,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Fit KNeighborsClassifier(n_jobs=-1, n_neighbors=4, weights='distance')\n"
                },
                {
                    "data": {
                        "text/plain": "KNeighborsClassifier(n_jobs=-1, n_neighbors=4, weights='distance')"
                    },
                    "execution_count": 60,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "model_0 = KNeighborsClassifier(n_neighbors=4, weights='distance', n_jobs=-1)\nprint('Fit', model_0)\nmodel_0.fit(x_train, y_train)"
        },
        {
            "cell_type": "code",
            "execution_count": 61,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Evaluate KNeighborsClassifier(n_jobs=-1, n_neighbors=4, weights='distance')\nThe accuracy is: 0.921\n"
                }
            ],
            "source": "print('Evaluate', model_0)\nscore = model_0.score(x_test, y_test)\nprint('The accuracy is:', score)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Dataset 2\n\n#### 232,365 images, 49 classes, each image is 28 x 28 or represented by a 794 element array"
        },
        {
            "cell_type": "code",
            "execution_count": 23,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "--2021-07-06 21:36:35--  http://codh.rois.ac.jp/kmnist/dataset/k49/k49-train-imgs.npz?raw=True\nResolving codh.rois.ac.jp (codh.rois.ac.jp)... 136.187.88.58\nConnecting to codh.rois.ac.jp (codh.rois.ac.jp)|136.187.88.58|:80... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 66117696 (63M)\nSaving to: \u2018k49-train-imgs.npz?raw=True\u2019\n\nk49-train-imgs.npz? 100%[===================>]  63.05M  9.60MB/s    in 6.8s    \n\n2021-07-06 21:36:42 (9.25 MB/s) - \u2018k49-train-imgs.npz?raw=True\u2019 saved [66117696/66117696]\n\n-rw-rw---- 1 wsuser watsonstudio 64M Feb  4  2019 k49-train-imgs.npz\n--2021-07-06 21:36:44--  http://codh.rois.ac.jp/kmnist/dataset/k49/k49-train-labels.npz?raw=True\nResolving codh.rois.ac.jp (codh.rois.ac.jp)... 136.187.88.58\nConnecting to codh.rois.ac.jp (codh.rois.ac.jp)|136.187.88.58|:80... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 164485 (161K)\nSaving to: \u2018k49-train-labels.npz?raw=True\u2019\n\nk49-train-labels.np 100%[===================>] 160.63K   384KB/s    in 0.4s    \n\n2021-07-06 21:36:45 (384 KB/s) - \u2018k49-train-labels.npz?raw=True\u2019 saved [164485/164485]\n\n-rw-rw---- 1 wsuser watsonstudio 161K Feb  4  2019 k49-train-labels.npz\n--2021-07-06 21:36:47--  http://codh.rois.ac.jp/kmnist/dataset/k49/k49-test-imgs.npz?raw=True\nResolving codh.rois.ac.jp (codh.rois.ac.jp)... 136.187.88.58\nConnecting to codh.rois.ac.jp (codh.rois.ac.jp)|136.187.88.58|:80... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 10971201 (10M)\nSaving to: \u2018k49-test-imgs.npz?raw=True\u2019\n\nk49-test-imgs.npz?r 100%[===================>]  10.46M  5.62MB/s    in 1.9s    \n\n2021-07-06 21:36:50 (5.62 MB/s) - \u2018k49-test-imgs.npz?raw=True\u2019 saved [10971201/10971201]\n\n-rw-rw---- 1 wsuser watsonstudio 11M Feb  4  2019 k49-test-imgs.npz\n--2021-07-06 21:36:52--  http://codh.rois.ac.jp/kmnist/dataset/k49/k49-test-labels.npz?raw=True\nResolving codh.rois.ac.jp (codh.rois.ac.jp)... 136.187.88.58\nConnecting to codh.rois.ac.jp (codh.rois.ac.jp)|136.187.88.58|:80... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 27450 (27K)\nSaving to: \u2018k49-test-labels.npz?raw=True\u2019\n\nk49-test-labels.npz 100%[===================>]  26.81K  --.-KB/s    in 0.1s    \n\n2021-07-06 21:36:53 (184 KB/s) - \u2018k49-test-labels.npz?raw=True\u2019 saved [27450/27450]\n\n-rw-rw---- 1 wsuser watsonstudio 27K Feb  4  2019 k49-test-labels.npz\n"
                }
            ],
            "source": "# download the train and test labels and images, \n# which are available onlyh in .npz format \n\n!wget http://codh.rois.ac.jp/kmnist/dataset/k49/k49-train-imgs.npz?raw=True\n!mv k49-train-imgs.npz?raw=True k49-train-imgs.npz\n!ls -lahr k49-train-imgs.npz\n\n!wget http://codh.rois.ac.jp/kmnist/dataset/k49/k49-train-labels.npz?raw=True\n!mv k49-train-labels.npz?raw=True k49-train-labels.npz\n!ls -lahr k49-train-labels.npz\n\n!wget http://codh.rois.ac.jp/kmnist/dataset/k49/k49-test-imgs.npz?raw=True\n!mv k49-test-imgs.npz?raw=True k49-test-imgs.npz\n!ls -lahr k49-test-imgs.npz\n\n!wget http://codh.rois.ac.jp/kmnist/dataset/k49/k49-test-labels.npz?raw=True\n!mv k49-test-labels.npz?raw=True k49-test-labels.npz\n!ls -lahr k49-test-labels.npz"
        },
        {
            "cell_type": "code",
            "execution_count": 28,
            "metadata": {},
            "outputs": [],
            "source": "def load(f):\n    return np.load(f)['arr_0']"
        },
        {
            "cell_type": "code",
            "execution_count": 29,
            "metadata": {},
            "outputs": [],
            "source": "x_train_k49 = load('k49-train-imgs.npz')\ny_train_k49 = load('k49-train-labels.npz')\nx_test_k49 = load('k49-test-imgs.npz')\ny_test_k49 = load('k49-test-labels.npz')\n\nx_train_k49 = x_train_k49.reshape(-1, 784)\nx_test_k49 = x_test_k49.reshape(-1, 784)\n\n\n# the numpy arrays used in the keras models need to be between 0 and 1\n# we keep x_train_k49 and x_test_k49 in the format of between 0 and 255\n# for use in the K-nearest Neighbors model below\n\n# create the validattion set\n\nx_train_k49, x_val_k49, y_train_k49, y_val_k49 = train_test_split(x_train_k49, y_train_k49, test_size = 0.2, random_state=35)\n\n# we need to normalize the numpy arrays so that each number in the numpy\n# array is between 0 and 1:\n\ntrain_images_k49 = x_train_k49 / 255\nval_images_k49 = x_val_k49 / 255\ntest_images_k49 = x_test_k49 / 255\n\n# and the labels can be the same\n\ntrain_labels_k49 = y_train_k49\nval_labels_k49 = y_val_k49\ntest_labels_k49 = y_test_k49"
        },
        {
            "cell_type": "code",
            "execution_count": 65,
            "metadata": {},
            "outputs": [],
            "source": "# model_1_k49 def\n# we have to change the last layer due to the 49 categories\n\nmodel_1_k49 = tf.keras.Sequential([\n    tf.keras.layers.Dense(128, activation=tf.nn.relu),\n    tf.keras.layers.Dense(49, activation=tf.nn.softmax)\n])"
        },
        {
            "cell_type": "code",
            "execution_count": 66,
            "metadata": {},
            "outputs": [],
            "source": "# model_1_k49 compile\nmodel_1_k49.compile(optimizer='adam',\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])"
        },
        {
            "cell_type": "code",
            "execution_count": 67,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Epoch 1/20\n5810/5810 [==============================] - 17s 3ms/step - loss: 1.3346 - accuracy: 0.6691 - val_loss: 0.6867 - val_accuracy: 0.8160\nEpoch 2/20\n5810/5810 [==============================] - 17s 3ms/step - loss: 0.6084 - accuracy: 0.8382 - val_loss: 0.5795 - val_accuracy: 0.8431\nEpoch 3/20\n5810/5810 [==============================] - 17s 3ms/step - loss: 0.4955 - accuracy: 0.8658 - val_loss: 0.5435 - val_accuracy: 0.8555\nEpoch 4/20\n5810/5810 [==============================] - 17s 3ms/step - loss: 0.4451 - accuracy: 0.8778 - val_loss: 0.5296 - val_accuracy: 0.8591\nEpoch 5/20\n5810/5810 [==============================] - 17s 3ms/step - loss: 0.4067 - accuracy: 0.8878 - val_loss: 0.5191 - val_accuracy: 0.8633\nEpoch 6/20\n5810/5810 [==============================] - 17s 3ms/step - loss: 0.3824 - accuracy: 0.8939 - val_loss: 0.5135 - val_accuracy: 0.8657\nEpoch 7/20\n5810/5810 [==============================] - 17s 3ms/step - loss: 0.3634 - accuracy: 0.8978 - val_loss: 0.5362 - val_accuracy: 0.8639\nEpoch 8/20\n5810/5810 [==============================] - 17s 3ms/step - loss: 0.3458 - accuracy: 0.9018 - val_loss: 0.5323 - val_accuracy: 0.8625\nEpoch 9/20\n5810/5810 [==============================] - 17s 3ms/step - loss: 0.3322 - accuracy: 0.9064 - val_loss: 0.5374 - val_accuracy: 0.8645\nEpoch 10/20\n5810/5810 [==============================] - 17s 3ms/step - loss: 0.3204 - accuracy: 0.9088 - val_loss: 0.5474 - val_accuracy: 0.8637\nEpoch 11/20\n5810/5810 [==============================] - 17s 3ms/step - loss: 0.3060 - accuracy: 0.9132 - val_loss: 0.5564 - val_accuracy: 0.8618\nEpoch 12/20\n5810/5810 [==============================] - 17s 3ms/step - loss: 0.2980 - accuracy: 0.9140 - val_loss: 0.5597 - val_accuracy: 0.8643\nEpoch 13/20\n5810/5810 [==============================] - 17s 3ms/step - loss: 0.2896 - accuracy: 0.9176 - val_loss: 0.5652 - val_accuracy: 0.8652\nEpoch 14/20\n5810/5810 [==============================] - 17s 3ms/step - loss: 0.2821 - accuracy: 0.9179 - val_loss: 0.5779 - val_accuracy: 0.8640\nEpoch 15/20\n5810/5810 [==============================] - 17s 3ms/step - loss: 0.2742 - accuracy: 0.9204 - val_loss: 0.5841 - val_accuracy: 0.8652\nEpoch 16/20\n5810/5810 [==============================] - 17s 3ms/step - loss: 0.2671 - accuracy: 0.9220 - val_loss: 0.5929 - val_accuracy: 0.8621\nEpoch 17/20\n5810/5810 [==============================] - 17s 3ms/step - loss: 0.2649 - accuracy: 0.9226 - val_loss: 0.5894 - val_accuracy: 0.8643\nEpoch 18/20\n5810/5810 [==============================] - 17s 3ms/step - loss: 0.2537 - accuracy: 0.9258 - val_loss: 0.6061 - val_accuracy: 0.8623\nEpoch 19/20\n5810/5810 [==============================] - 17s 3ms/step - loss: 0.2500 - accuracy: 0.9273 - val_loss: 0.6062 - val_accuracy: 0.8658\nEpoch 20/20\n5810/5810 [==============================] - 17s 3ms/step - loss: 0.2424 - accuracy: 0.9297 - val_loss: 0.6293 - val_accuracy: 0.8618\n"
                },
                {
                    "data": {
                        "text/plain": "<tensorflow.python.keras.callbacks.History at 0x7f8b28410450>"
                    },
                    "execution_count": 67,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "# model_1_k49 fit\n# as in the first data set, this model could probably use more epochs\nmodel_1_k49.fit(train_images_k49, train_labels_k49, epochs=20, verbose=1, validation_data=(val_images_k49, val_labels_k49))"
        },
        {
            "cell_type": "code",
            "execution_count": 68,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "model_1_k49 test loss: 1.1436818838119507\nmodel_1_k49 test accuracy 0.7781928777694702\n"
                }
            ],
            "source": "score = model_1_k49.evaluate(test_images_k49, test_labels_k49, verbose=0)\nprint('model_1_k49 test loss:', score[0])\nprint('model_1_k49 test accuracy', score[1])"
        },
        {
            "cell_type": "code",
            "execution_count": 69,
            "metadata": {},
            "outputs": [],
            "source": "# model_1_2_k49 def\n\nmodel_1_2_k49 = tf.keras.Sequential([\n    tf.keras.layers.Dense(128, activation=tf.nn.relu),\n    tf.keras.layers.Dense(128, activation=tf.nn.relu),    \n    tf.keras.layers.Dense(128, activation=tf.nn.relu),\n    tf.keras.layers.Dense(128, activation=tf.nn.relu),    \n    tf.keras.layers.Dense(49, activation=tf.nn.softmax)\n])"
        },
        {
            "cell_type": "code",
            "execution_count": 70,
            "metadata": {},
            "outputs": [],
            "source": "# model_1_2_k49 compile\nmodel_1_2_k49.compile(optimizer='adam',\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])"
        },
        {
            "cell_type": "code",
            "execution_count": 71,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Epoch 1/20\n5810/5810 [==============================] - 21s 4ms/step - loss: 1.2966 - accuracy: 0.6590 - val_loss: 0.5920 - val_accuracy: 0.8348\nEpoch 2/20\n5810/5810 [==============================] - 21s 4ms/step - loss: 0.5320 - accuracy: 0.8505 - val_loss: 0.5129 - val_accuracy: 0.8574\nEpoch 3/20\n5810/5810 [==============================] - 21s 4ms/step - loss: 0.4330 - accuracy: 0.8773 - val_loss: 0.4968 - val_accuracy: 0.8639\nEpoch 4/20\n5810/5810 [==============================] - 21s 4ms/step - loss: 0.3788 - accuracy: 0.8917 - val_loss: 0.4635 - val_accuracy: 0.8755\nEpoch 5/20\n5810/5810 [==============================] - 20s 4ms/step - loss: 0.3378 - accuracy: 0.9017 - val_loss: 0.4821 - val_accuracy: 0.8728\nEpoch 6/20\n5810/5810 [==============================] - 21s 4ms/step - loss: 0.3130 - accuracy: 0.9092 - val_loss: 0.4467 - val_accuracy: 0.8822\nEpoch 7/20\n5810/5810 [==============================] - 21s 4ms/step - loss: 0.2920 - accuracy: 0.9150 - val_loss: 0.4589 - val_accuracy: 0.8812\nEpoch 8/20\n5810/5810 [==============================] - 20s 4ms/step - loss: 0.2773 - accuracy: 0.9195 - val_loss: 0.4892 - val_accuracy: 0.8745\nEpoch 9/20\n5810/5810 [==============================] - 20s 4ms/step - loss: 0.2661 - accuracy: 0.9222 - val_loss: 0.4752 - val_accuracy: 0.8800\nEpoch 10/20\n5810/5810 [==============================] - 21s 4ms/step - loss: 0.2555 - accuracy: 0.9249 - val_loss: 0.5045 - val_accuracy: 0.8772\nEpoch 11/20\n5810/5810 [==============================] - 21s 4ms/step - loss: 0.2431 - accuracy: 0.9279 - val_loss: 0.5006 - val_accuracy: 0.8775\nEpoch 12/20\n5810/5810 [==============================] - 21s 4ms/step - loss: 0.2397 - accuracy: 0.9294 - val_loss: 0.5101 - val_accuracy: 0.8817\nEpoch 13/20\n5810/5810 [==============================] - 20s 4ms/step - loss: 0.2357 - accuracy: 0.9304 - val_loss: 0.5119 - val_accuracy: 0.8823\nEpoch 14/20\n5810/5810 [==============================] - 21s 4ms/step - loss: 0.2276 - accuracy: 0.9330 - val_loss: 0.5080 - val_accuracy: 0.8851\nEpoch 15/20\n5810/5810 [==============================] - 21s 4ms/step - loss: 0.2212 - accuracy: 0.9340 - val_loss: 0.5298 - val_accuracy: 0.8791\nEpoch 16/20\n5810/5810 [==============================] - 20s 4ms/step - loss: 0.2203 - accuracy: 0.9344 - val_loss: 0.5475 - val_accuracy: 0.8827\nEpoch 17/20\n5810/5810 [==============================] - 21s 4ms/step - loss: 0.2136 - accuracy: 0.9376 - val_loss: 0.5406 - val_accuracy: 0.8838\nEpoch 18/20\n5810/5810 [==============================] - 21s 4ms/step - loss: 0.2101 - accuracy: 0.9377 - val_loss: 0.5515 - val_accuracy: 0.8815\nEpoch 19/20\n5810/5810 [==============================] - 21s 4ms/step - loss: 0.2046 - accuracy: 0.9393 - val_loss: 0.5602 - val_accuracy: 0.8835\nEpoch 20/20\n5810/5810 [==============================] - 20s 4ms/step - loss: 0.2035 - accuracy: 0.9402 - val_loss: 0.5778 - val_accuracy: 0.8811\n"
                },
                {
                    "data": {
                        "text/plain": "<tensorflow.python.keras.callbacks.History at 0x7f8b041f0a50>"
                    },
                    "execution_count": 71,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "# model_1_2_k49 fit\nmodel_1_2_k49.fit(train_images_k49, train_labels_k49, epochs=20, verbose=1, validation_data=(val_images_k49, val_labels_k49))"
        },
        {
            "cell_type": "code",
            "execution_count": 72,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Model: \"sequential_14\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense_43 (Dense)             (None, 128)               100480    \n_________________________________________________________________\ndense_44 (Dense)             (None, 128)               16512     \n_________________________________________________________________\ndense_45 (Dense)             (None, 128)               16512     \n_________________________________________________________________\ndense_46 (Dense)             (None, 128)               16512     \n_________________________________________________________________\ndense_47 (Dense)             (None, 49)                6321      \n=================================================================\nTotal params: 156,337\nTrainable params: 156,337\nNon-trainable params: 0\n_________________________________________________________________\n"
                }
            ],
            "source": "model_1_2_k49.summary()"
        },
        {
            "cell_type": "code",
            "execution_count": 73,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "model_1_2_k49 test loss: 0.9938585758209229\nmodel_1_2_k49 test accuracy 0.8113471865653992\n"
                }
            ],
            "source": "score = model_1_2_k49.evaluate(test_images_k49, test_labels_k49, verbose=0)\nprint('model_1_2_k49 test loss:', score[0])\nprint('model_1_2_k49 test accuracy', score[1])"
        },
        {
            "cell_type": "code",
            "execution_count": 74,
            "metadata": {},
            "outputs": [],
            "source": "# model_1_5_k49 def\n\nmodel_1_5_k49 = tf.keras.Sequential([\n    tf.keras.layers.Dense(128, activation=tf.nn.relu),\n    tf.keras.layers.Dense(128, activation=tf.nn.relu),    \n    tf.keras.layers.Dense(128, activation=tf.nn.relu),\n    tf.keras.layers.Dense(128, activation=tf.nn.relu),    \n    tf.keras.layers.Dense(49, activation=tf.nn.softmax)\n])"
        },
        {
            "cell_type": "code",
            "execution_count": 75,
            "metadata": {},
            "outputs": [],
            "source": "# model_1_5_k49 compile\n\nmodel_1_5_k49.compile(optimizer='sgd',\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])"
        },
        {
            "cell_type": "code",
            "execution_count": 76,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Epoch 1/20\n5810/5810 [==============================] - 21s 4ms/step - loss: 2.6352 - accuracy: 0.3462 - val_loss: 1.1710 - val_accuracy: 0.6892\nEpoch 2/20\n5810/5810 [==============================] - 20s 3ms/step - loss: 1.0330 - accuracy: 0.7303 - val_loss: 1.0079 - val_accuracy: 0.7305\nEpoch 3/20\n5810/5810 [==============================] - 20s 3ms/step - loss: 0.7628 - accuracy: 0.7966 - val_loss: 0.6947 - val_accuracy: 0.8121\nEpoch 4/20\n5810/5810 [==============================] - 19s 3ms/step - loss: 0.6233 - accuracy: 0.8309 - val_loss: 0.6231 - val_accuracy: 0.8309\nEpoch 5/20\n5810/5810 [==============================] - 20s 3ms/step - loss: 0.5449 - accuracy: 0.8506 - val_loss: 0.6295 - val_accuracy: 0.8262\nEpoch 6/20\n5810/5810 [==============================] - 20s 3ms/step - loss: 0.4840 - accuracy: 0.8665 - val_loss: 0.5403 - val_accuracy: 0.8510\nEpoch 7/20\n5810/5810 [==============================] - 20s 3ms/step - loss: 0.4447 - accuracy: 0.8773 - val_loss: 0.5401 - val_accuracy: 0.8514\nEpoch 8/20\n5810/5810 [==============================] - 20s 3ms/step - loss: 0.4098 - accuracy: 0.8867 - val_loss: 0.4804 - val_accuracy: 0.8681\nEpoch 9/20\n5810/5810 [==============================] - 20s 3ms/step - loss: 0.3785 - accuracy: 0.8940 - val_loss: 0.4610 - val_accuracy: 0.8732\nEpoch 10/20\n5810/5810 [==============================] - 20s 3ms/step - loss: 0.3554 - accuracy: 0.9002 - val_loss: 0.4448 - val_accuracy: 0.8798\nEpoch 11/20\n5810/5810 [==============================] - 20s 3ms/step - loss: 0.3326 - accuracy: 0.9070 - val_loss: 0.4586 - val_accuracy: 0.8750\nEpoch 12/20\n5810/5810 [==============================] - 20s 3ms/step - loss: 0.3158 - accuracy: 0.9113 - val_loss: 0.5064 - val_accuracy: 0.8636\nEpoch 13/20\n5810/5810 [==============================] - 20s 3ms/step - loss: 0.2987 - accuracy: 0.9156 - val_loss: 0.4452 - val_accuracy: 0.8785\nEpoch 14/20\n5810/5810 [==============================] - 20s 3ms/step - loss: 0.2822 - accuracy: 0.9204 - val_loss: 0.5015 - val_accuracy: 0.8645\nEpoch 15/20\n5810/5810 [==============================] - 20s 3ms/step - loss: 0.2683 - accuracy: 0.9242 - val_loss: 0.4251 - val_accuracy: 0.8858\nEpoch 16/20\n5810/5810 [==============================] - 20s 3ms/step - loss: 0.2592 - accuracy: 0.9261 - val_loss: 0.4898 - val_accuracy: 0.8695\nEpoch 17/20\n5810/5810 [==============================] - 20s 3ms/step - loss: 0.2452 - accuracy: 0.9299 - val_loss: 0.5704 - val_accuracy: 0.8511\nEpoch 18/20\n5810/5810 [==============================] - 20s 3ms/step - loss: 0.2343 - accuracy: 0.9324 - val_loss: 0.4461 - val_accuracy: 0.8793\nEpoch 19/20\n5810/5810 [==============================] - 20s 3ms/step - loss: 0.2255 - accuracy: 0.9360 - val_loss: 0.4174 - val_accuracy: 0.8902\nEpoch 20/20\n5810/5810 [==============================] - 19s 3ms/step - loss: 0.2164 - accuracy: 0.9376 - val_loss: 0.4592 - val_accuracy: 0.8805\n"
                },
                {
                    "data": {
                        "text/plain": "<tensorflow.python.keras.callbacks.History at 0x7f8afc7d1290>"
                    },
                    "execution_count": 76,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "# model_1_5_k49 fit\n\nmodel_1_5_k49.fit(train_images_k49, train_labels_k49, epochs=20, verbose=1, validation_data=(val_images_k49, val_labels_k49))"
        },
        {
            "cell_type": "code",
            "execution_count": 77,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Model: \"sequential_15\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense_48 (Dense)             (None, 128)               100480    \n_________________________________________________________________\ndense_49 (Dense)             (None, 128)               16512     \n_________________________________________________________________\ndense_50 (Dense)             (None, 128)               16512     \n_________________________________________________________________\ndense_51 (Dense)             (None, 128)               16512     \n_________________________________________________________________\ndense_52 (Dense)             (None, 49)                6321      \n=================================================================\nTotal params: 156,337\nTrainable params: 156,337\nNon-trainable params: 0\n_________________________________________________________________\n"
                }
            ],
            "source": "# model_1_5_k49 summary\n\nmodel_1_5_k49.summary()"
        },
        {
            "cell_type": "code",
            "execution_count": 78,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "model_1_5_k49 test loss: 0.816804826259613\nmodel_1_5_k49 test accuracy 0.8062105774879456\n"
                }
            ],
            "source": "score = model_1_5_k49.evaluate(test_images_k49, test_labels_k49, verbose=0)\nprint('model_1_5_k49 test loss:', score[0])\nprint('model_1_5_k49 test accuracy', score[1])"
        },
        {
            "cell_type": "code",
            "execution_count": 31,
            "metadata": {},
            "outputs": [],
            "source": "# model_2_k49 def\n\nbatch_size = 128\nnum_classes = 49       #  here we specify the number of classes as 49 instead of 10\nepochs = 12\n\n# input image dimensions\nimg_rows, img_cols = 28, 28\n\nx_train = train_images_k49\nx_val = val_images_k49\nx_test = test_images_k49\n\ny_train = train_labels_k49\ny_val = val_labels_k49\ny_test = test_labels_k49\n\nif K.image_data_format() == 'channels_first':\n    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n    x_val = x_val.reshape(x_val.shape[0], 1, img_rows, img_cols)\n    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n    input_shape = (1, img_rows, img_cols)\nelse:\n    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n    x_val = x_val.reshape(x_val.shape[0], img_rows, img_cols, 1)\n    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n    input_shape = (img_rows, img_cols, 1)\n\nmodel_2_k49 = tf.keras.Sequential()\nmodel_2_k49.add(Conv2D(32, kernel_size=(3, 3),\n                 activation='relu',\n                 input_shape=input_shape))\nmodel_2_k49.add(Conv2D(64, (3, 3), activation='relu'))\nmodel_2_k49.add(MaxPooling2D(pool_size=(2, 2)))\nmodel_2_k49.add(Dropout(0.25))\nmodel_2_k49.add(tf.keras.layers.Flatten())\nmodel_2_k49.add(tf.keras.layers.Dense(128, activation='relu'))\nmodel_2_k49.add(Dropout(0.5))\nmodel_2_k49.add(tf.keras.layers.Dense(num_classes, activation='softmax'))\n\n# model_2 compile\n\nmodel_2_k49.compile(optimizer='adam', \n          loss='sparse_categorical_crossentropy',\n          metrics=['accuracy'])"
        },
        {
            "cell_type": "code",
            "execution_count": 32,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Epoch 1/12\n1453/1453 [==============================] - 33s 23ms/step - loss: 1.5434 - accuracy: 0.6072 - val_loss: 0.3762 - val_accuracy: 0.8996\nEpoch 2/12\n1453/1453 [==============================] - 19s 13ms/step - loss: 0.6201 - accuracy: 0.8301 - val_loss: 0.2826 - val_accuracy: 0.9241\nEpoch 3/12\n1453/1453 [==============================] - 19s 13ms/step - loss: 0.4797 - accuracy: 0.8648 - val_loss: 0.2390 - val_accuracy: 0.9351\nEpoch 4/12\n1453/1453 [==============================] - 19s 13ms/step - loss: 0.4136 - accuracy: 0.8842 - val_loss: 0.2180 - val_accuracy: 0.9407\nEpoch 5/12\n1453/1453 [==============================] - 19s 13ms/step - loss: 0.3580 - accuracy: 0.8973 - val_loss: 0.1980 - val_accuracy: 0.9465\nEpoch 6/12\n1453/1453 [==============================] - 19s 13ms/step - loss: 0.3273 - accuracy: 0.9055 - val_loss: 0.1907 - val_accuracy: 0.9485\nEpoch 7/12\n1453/1453 [==============================] - 19s 13ms/step - loss: 0.3007 - accuracy: 0.9115 - val_loss: 0.1792 - val_accuracy: 0.9512\nEpoch 8/12\n1453/1453 [==============================] - 19s 13ms/step - loss: 0.2808 - accuracy: 0.9172 - val_loss: 0.1785 - val_accuracy: 0.9521\nEpoch 9/12\n1453/1453 [==============================] - 19s 13ms/step - loss: 0.2636 - accuracy: 0.9213 - val_loss: 0.1755 - val_accuracy: 0.9527\nEpoch 10/12\n1453/1453 [==============================] - 19s 13ms/step - loss: 0.2492 - accuracy: 0.9250 - val_loss: 0.1746 - val_accuracy: 0.9533\nEpoch 11/12\n1453/1453 [==============================] - 19s 13ms/step - loss: 0.2392 - accuracy: 0.9277 - val_loss: 0.1706 - val_accuracy: 0.9545\nEpoch 12/12\n1453/1453 [==============================] - 19s 13ms/step - loss: 0.2282 - accuracy: 0.9301 - val_loss: 0.1676 - val_accuracy: 0.9550\n"
                },
                {
                    "data": {
                        "text/plain": "<tensorflow.python.keras.callbacks.History at 0x7f3c183c8c50>"
                    },
                    "execution_count": 32,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "# fit model2_k49\n\nmodel_2_k49.fit(x_train, y_train,\n         batch_size=batch_size,\n         epochs=epochs,\n         verbose=1,\n         validation_data=(x_val, y_val))"
        },
        {
            "cell_type": "code",
            "execution_count": 33,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "model_2_k49 test loss: 0.3585013747215271\nmodel_2_k49 test accuracy 0.9141567349433899\n"
                }
            ],
            "source": "score = model_2_k49.evaluate(x_test, y_test, verbose=0)\nprint('model_2_k49 test loss:', score[0])\nprint('model_2_k49 test accuracy', score[1])"
        },
        {
            "cell_type": "code",
            "execution_count": 34,
            "metadata": {},
            "outputs": [],
            "source": "# model_2_1_k49 def\n\nmodel_2_1_k49 = tf.keras.Sequential()\nmodel_2_1_k49.add(Conv2D(32, kernel_size=(3, 3),\n                 activation='relu',\n                 input_shape=input_shape))\nmodel_2_1_k49.add(Conv2D(64, (3, 3), activation='relu'))\nmodel_2_1_k49.add(MaxPooling2D(pool_size=(2, 2)))\nmodel_2_1_k49.add(Dropout(0.25))\nmodel_2_1_k49.add(tf.keras.layers.Flatten())\nmodel_2_1_k49.add(tf.keras.layers.Dense(128, activation='relu'))\nmodel_2_1_k49.add(Dropout(0.5))\nmodel_2_1_k49.add(tf.keras.layers.Dense(num_classes, activation='softmax'))"
        },
        {
            "cell_type": "code",
            "execution_count": 35,
            "metadata": {},
            "outputs": [],
            "source": "# change of optimizer and compile model_2_k49\n\nmodel_2_1_k49.compile(loss='sparse_categorical_crossentropy',\n              optimizer=tf.keras.optimizers.Adadelta(learning_rate=1.0),\n              metrics=['accuracy'])"
        },
        {
            "cell_type": "code",
            "execution_count": 36,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Epoch 1/12\n1453/1453 [==============================] - 20s 13ms/step - loss: 1.5294 - accuracy: 0.6077 - val_loss: 0.3793 - val_accuracy: 0.8993\nEpoch 2/12\n1453/1453 [==============================] - 19s 13ms/step - loss: 0.5717 - accuracy: 0.8447 - val_loss: 0.2760 - val_accuracy: 0.9256\nEpoch 3/12\n1453/1453 [==============================] - 19s 13ms/step - loss: 0.4566 - accuracy: 0.8744 - val_loss: 0.2415 - val_accuracy: 0.9347\nEpoch 4/12\n1453/1453 [==============================] - 19s 13ms/step - loss: 0.3969 - accuracy: 0.8914 - val_loss: 0.2340 - val_accuracy: 0.9379\nEpoch 5/12\n1453/1453 [==============================] - 19s 13ms/step - loss: 0.3652 - accuracy: 0.8994 - val_loss: 0.2144 - val_accuracy: 0.9421\nEpoch 6/12\n1453/1453 [==============================] - 19s 13ms/step - loss: 0.3430 - accuracy: 0.9052 - val_loss: 0.2069 - val_accuracy: 0.9457\nEpoch 7/12\n1453/1453 [==============================] - 19s 13ms/step - loss: 0.3321 - accuracy: 0.9090 - val_loss: 0.2159 - val_accuracy: 0.9473\nEpoch 8/12\n1453/1453 [==============================] - 19s 13ms/step - loss: 0.3265 - accuracy: 0.9114 - val_loss: 0.1974 - val_accuracy: 0.9485\nEpoch 9/12\n1453/1453 [==============================] - 19s 13ms/step - loss: 0.3202 - accuracy: 0.9137 - val_loss: 0.3551 - val_accuracy: 0.9404\nEpoch 10/12\n1453/1453 [==============================] - 19s 13ms/step - loss: 0.3170 - accuracy: 0.9158 - val_loss: 0.2060 - val_accuracy: 0.9492\nEpoch 11/12\n1453/1453 [==============================] - 19s 13ms/step - loss: 0.3110 - accuracy: 0.9167 - val_loss: 0.1865 - val_accuracy: 0.9503\nEpoch 12/12\n1453/1453 [==============================] - 19s 13ms/step - loss: 0.3136 - accuracy: 0.9156 - val_loss: 0.2091 - val_accuracy: 0.9494\n"
                },
                {
                    "data": {
                        "text/plain": "<tensorflow.python.keras.callbacks.History at 0x7f3c18217390>"
                    },
                    "execution_count": 36,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "# fit model_2_k49 after change of optmizer\n\nmodel_2_1_k49.fit(x_train, y_train,\n           batch_size=batch_size,\n           epochs=epochs,\n           verbose=1,\n           validation_data=(x_val, y_val))"
        },
        {
            "cell_type": "code",
            "execution_count": 39,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "model_2_1_k49 test loss: 0.46799635887145996\nmodel_2_1_k49 test accuracy 0.8956338763237\n"
                }
            ],
            "source": "score = model_2_1_k49.evaluate(x_test, y_test, verbose=0)\nprint('model_2_1_k49 test loss:', score[0])\nprint('model_2_1_k49 test accuracy', score[1])"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### Model:  K Nearest Neighbors - k49"
        },
        {
            "cell_type": "code",
            "execution_count": 85,
            "metadata": {},
            "outputs": [],
            "source": "# recall that the two numpy image arrays used below, namely x_train_k49 and x_test_k49\n# are arrays of integers, which works fine for KNN"
        },
        {
            "cell_type": "code",
            "execution_count": 86,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Fit KNeighborsClassifier(n_jobs=-1, n_neighbors=4, weights='distance')\n"
                },
                {
                    "data": {
                        "text/plain": "KNeighborsClassifier(n_jobs=-1, n_neighbors=4, weights='distance')"
                    },
                    "execution_count": 86,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "x_train_k49 = load('k49-train-imgs.npz')\ny_train_k49 = load('k49-train-labels.npz')\nx_test_k49 = load('k49-test-imgs.npz')\ny_test_k49 = load('k49-test-labels.npz')\n\nx_train_k49 = x_train_k49.reshape(-1, 784)\nx_test_k49 = x_test_k49.reshape(-1, 784)\n\nmodel_0_k49 = KNeighborsClassifier(n_neighbors=4, weights='distance', n_jobs=-1)\nprint('Fit', model_0_k49)\nmodel_0_k49.fit(x_train_k49, y_train_k49)"
        },
        {
            "cell_type": "code",
            "execution_count": 87,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Evaluate KNeighborsClassifier(n_jobs=-1, n_neighbors=4, weights='distance')\nThe accuracy is: 0.8551638259786754\n"
                }
            ],
            "source": "print('Evaluate', model_0_k49)\nscore = model_0_k49.score(x_test_k49, y_test_k49)\nprint('The accuracy is:', score)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### The Third Dataset\n\n#### Includes Kanji characters, this data has 3832 different classes and consists of 140,426 images, each image is 64 X 64 pixels\n#### This dataset is not as processed as the other two. It is just a bunch of png images in a directory inside an archive file."
        },
        {
            "cell_type": "code",
            "execution_count": 40,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "--2021-07-06 21:49:18--  http://codh.rois.ac.jp/kmnist/dataset/kkanji/kkanji.tar?raw=True\nResolving codh.rois.ac.jp (codh.rois.ac.jp)... 136.187.88.58\nConnecting to codh.rois.ac.jp (codh.rois.ac.jp)|136.187.88.58|:80... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 324290560 (309M) [application/x-tar]\nSaving to: \u2018kkanji.tar?raw=True\u2019\n\nkkanji.tar?raw=True 100%[===================>] 309.27M  6.78MB/s    in 64s     \n\n2021-07-06 21:50:22 (4.84 MB/s) - \u2018kkanji.tar?raw=True\u2019 saved [324290560/324290560]\n\n-rw-rw---- 1 wsuser watsonstudio 310M Dec  8  2018 kkanji.tar\nkkanji2/\nkkanji2/U+5B87/\nkkanji2/U+5B87/72d56fcb33d10fe0.png\nkkanji2/U+5B87/75f7923797777c74.png\nkkanji2/U+5B87/69d6becd4f8f2d61.png\nkkanji2/U+5B87/522dd01c5f9573f5.png\nkkanji2/U+5B87/36aadd8d92c64049.png\nkkanji2/U+5B87/58f75629e53b9e63.png\nkkanji2/U+5B87/557950fbb39b019b.png\nkkanji2/U+5B87/c4ca643dbc0299b6.png\nkkanji2/U+5B87/02f161e7e7a3c364.png\nkkanji2/U+5B87/c45553bb4a35c8d4.png\nkkanji2/U+5B87/ffa955bd6cb43af8.png\nkkanji2/U+5B87/d0387f14448f2a95.png\nkkanji2/U+5B87/d8c4a0dbd99fc02f.png\nkkanji2/U+5B87/419beef7f2c593da.png\nkkanji2/U+5B87/f2308287339f973d.png\nkkanji2/U+5B87/f885e3a957b99e8e.png\nkkanji2/U+5B87/f5e7a417e3d831d8.png\nkkanji2/U+5B87/17681a9b66db23e0.png\nkkanji2/U+5B87/cc2b8cd01c984262.png\nkkanji2/U+5B87/3e76b512ef42c8f2.png\nkkanji2/U+5B87/49276561d0ca08e0.png\nkkanji2/U+5B87/a169cf27462e020d.png\nkkanji2/U+5B87/61b85600b3fdbea9.png\nkkanji2/U+5B87/112b17b885fbfa61.png\nkkanji2/U+5B87/99654e0e2e597032.png\nkkanji2/U+5B87/7af7faf202121801.png\nkkanji2/U+5B87/8a6b7ea64d025e6d.png\nkkanji2/U+5B87/3c965132ab259d2b.png\nkkanji2/U+5B87/e5e120e4a9d70b6b.png\nkkanji2/U+5B87/a83b76ab3c9ab31c.png\nkkanji2/U+5B87/5a3cf89898fb6100.png\nkkanji2/U+5B87/d84ddc102e8425b6.png\nkkanji2/U+5B87/bdf7cd68b0df8857.png\nkkanji2/U+5B87/a0a4815c3bfe9f15.png\nkkanji2/U+5B87/b2e74a3d92ef78ac.png\nkkanji2/U+5B87/25e9d668d9b64914.png\nkkanji2/U+5B87/5b6ee9b9d03fc3ad.png\nkkanji2/U+583A/\nkkanji2/U+583A/2adaddcb1db613b1.png\nkkanji2/U+583A/3bd7379e66373395.png\nkkanji2/U+583A/ea8dd8f0d7375e9c.png\nkkanji2/U+583A/f738049a58a97177.png\nkkanji2/U+583A/b3a041d6510720b2.png\nkkanji2/U+583A/3921ca3a71dcaaff.png\nkkanji2/U+583A/21d02bce1618fd05.png\nkkanji2/U+583A/df87ce26b03e824d.png\nkkanji2/U+583A/5e5b7454b683e6b0.png\nkkanji2/U+583A/10063d313e1474aa.png\nkkanji2/U+583A/84e528c1a87102b4.png\nkkanji2/U+583A/f8db9187baf4b352.png\nkkanji2/U+583A/e1eb722d60ac207c.png\nkkanji2/U+583A/ab464d2670b260bc.png\nkkanji2/U+583A/596e410f8ae20a07.png\nkkanji2/U+583A/805fa5f10779d104.png\nkkanji2/U+583A/66e27da7a955f8ea.png\nkkanji2/U+583A/d43d5197995e7345.png\nkkanji2/U+583A/0ceea4b4033e6c66.png\nkkanji2/U+5ECA/\nkkanji2/U+5ECA/47d36e0f7df9b8ab.png\nkkanji2/U+5ECA/ea6adb0abe33425c.png\nkkanji2/U+5ECA/542e3b1950722456.png\nkkanji2/U+5ECA/3b16e7bcc8a3a717.png\nkkanji2/U+5ECA/7f899bc5008c5824.png\nkkanji2/U+9CF6/\nkkanji2/U+9CF6/e52af171049817f9.png\nkkanji2/U+9CF6/3a09e6488fd81fcd.png\nkkanji2/U+9CF6/ae5f75bd30a73375.png\nkkanji2/U+5A9B/\ntar: write error\n"
                }
            ],
            "source": "# we download the archive:\n\n!wget http://codh.rois.ac.jp/kmnist/dataset/kkanji/kkanji.tar?raw=True\n!mv kkanji.tar?raw=True kkanji.tar\n!ls -lahr kkanji.tar\n\n# list the contents of the archive\n# limit output to the first 70 files\n\n!tar -tf kkanji.tar | head -70"
        },
        {
            "cell_type": "code",
            "execution_count": 41,
            "metadata": {},
            "outputs": [],
            "source": "# extract the archive:\n!tar -xf kkanji.tar"
        },
        {
            "cell_type": "code",
            "execution_count": 42,
            "metadata": {},
            "outputs": [],
            "source": "# put the codepoints in a file\n# let the first line of the file be name\n# of the column in the dataframe we are creating\n!echo codepoint > codepoints.csv\n!ls kkanji2 >> codepoints.csv"
        },
        {
            "cell_type": "code",
            "execution_count": 43,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "/home/wsuser/work/codepoints.csv\n"
                }
            ],
            "source": "# verify the file's path:\nprint (os.path.abspath(\"codepoints.csv\"))"
        },
        {
            "cell_type": "code",
            "execution_count": 44,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>codepoint</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>U+241C6</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>U+24FA3</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>U+25DA1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>U+27752</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>U+29780</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>3827</th>\n      <td>U+FA38</td>\n    </tr>\n    <tr>\n      <th>3828</th>\n      <td>U+FA45</td>\n    </tr>\n    <tr>\n      <th>3829</th>\n      <td>U+FA4A</td>\n    </tr>\n    <tr>\n      <th>3830</th>\n      <td>U+FA55</td>\n    </tr>\n    <tr>\n      <th>3831</th>\n      <td>U+FA5C</td>\n    </tr>\n  </tbody>\n</table>\n<p>3832 rows \u00d7 1 columns</p>\n</div>",
                        "text/plain": "     codepoint\n0      U+241C6\n1      U+24FA3\n2      U+25DA1\n3      U+27752\n4      U+29780\n...        ...\n3827    U+FA38\n3828    U+FA45\n3829    U+FA4A\n3830    U+FA55\n3831    U+FA5C\n\n[3832 rows x 1 columns]"
                    },
                    "execution_count": 44,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "# read all the lines of the file into pandas dataframe\n# including the column header which is already in the file\n# display new dataframe\n# here we can confirm that the data has 3832 classes\n\ndf_kanji_classmap = pd.read_csv(\"codepoints.csv\")\ndf_kanji_classmap"
        },
        {
            "cell_type": "code",
            "execution_count": 45,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "image/png": "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\n",
                        "text/plain": "<Figure size 432x288 with 1 Axes>"
                    },
                    "metadata": {
                        "needs_background": "light"
                    },
                    "output_type": "display_data"
                }
            ],
            "source": "# we'll need to read the images in to transform them\n#read the first listed image in the first folder and display it\nimg = mping.imread('kkanji2/U+5B87/72d56fcb33d10fe0.png')\nplt.imshow(img)\nplt.show()\n# note that this image in only \"first\" as listed from the tar arcive above\n# it is not \"first\" in terms of the codepoint listing we created directly above"
        },
        {
            "cell_type": "code",
            "execution_count": 46,
            "metadata": {},
            "outputs": [],
            "source": "# create a pandas dataframe that contains the codepoint for each image, \n# and its full path in the os and display that dataframe\n\n#  ** reminder:  makes sure to set the path line correctly using the fact that\n# the directory kkanji2 is in the same directory as the file codepoints.csv\n\ndata = []\ndir = os.path.realpath('/home/wsuser/work/kkanji2')\nfor r, d, f in os.walk(dir):\n    for file in f:\n        if \".png\" in file:\n            data.append((r.split('/')[-1],os.path.join(r,file)))\ndf_kanji2 = pd.DataFrame(data, columns=['codepoint', 'image_file_path']).sort_values(by=['codepoint'], ignore_index = True)"
        },
        {
            "cell_type": "code",
            "execution_count": 47,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>codepoint</th>\n      <th>image_file_path</th>\n      <th>np_array</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>U+241C6</td>\n      <td>/home/wsuser/work/kkanji2/U+241C6/c0d603c6ce4a...</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>U+241C6</td>\n      <td>/home/wsuser/work/kkanji2/U+241C6/689fa55040ec...</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>U+24FA3</td>\n      <td>/home/wsuser/work/kkanji2/U+24FA3/80582798ed70...</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>U+24FA3</td>\n      <td>/home/wsuser/work/kkanji2/U+24FA3/4190e728bfc9...</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>U+25DA1</td>\n      <td>/home/wsuser/work/kkanji2/U+25DA1/512d7fcacddd...</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>140419</th>\n      <td>U+FA55</td>\n      <td>/home/wsuser/work/kkanji2/U+FA55/093ab4bc62fab...</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>140420</th>\n      <td>U+FA55</td>\n      <td>/home/wsuser/work/kkanji2/U+FA55/830a4ce68f658...</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>140421</th>\n      <td>U+FA55</td>\n      <td>/home/wsuser/work/kkanji2/U+FA55/bad3a39f22550...</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>140422</th>\n      <td>U+FA5C</td>\n      <td>/home/wsuser/work/kkanji2/U+FA5C/679e4b2f026f6...</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>140423</th>\n      <td>U+FA5C</td>\n      <td>/home/wsuser/work/kkanji2/U+FA5C/15e2060396eba...</td>\n      <td></td>\n    </tr>\n  </tbody>\n</table>\n<p>140424 rows \u00d7 3 columns</p>\n</div>",
                        "text/plain": "       codepoint                                    image_file_path np_array\n0        U+241C6  /home/wsuser/work/kkanji2/U+241C6/c0d603c6ce4a...         \n1        U+241C6  /home/wsuser/work/kkanji2/U+241C6/689fa55040ec...         \n2        U+24FA3  /home/wsuser/work/kkanji2/U+24FA3/80582798ed70...         \n3        U+24FA3  /home/wsuser/work/kkanji2/U+24FA3/4190e728bfc9...         \n4        U+25DA1  /home/wsuser/work/kkanji2/U+25DA1/512d7fcacddd...         \n...          ...                                                ...      ...\n140419    U+FA55  /home/wsuser/work/kkanji2/U+FA55/093ab4bc62fab...         \n140420    U+FA55  /home/wsuser/work/kkanji2/U+FA55/830a4ce68f658...         \n140421    U+FA55  /home/wsuser/work/kkanji2/U+FA55/bad3a39f22550...         \n140422    U+FA5C  /home/wsuser/work/kkanji2/U+FA5C/679e4b2f026f6...         \n140423    U+FA5C  /home/wsuser/work/kkanji2/U+FA5C/15e2060396eba...         \n\n[140424 rows x 3 columns]"
                    },
                    "execution_count": 47,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "# add a blank column to the dataframe with column name 'np_array'\n\ndf_kanji2['np_array'] = \"\"\ndf_kanji2"
        },
        {
            "cell_type": "code",
            "execution_count": 48,
            "metadata": {},
            "outputs": [],
            "source": "# convert the pandas dataframe into a pyspark dataframe\ndf_kanji2_pyspk = spark.createDataFrame(df_kanji2)"
        },
        {
            "cell_type": "code",
            "execution_count": 49,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "+---------+--------------------+--------+----------+\n|codepoint|     image_file_path|np_array|classIndex|\n+---------+--------------------+--------+----------+\n|  U+241C6|/home/wsuser/work...|        |    2606.0|\n|  U+241C6|/home/wsuser/work...|        |    2606.0|\n|  U+24FA3|/home/wsuser/work...|        |    2607.0|\n|  U+24FA3|/home/wsuser/work...|        |    2607.0|\n|  U+25DA1|/home/wsuser/work...|        |    3017.0|\n|  U+27752|/home/wsuser/work...|        |    1966.0|\n|  U+27752|/home/wsuser/work...|        |    1966.0|\n|  U+27752|/home/wsuser/work...|        |    1966.0|\n|  U+27752|/home/wsuser/work...|        |    1966.0|\n|  U+27752|/home/wsuser/work...|        |    1966.0|\n|  U+29780|/home/wsuser/work...|        |    3018.0|\n|  U+29DDA|/home/wsuser/work...|        |    3019.0|\n|  U+29E75|/home/wsuser/work...|        |    3020.0|\n|   U+4093|/home/wsuser/work...|        |    3021.0|\n|   U+4453|/home/wsuser/work...|        |     680.0|\n|   U+4453|/home/wsuser/work...|        |     680.0|\n|   U+4453|/home/wsuser/work...|        |     680.0|\n|   U+4453|/home/wsuser/work...|        |     680.0|\n|   U+4453|/home/wsuser/work...|        |     680.0|\n|   U+4453|/home/wsuser/work...|        |     680.0|\n+---------+--------------------+--------+----------+\nonly showing top 20 rows\n\n"
                }
            ],
            "source": "# our data has 3,831 different classes each with unique string names\n# which is based on their character codepoints\n# but we want simple numeric class index\n# so we instantiate a StringIndexer in spark:\n\nindexer = StringIndexer(inputCol=\"codepoint\",outputCol=\"classIndex\")\nindexed_df = indexer.fit(df_kanji2_pyspk).transform(df_kanji2_pyspk)\nindexed_df.show()"
        },
        {
            "cell_type": "code",
            "execution_count": 50,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>codepoint</th>\n      <th>image_file_path</th>\n      <th>np_array</th>\n      <th>classIndex</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>U+241C6</td>\n      <td>/home/wsuser/work/kkanji2/U+241C6/c0d603c6ce4a...</td>\n      <td></td>\n      <td>2606.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>U+241C6</td>\n      <td>/home/wsuser/work/kkanji2/U+241C6/689fa55040ec...</td>\n      <td></td>\n      <td>2606.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>U+24FA3</td>\n      <td>/home/wsuser/work/kkanji2/U+24FA3/80582798ed70...</td>\n      <td></td>\n      <td>2607.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>U+24FA3</td>\n      <td>/home/wsuser/work/kkanji2/U+24FA3/4190e728bfc9...</td>\n      <td></td>\n      <td>2607.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>U+25DA1</td>\n      <td>/home/wsuser/work/kkanji2/U+25DA1/512d7fcacddd...</td>\n      <td></td>\n      <td>3017.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>140419</th>\n      <td>U+FA55</td>\n      <td>/home/wsuser/work/kkanji2/U+FA55/093ab4bc62fab...</td>\n      <td></td>\n      <td>1041.0</td>\n    </tr>\n    <tr>\n      <th>140420</th>\n      <td>U+FA55</td>\n      <td>/home/wsuser/work/kkanji2/U+FA55/830a4ce68f658...</td>\n      <td></td>\n      <td>1041.0</td>\n    </tr>\n    <tr>\n      <th>140421</th>\n      <td>U+FA55</td>\n      <td>/home/wsuser/work/kkanji2/U+FA55/bad3a39f22550...</td>\n      <td></td>\n      <td>1041.0</td>\n    </tr>\n    <tr>\n      <th>140422</th>\n      <td>U+FA5C</td>\n      <td>/home/wsuser/work/kkanji2/U+FA5C/679e4b2f026f6...</td>\n      <td></td>\n      <td>3016.0</td>\n    </tr>\n    <tr>\n      <th>140423</th>\n      <td>U+FA5C</td>\n      <td>/home/wsuser/work/kkanji2/U+FA5C/15e2060396eba...</td>\n      <td></td>\n      <td>3016.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>140424 rows \u00d7 4 columns</p>\n</div>",
                        "text/plain": "       codepoint                                    image_file_path np_array  \\\n0        U+241C6  /home/wsuser/work/kkanji2/U+241C6/c0d603c6ce4a...            \n1        U+241C6  /home/wsuser/work/kkanji2/U+241C6/689fa55040ec...            \n2        U+24FA3  /home/wsuser/work/kkanji2/U+24FA3/80582798ed70...            \n3        U+24FA3  /home/wsuser/work/kkanji2/U+24FA3/4190e728bfc9...            \n4        U+25DA1  /home/wsuser/work/kkanji2/U+25DA1/512d7fcacddd...            \n...          ...                                                ...      ...   \n140419    U+FA55  /home/wsuser/work/kkanji2/U+FA55/093ab4bc62fab...            \n140420    U+FA55  /home/wsuser/work/kkanji2/U+FA55/830a4ce68f658...            \n140421    U+FA55  /home/wsuser/work/kkanji2/U+FA55/bad3a39f22550...            \n140422    U+FA5C  /home/wsuser/work/kkanji2/U+FA5C/679e4b2f026f6...            \n140423    U+FA5C  /home/wsuser/work/kkanji2/U+FA5C/15e2060396eba...            \n\n        classIndex  \n0           2606.0  \n1           2606.0  \n2           2607.0  \n3           2607.0  \n4           3017.0  \n...            ...  \n140419      1041.0  \n140420      1041.0  \n140421      1041.0  \n140422      3016.0  \n140423      3016.0  \n\n[140424 rows x 4 columns]"
                    },
                    "execution_count": 50,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "# transform back to pandas dataframe:\ndf_kanji2 = indexed_df.toPandas()\ndf_kanji2"
        },
        {
            "cell_type": "code",
            "execution_count": 51,
            "metadata": {},
            "outputs": [],
            "source": "# add a column containing and numpy array of the image indicated in the path in image_file_path\n# and at the same time, flatten each image from a 64 x 64 numpy array to a single\n# dimension 4096 element long numpy array\ndf_kanji2['np_array'] = df_kanji2['image_file_path'].apply(lambda x: np.asarray(Image.open(x))).apply(lambda y: np.reshape(y,(4096,)))"
        },
        {
            "cell_type": "code",
            "execution_count": 52,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>codepoint</th>\n      <th>image_file_path</th>\n      <th>np_array</th>\n      <th>classIndex</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>U+241C6</td>\n      <td>/home/wsuser/work/kkanji2/U+241C6/c0d603c6ce4a...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>2606.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>U+241C6</td>\n      <td>/home/wsuser/work/kkanji2/U+241C6/689fa55040ec...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>2606.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>U+24FA3</td>\n      <td>/home/wsuser/work/kkanji2/U+24FA3/80582798ed70...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>2607.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>U+24FA3</td>\n      <td>/home/wsuser/work/kkanji2/U+24FA3/4190e728bfc9...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>2607.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>U+25DA1</td>\n      <td>/home/wsuser/work/kkanji2/U+25DA1/512d7fcacddd...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>3017.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>140419</th>\n      <td>U+FA55</td>\n      <td>/home/wsuser/work/kkanji2/U+FA55/093ab4bc62fab...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>1041.0</td>\n    </tr>\n    <tr>\n      <th>140420</th>\n      <td>U+FA55</td>\n      <td>/home/wsuser/work/kkanji2/U+FA55/830a4ce68f658...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>1041.0</td>\n    </tr>\n    <tr>\n      <th>140421</th>\n      <td>U+FA55</td>\n      <td>/home/wsuser/work/kkanji2/U+FA55/bad3a39f22550...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>1041.0</td>\n    </tr>\n    <tr>\n      <th>140422</th>\n      <td>U+FA5C</td>\n      <td>/home/wsuser/work/kkanji2/U+FA5C/679e4b2f026f6...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>3016.0</td>\n    </tr>\n    <tr>\n      <th>140423</th>\n      <td>U+FA5C</td>\n      <td>/home/wsuser/work/kkanji2/U+FA5C/15e2060396eba...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>3016.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>140424 rows \u00d7 4 columns</p>\n</div>",
                        "text/plain": "       codepoint                                    image_file_path  \\\n0        U+241C6  /home/wsuser/work/kkanji2/U+241C6/c0d603c6ce4a...   \n1        U+241C6  /home/wsuser/work/kkanji2/U+241C6/689fa55040ec...   \n2        U+24FA3  /home/wsuser/work/kkanji2/U+24FA3/80582798ed70...   \n3        U+24FA3  /home/wsuser/work/kkanji2/U+24FA3/4190e728bfc9...   \n4        U+25DA1  /home/wsuser/work/kkanji2/U+25DA1/512d7fcacddd...   \n...          ...                                                ...   \n140419    U+FA55  /home/wsuser/work/kkanji2/U+FA55/093ab4bc62fab...   \n140420    U+FA55  /home/wsuser/work/kkanji2/U+FA55/830a4ce68f658...   \n140421    U+FA55  /home/wsuser/work/kkanji2/U+FA55/bad3a39f22550...   \n140422    U+FA5C  /home/wsuser/work/kkanji2/U+FA5C/679e4b2f026f6...   \n140423    U+FA5C  /home/wsuser/work/kkanji2/U+FA5C/15e2060396eba...   \n\n                                                 np_array  classIndex  \n0       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...      2606.0  \n1       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...      2606.0  \n2       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...      2607.0  \n3       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...      2607.0  \n4       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...      3017.0  \n...                                                   ...         ...  \n140419  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...      1041.0  \n140420  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...      1041.0  \n140421  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...      1041.0  \n140422  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...      3016.0  \n140423  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...      3016.0  \n\n[140424 rows x 4 columns]"
                    },
                    "execution_count": 52,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "df_kanji2"
        },
        {
            "cell_type": "code",
            "execution_count": 53,
            "metadata": {},
            "outputs": [],
            "source": "# take the np_array and classIndex columns out of the dataframe and put them into two numpy arrays\nimages_kanji = df_kanji2[\"np_array\"].to_numpy()\nlabels_kanji = df_kanji2[\"classIndex\"].to_numpy()"
        },
        {
            "cell_type": "code",
            "execution_count": 54,
            "metadata": {},
            "outputs": [],
            "source": "# at this point, the images are stored in a numpy array containing numpy arrays for each each, but instead\n# we want a two dimensional numpy array, so convert that as follows\nimages_kanji = np.stack(images_kanji)"
        },
        {
            "cell_type": "code",
            "execution_count": 55,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "(140424, 4096)"
                    },
                    "execution_count": 55,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "images_kanji.shape"
        },
        {
            "cell_type": "code",
            "execution_count": 56,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "(140424,)"
                    },
                    "execution_count": 56,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "labels_kanji.shape"
        },
        {
            "cell_type": "code",
            "execution_count": 57,
            "metadata": {},
            "outputs": [],
            "source": "# now that the image and label arrays have the right dimensions, \n# we need to divide them into training, validation, and test sets:\n\nx_train_kanji, x_test_kanji, y_train_kanji, y_test_kanji = train_test_split(images_kanji, labels_kanji, test_size = 0.2, random_state=42)\nx_train_kanji, x_val_kanji, y_train_kanji, y_val_kanji = train_test_split(x_train_kanji, y_train_kanji, test_size = 0.2, random_state=28)"
        },
        {
            "cell_type": "code",
            "execution_count": 58,
            "metadata": {},
            "outputs": [],
            "source": "# we need to normalize each of the training and test image numpy arrays so that each number in the numpy\n# array is between 0 and 1:\n\nx_train_kanji =  x_train_kanji / 255\nx_test_kanji = x_test_kanji / 255\nx_val_kanji = x_val_kanji / 255"
        },
        {
            "cell_type": "code",
            "execution_count": 107,
            "metadata": {},
            "outputs": [],
            "source": "# model_1_2_k def\n\nmodel_1_2_k = tf.keras.Sequential([\n    tf.keras.layers.Dense(1024, activation=tf.nn.relu),\n    tf.keras.layers.Dense(1024, activation=tf.nn.relu),    \n    tf.keras.layers.Dense(1024, activation=tf.nn.relu),\n    tf.keras.layers.Dense(1024, activation=tf.nn.relu),    \n    tf.keras.layers.Dense(3832, activation=tf.nn.softmax)\n])"
        },
        {
            "cell_type": "code",
            "execution_count": 108,
            "metadata": {},
            "outputs": [],
            "source": "# model_1_2_k compile\nmodel_1_2_k.compile(optimizer='adam',\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])"
        },
        {
            "cell_type": "code",
            "execution_count": 109,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Epoch 1/20\n2809/2809 [==============================] - 26s 9ms/step - loss: 5.4430 - accuracy: 0.1814 - val_loss: 3.8729 - val_accuracy: 0.3508\nEpoch 2/20\n2809/2809 [==============================] - 25s 9ms/step - loss: 3.3847 - accuracy: 0.3924 - val_loss: 3.4035 - val_accuracy: 0.4237\nEpoch 3/20\n2809/2809 [==============================] - 25s 9ms/step - loss: 2.7520 - accuracy: 0.4693 - val_loss: 3.3251 - val_accuracy: 0.4562\nEpoch 4/20\n2809/2809 [==============================] - 26s 9ms/step - loss: 2.3661 - accuracy: 0.5229 - val_loss: 3.2626 - val_accuracy: 0.4744\nEpoch 5/20\n2809/2809 [==============================] - 25s 9ms/step - loss: 2.1251 - accuracy: 0.5528 - val_loss: 3.2481 - val_accuracy: 0.4820\nEpoch 6/20\n2809/2809 [==============================] - 25s 9ms/step - loss: 1.9293 - accuracy: 0.5812 - val_loss: 3.3912 - val_accuracy: 0.4891\nEpoch 7/20\n2809/2809 [==============================] - 25s 9ms/step - loss: 1.7918 - accuracy: 0.6028 - val_loss: 3.4771 - val_accuracy: 0.4904\nEpoch 8/20\n2809/2809 [==============================] - 25s 9ms/step - loss: 1.7049 - accuracy: 0.6174 - val_loss: 3.7173 - val_accuracy: 0.4980\nEpoch 9/20\n2809/2809 [==============================] - 26s 9ms/step - loss: 1.6255 - accuracy: 0.6320 - val_loss: 3.7000 - val_accuracy: 0.4974\nEpoch 10/20\n2809/2809 [==============================] - 25s 9ms/step - loss: 1.5617 - accuracy: 0.6402 - val_loss: 3.9172 - val_accuracy: 0.4982\nEpoch 11/20\n2809/2809 [==============================] - 25s 9ms/step - loss: 1.4897 - accuracy: 0.6541 - val_loss: 4.0353 - val_accuracy: 0.4943\nEpoch 12/20\n2809/2809 [==============================] - 25s 9ms/step - loss: 1.4704 - accuracy: 0.6590 - val_loss: 4.0323 - val_accuracy: 0.5013\nEpoch 13/20\n2809/2809 [==============================] - 25s 9ms/step - loss: 1.4244 - accuracy: 0.6687 - val_loss: 4.2674 - val_accuracy: 0.5006\nEpoch 14/20\n2809/2809 [==============================] - 25s 9ms/step - loss: 1.4331 - accuracy: 0.6685 - val_loss: 4.3925 - val_accuracy: 0.4975\nEpoch 15/20\n2809/2809 [==============================] - 26s 9ms/step - loss: 1.3898 - accuracy: 0.6772 - val_loss: 4.4749 - val_accuracy: 0.4999\nEpoch 16/20\n2809/2809 [==============================] - 26s 9ms/step - loss: 1.3866 - accuracy: 0.6781 - val_loss: 4.4981 - val_accuracy: 0.4965\nEpoch 17/20\n2809/2809 [==============================] - 25s 9ms/step - loss: 1.3509 - accuracy: 0.6867 - val_loss: 4.6075 - val_accuracy: 0.4899\nEpoch 18/20\n2809/2809 [==============================] - 25s 9ms/step - loss: 1.3724 - accuracy: 0.6848 - val_loss: 4.8141 - val_accuracy: 0.4938\nEpoch 19/20\n2809/2809 [==============================] - 26s 9ms/step - loss: 1.3495 - accuracy: 0.6891 - val_loss: 4.8556 - val_accuracy: 0.4931\nEpoch 20/20\n2809/2809 [==============================] - 25s 9ms/step - loss: 1.3427 - accuracy: 0.6916 - val_loss: 4.9066 - val_accuracy: 0.4869\n"
                },
                {
                    "data": {
                        "text/plain": "<tensorflow.python.keras.callbacks.History at 0x7f8bec16c890>"
                    },
                    "execution_count": 109,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "# model_1_2_k fit\nmodel_1_2_k.fit(x_train_kanji, y_train_kanji, epochs=20, verbose=1, validation_data=(x_val_kanji, y_val_kanji))"
        },
        {
            "cell_type": "code",
            "execution_count": 110,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "model_1_2_k test loss: 4.8285813331604\nmodel_1_2_k test accuracy 0.49211323261260986\n"
                }
            ],
            "source": "score = model_1_2_k.evaluate(x_test_kanji, y_test_kanji, verbose=0)\nprint('model_1_2_k test loss:', score[0])\nprint('model_1_2_k test accuracy', score[1])"
        },
        {
            "cell_type": "code",
            "execution_count": 59,
            "metadata": {},
            "outputs": [],
            "source": "# model_2_k def\n\nbatch_size = 128\nnum_classes = 3832       #  here we specify the number of classes as 3832 instead of 10\nepochs = 12\n\n# input image dimensions\nimg_rows, img_cols = 64, 64"
        },
        {
            "cell_type": "code",
            "execution_count": 60,
            "metadata": {},
            "outputs": [],
            "source": "x_train = x_train_kanji\nx_val = x_val_kanji\nx_test = x_test_kanji\n\ny_train = y_train_kanji\ny_val = y_val_kanji\ny_test = y_test_kanji"
        },
        {
            "cell_type": "code",
            "execution_count": 61,
            "metadata": {},
            "outputs": [],
            "source": "if K.image_data_format() == 'channels_first':\n    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n    x_val = x_val.reshape(x_val.shape[0], 1, img_rows, img_cols)\n    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n    input_shape = (1, img_rows, img_cols)\nelse:\n    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n    x_val = x_val.reshape(x_val.shape[0], img_rows, img_cols, 1)    \n    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n    input_shape = (img_rows, img_cols, 1)"
        },
        {
            "cell_type": "code",
            "execution_count": 62,
            "metadata": {},
            "outputs": [],
            "source": "model_2_k = tf.keras.Sequential()\nmodel_2_k.add(Conv2D(64, kernel_size=(3, 3),\n                 activation='relu',\n                 input_shape=input_shape))\nmodel_2_k.add(Conv2D(128, (3, 3), activation='relu'))\nmodel_2_k.add(MaxPooling2D(pool_size=(2, 2)))\nmodel_2_k.add(Dropout(0.25))\nmodel_2_k.add(tf.keras.layers.Flatten())\nmodel_2_k.add(tf.keras.layers.Dense(256, activation='relu'))\nmodel_2_k.add(Dropout(0.5))\nmodel_2_k.add(tf.keras.layers.Dense(num_classes, activation='softmax'))"
        },
        {
            "cell_type": "code",
            "execution_count": 63,
            "metadata": {},
            "outputs": [],
            "source": "# model_2 compile\n\nmodel_2_k.compile(optimizer='adam', \n          loss='sparse_categorical_crossentropy',\n          metrics=['accuracy'])"
        },
        {
            "cell_type": "code",
            "execution_count": 64,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Epoch 1/12\n703/703 [==============================] - 134s 170ms/step - loss: 5.6676 - accuracy: 0.1947 - val_loss: 3.2808 - val_accuracy: 0.5048\nEpoch 2/12\n703/703 [==============================] - 99s 141ms/step - loss: 3.2911 - accuracy: 0.4482 - val_loss: 2.4609 - val_accuracy: 0.6144\nEpoch 3/12\n703/703 [==============================] - 99s 141ms/step - loss: 2.4817 - accuracy: 0.5461 - val_loss: 2.0657 - val_accuracy: 0.6678\nEpoch 4/12\n703/703 [==============================] - 99s 141ms/step - loss: 1.9401 - accuracy: 0.6114 - val_loss: 1.8827 - val_accuracy: 0.6905\nEpoch 5/12\n703/703 [==============================] - 99s 141ms/step - loss: 1.5652 - accuracy: 0.6651 - val_loss: 1.7582 - val_accuracy: 0.7099\nEpoch 6/12\n703/703 [==============================] - 99s 141ms/step - loss: 1.2913 - accuracy: 0.7024 - val_loss: 1.7602 - val_accuracy: 0.7184\nEpoch 7/12\n703/703 [==============================] - 99s 140ms/step - loss: 1.0565 - accuracy: 0.7450 - val_loss: 1.7467 - val_accuracy: 0.7212\nEpoch 8/12\n703/703 [==============================] - 99s 140ms/step - loss: 0.8943 - accuracy: 0.7727 - val_loss: 1.8429 - val_accuracy: 0.7256\nEpoch 9/12\n703/703 [==============================] - 99s 140ms/step - loss: 0.7759 - accuracy: 0.7945 - val_loss: 1.8273 - val_accuracy: 0.7290\nEpoch 10/12\n703/703 [==============================] - 99s 140ms/step - loss: 0.6776 - accuracy: 0.8133 - val_loss: 1.8584 - val_accuracy: 0.7270\nEpoch 11/12\n703/703 [==============================] - 99s 140ms/step - loss: 0.6013 - accuracy: 0.8322 - val_loss: 1.8857 - val_accuracy: 0.7309\nEpoch 12/12\n703/703 [==============================] - 99s 140ms/step - loss: 0.5485 - accuracy: 0.8453 - val_loss: 1.9796 - val_accuracy: 0.7266\n"
                },
                {
                    "data": {
                        "text/plain": "<tensorflow.python.keras.callbacks.History at 0x7f3ca9beaf50>"
                    },
                    "execution_count": 64,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "# fit model_2_k\n\nmodel_2_k.fit(x_train, y_train,\n         batch_size=batch_size,\n         epochs=epochs,\n         verbose=1,\n         validation_data=(x_val, y_val))"
        },
        {
            "cell_type": "code",
            "execution_count": 65,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "model_2_k test loss: 1.9494651556015015\nmodel_2_k test accuracy 0.7291436791419983\n"
                }
            ],
            "source": "score = model_2_k.evaluate(x_test, y_test, verbose=0)\nprint('model_2_k test loss:', score[0])\nprint('model_2_k test accuracy', score[1])"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": ""
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.7 + GPU",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.7.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}