{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "collapsed": true
            },
            "source": "### Model Training\n\nOnce your model is defined, it can be trained. This can happen on a single thread or on a parallel framework like Watson Machine Learning or Apache Spark. In the most simple case Model Definition and Model Training is just a couple of LOCs (lines of code) away. In the case of Watson Machine Learning or Apache Spark models might need to get serialized and transferred to another technology / framework.\n\nPlease specify and justify the technologies used for model definition and training in the ADD. \n\nOnce you think you have achieved a descent model performance save the notebook according to the process model\u2019s naming convention and proceed to the model evaluation task."
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Waiting for a Spark session to start...\nSpark Initialization Done! ApplicationId = app-20210624022829-0000\nKERNEL_ID = 0ce1c979-00e6-4888-84f3-cb162ef38084\nCollecting tensorflow\n  Downloading tensorflow-2.5.0-cp37-cp37m-manylinux2010_x86_64.whl (454.3 MB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 454.3 MB 26 kB/s /s eta 0:00:014 MB 8.1 MB/s eta 0:00:52                    | 59.7 MB 8.1 MB/s eta 0:00:49\ufffd\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d                 | 203.6 MB 70.3 MB/s eta 0:00:04\n\u001b[?25hCollecting keras-preprocessing~=1.1.2\n  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 42 kB 838 kB/s  eta 0:00:01\n\u001b[?25hCollecting keras-nightly~=2.5.0.dev\n  Downloading keras_nightly-2.5.0.dev2021032900-py2.py3-none-any.whl (1.2 MB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.2 MB 58.9 MB/s eta 0:00:01\n\u001b[?25hCollecting grpcio~=1.34.0\n  Downloading grpcio-1.34.1-cp37-cp37m-manylinux2014_x86_64.whl (4.0 MB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4.0 MB 20.4 MB/s eta 0:00:01\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f                 | 1.8 MB 20.4 MB/s eta 0:00:01\n\u001b[?25hCollecting flatbuffers~=1.12.0\n  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\nCollecting wheel~=0.35\n  Downloading wheel-0.36.2-py2.py3-none-any.whl (35 kB)\nCollecting tensorflow-estimator<2.6.0,>=2.5.0rc0\n  Downloading tensorflow_estimator-2.5.0-py2.py3-none-any.whl (462 kB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 462 kB 49.0 MB/s eta 0:00:01\n\u001b[?25hCollecting tensorboard~=2.5\n  Downloading tensorboard-2.5.0-py3-none-any.whl (6.0 MB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 6.0 MB 53.0 MB/s eta 0:00:01\n\u001b[?25hCollecting termcolor~=1.1.0\n  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\nCollecting typing-extensions~=3.7.4\n  Downloading typing_extensions-3.7.4.3-py3-none-any.whl (22 kB)\nCollecting astunparse~=1.6.3\n  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\nCollecting numpy~=1.19.2\n  Downloading numpy-1.19.5-cp37-cp37m-manylinux2010_x86_64.whl (14.8 MB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 14.8 MB 54.2 MB/s eta 0:00:01.2 MB 54.2 MB/s eta 0:00:01\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e            | 8.9 MB 54.2 MB/s eta 0:00:01\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e   | 13.1 MB 54.2 MB/s eta 0:00:01\n\u001b[?25hCollecting h5py~=3.1.0\n  Downloading h5py-3.1.0-cp37-cp37m-manylinux1_x86_64.whl (4.0 MB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4.0 MB 33.5 MB/s eta 0:00:011.6 MB 33.5 MB/s eta 0:00:01\n\u001b[?25hCollecting google-pasta~=0.2\n  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 57 kB 4.0 MB/s  eta 0:00:01\n\u001b[?25hCollecting six~=1.15.0\n  Downloading six-1.15.0-py2.py3-none-any.whl (10 kB)\nCollecting opt-einsum~=3.3.0\n  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 65 kB 2.8 MB/s  eta 0:00:01\n\u001b[?25hCollecting wrapt~=1.12.1\n  Downloading wrapt-1.12.1.tar.gz (27 kB)\nCollecting protobuf>=3.9.2\n  Downloading protobuf-3.17.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.0 MB 57.7 MB/s eta 0:00:01    |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e                      | 296 kB 57.7 MB/s eta 0:00:01\n\u001b[?25hCollecting gast==0.4.0\n  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\nCollecting absl-py~=0.10\n  Downloading absl_py-0.13.0-py3-none-any.whl (132 kB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 132 kB 49.8 MB/s eta 0:00:01\n\u001b[?25hCollecting tensorboard-data-server<0.7.0,>=0.6.0\n  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4.9 MB 46.7 MB/s eta 0:00:012.7 MB 46.7 MB/s eta 0:00:01\n\u001b[?25hCollecting google-auth<2,>=1.6.3\n  Downloading google_auth-1.32.0-py2.py3-none-any.whl (147 kB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 147 kB 23.6 MB/s eta 0:00:01\n\u001b[?25hCollecting tensorboard-plugin-wit>=1.6.0\n  Downloading tensorboard_plugin_wit-1.8.0-py3-none-any.whl (781 kB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 781 kB 42.6 MB/s eta 0:00:01\n\u001b[?25hCollecting markdown>=2.6.8\n  Downloading Markdown-3.3.4-py3-none-any.whl (97 kB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 97 kB 5.5 MB/s  eta 0:00:01\n\u001b[?25hCollecting google-auth-oauthlib<0.5,>=0.4.1\n  Downloading google_auth_oauthlib-0.4.4-py2.py3-none-any.whl (18 kB)\nCollecting setuptools>=41.0.0\n  Downloading setuptools-57.0.0-py3-none-any.whl (821 kB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 821 kB 27.8 MB/s eta 0:00:01\n\u001b[?25hCollecting werkzeug>=0.11.15\n  Downloading Werkzeug-2.0.1-py3-none-any.whl (288 kB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 288 kB 34.4 MB/s eta 0:00:01\n\u001b[?25hCollecting requests<3,>=2.21.0\n  Downloading requests-2.25.1-py2.py3-none-any.whl (61 kB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 61 kB 6.5 MB/s  eta 0:00:01\n\u001b[?25hCollecting cached-property; python_version < \"3.8\"\n  Downloading cached_property-1.5.2-py2.py3-none-any.whl (7.6 kB)\nCollecting cachetools<5.0,>=2.0.0\n  Downloading cachetools-4.2.2-py3-none-any.whl (11 kB)\nCollecting rsa<5,>=3.1.4; python_version >= \"3.6\"\n  Downloading rsa-4.7.2-py3-none-any.whl (34 kB)\nCollecting pyasn1-modules>=0.2.1\n  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 155 kB 63.2 MB/s eta 0:00:01\n\u001b[?25hCollecting importlib-metadata; python_version < \"3.8\"\n  Downloading importlib_metadata-4.5.0-py3-none-any.whl (17 kB)\nCollecting requests-oauthlib>=0.7.0\n  Downloading requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\nCollecting certifi>=2017.4.17\n  Downloading certifi-2021.5.30-py2.py3-none-any.whl (145 kB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 145 kB 55.2 MB/s eta 0:00:01\n\u001b[?25hCollecting urllib3<1.27,>=1.21.1\n  Downloading urllib3-1.26.5-py2.py3-none-any.whl (138 kB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 138 kB 49.6 MB/s eta 0:00:01\n\u001b[?25hCollecting idna<3,>=2.5\n  Downloading idna-2.10-py2.py3-none-any.whl (58 kB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 58 kB 4.1 MB/s  eta 0:00:01\n\u001b[?25hCollecting chardet<5,>=3.0.2\n  Downloading chardet-4.0.0-py2.py3-none-any.whl (178 kB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 178 kB 47.1 MB/s eta 0:00:01\n\u001b[?25hCollecting pyasn1>=0.1.3\n  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 77 kB 4.7 MB/s  eta 0:00:01\n\u001b[?25hCollecting zipp>=0.5\n  Downloading zipp-3.4.1-py3-none-any.whl (5.2 kB)\nCollecting oauthlib>=3.0.0\n  Downloading oauthlib-3.1.1-py2.py3-none-any.whl (146 kB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 146 kB 56.6 MB/s eta 0:00:01\n\u001b[?25hBuilding wheels for collected packages: termcolor, wrapt\n  Building wheel for termcolor (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4830 sha256=ef843b5147ec485cc9200024242268300dedfc5aaa41bce45bdf000db50e8f54\n  Stored in directory: /home/spark/shared/.cache/pip/wheels/3f/e3/ec/8a8336ff196023622fbcb36de0c5a5c218cbb24111d1d4c7f2\n  Building wheel for wrapt (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for wrapt: filename=wrapt-1.12.1-cp37-cp37m-linux_x86_64.whl size=76683 sha256=8b8a794cdfdcfb839984092a56fc4e19963e52697d14c2b94e48b438dacc0c20\n  Stored in directory: /home/spark/shared/.cache/pip/wheels/62/76/4c/aa25851149f3f6d9785f6c869387ad82b3fd37582fa8147ac6\nSuccessfully built termcolor wrapt\n\u001b[31mERROR: conda 4.8.2 requires ruamel_yaml>=0.11.14, which is not installed.\u001b[0m\n\u001b[31mERROR: botocore 1.16.11 has requirement urllib3<1.26,>=1.20, but you'll have urllib3 1.26.5 which is incompatible.\u001b[0m\n\u001b[31mERROR: aiohttp 3.6.2 has requirement chardet<4.0,>=2.0, but you'll have chardet 4.0.0 which is incompatible.\u001b[0m\nInstalling collected packages: numpy, six, keras-preprocessing, keras-nightly, grpcio, flatbuffers, wheel, tensorflow-estimator, tensorboard-data-server, cachetools, pyasn1, rsa, pyasn1-modules, setuptools, google-auth, tensorboard-plugin-wit, absl-py, typing-extensions, zipp, importlib-metadata, markdown, certifi, urllib3, idna, chardet, requests, oauthlib, requests-oauthlib, google-auth-oauthlib, werkzeug, protobuf, tensorboard, termcolor, astunparse, cached-property, h5py, google-pasta, opt-einsum, wrapt, gast, tensorflow\nSuccessfully installed absl-py-0.13.0 astunparse-1.6.3 cached-property-1.5.2 cachetools-4.2.2 certifi-2021.5.30 chardet-4.0.0 flatbuffers-1.12 gast-0.4.0 google-auth-1.32.0 google-auth-oauthlib-0.4.4 google-pasta-0.2.0 grpcio-1.34.1 h5py-3.1.0 idna-2.10 importlib-metadata-4.5.0 keras-nightly-2.5.0.dev2021032900 keras-preprocessing-1.1.2 markdown-3.3.4 numpy-1.19.5 oauthlib-3.1.1 opt-einsum-3.3.0 protobuf-3.17.3 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-2.25.1 requests-oauthlib-1.3.0 rsa-4.7.2 setuptools-57.0.0 six-1.15.0 tensorboard-2.5.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.0 tensorflow-2.5.0 tensorflow-estimator-2.5.0 termcolor-1.1.0 typing-extensions-3.7.4.3 urllib3-1.26.5 werkzeug-2.0.1 wheel-0.36.2 wrapt-1.12.1 zipp-3.4.1\nCollecting python-mnist\n  Downloading python_mnist-0.7-py2.py3-none-any.whl (9.6 kB)\nInstalling collected packages: python-mnist\nSuccessfully installed python-mnist-0.7\n\u001b[33mWARNING: Target directory /home/spark/shared/user-libs/python3.7/bin already exists. Specify --upgrade to force replacement.\u001b[0m\nCollecting Pillow\n  Downloading Pillow-8.2.0-cp37-cp37m-manylinux1_x86_64.whl (3.0 MB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3.0 MB 20.1 MB/s eta 0:00:01\n\u001b[?25hInstalling collected packages: Pillow\nSuccessfully installed Pillow-8.2.0\nCollecting pyspark\n  Downloading pyspark-3.1.2.tar.gz (212.4 MB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 212.4 MB 112 kB/s  eta 0:00:01              | 63.3 MB 15.5 MB/s eta 0:00:10\n\u001b[?25hCollecting py4j==0.10.9\n  Downloading py4j-0.10.9-py2.py3-none-any.whl (198 kB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 198 kB 83.5 MB/s eta 0:00:01\n\u001b[?25hBuilding wheels for collected packages: pyspark\n  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.1.2-py2.py3-none-any.whl size=212880768 sha256=532d03fafa44071ad06bfaa407c6b5f7fde21196770a11dc43a2f543e9b500ed\n  Stored in directory: /home/spark/shared/.cache/pip/wheels/a5/0a/c1/9561f6fecb759579a7d863dcd846daaa95f598744e71b02c77\nSuccessfully built pyspark\n\u001b[31mERROR: sparktspy-nojars 2.0.5.0 has requirement pyspark==3.0.1, but you'll have pyspark 3.1.2 which is incompatible.\u001b[0m\nInstalling collected packages: py4j, pyspark\nSuccessfully installed py4j-0.10.9 pyspark-3.1.2\n\u001b[33mWARNING: Target directory /home/spark/shared/user-libs/python3.7/bin already exists. Specify --upgrade to force replacement.\u001b[0m\nCollecting scikit-learn\n  Downloading scikit_learn-0.24.2-cp37-cp37m-manylinux2010_x86_64.whl (22.3 MB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 22.3 MB 13.5 MB/s eta 0:00:01\n\u001b[?25hCollecting scipy>=0.19.1\n  Downloading scipy-1.7.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (28.5 MB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 28.5 MB 29.3 MB/s eta 0:00:01\n\u001b[?25hCollecting numpy>=1.13.3\n  Downloading numpy-1.21.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 15.7 MB 40.0 MB/s eta 0:00:01\n\u001b[?25hCollecting joblib>=0.11\n  Downloading joblib-1.0.1-py3-none-any.whl (303 kB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 303 kB 63.9 MB/s eta 0:00:01\n\u001b[?25hCollecting threadpoolctl>=2.0.0\n  Downloading threadpoolctl-2.1.0-py3-none-any.whl (12 kB)\n\u001b[31mERROR: tensorflow 2.5.0 has requirement numpy~=1.19.2, but you'll have numpy 1.21.0 which is incompatible.\u001b[0m\n\u001b[31mERROR: sparktspy-nojars 2.0.5.0 has requirement pyspark==3.0.1, but you'll have pyspark 3.1.2 which is incompatible.\u001b[0m\nInstalling collected packages: numpy, scipy, joblib, threadpoolctl, scikit-learn\nSuccessfully installed joblib-1.0.1 numpy-1.21.0 scikit-learn-0.24.2 scipy-1.7.0 threadpoolctl-2.1.0\n\u001b[33mWARNING: Target directory /home/spark/shared/user-libs/python3.7/__pycache__ already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mWARNING: Target directory /home/spark/shared/user-libs/python3.7/numpy.libs already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mWARNING: Target directory /home/spark/shared/user-libs/python3.7/numpy already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mWARNING: Target directory /home/spark/shared/user-libs/python3.7/bin already exists. Specify --upgrade to force replacement.\u001b[0m\n"
                }
            ],
            "source": "!pip install tensorflow\n!pip install python-mnist\n!pip install Pillow\n!pip install pyspark\n!pip install scikit-learn"
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "'2.5.0'"
                    },
                    "execution_count": 2,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "%matplotlib inline\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom matplotlib import image\nimport tensorflow as tf\nimport seaborn as sns\nfrom mnist import MNIST\nimport numpy as np\nimport PIL\nfrom PIL import Image\nimport os\nimport matplotlib.image as mping\nfrom pyspark import SparkContext, SparkConf\nfrom pyspark.sql import SparkSession\nfrom pyspark.ml.feature import StringIndexer\nfrom numpy import asarray\nimport pandas as pd\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout\nfrom tensorflow.keras import backend as K\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.decomposition import PCA\n\ntf.__version__"
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "'8.2.0'"
                    },
                    "execution_count": 3,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "PIL.__version__"
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": "# fire up the spark session\n# comment this cell out for a spark server\n#sc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\"))\n\n#spark = SparkSession \\\n#    .builder \\\n#    .getOrCreate()"
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": "# enable arrow which lets us transfrom a pandas dataframe into a pyspark dataframe\nspark.conf.set(\"spark.sql.execution.arrow.enabled\",\"true\")"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## First dataset\n\n#### 60,000 images and 10 classes, each image is 28 x 28 or represented by a 794 element array"
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "--2021-06-21 23:08:31--  http://codh.rois.ac.jp/kmnist/dataset/kmnist/train-images-idx3-ubyte.gz?raw=True\nResolving codh.rois.ac.jp (codh.rois.ac.jp)... 136.187.88.58\nConnecting to codh.rois.ac.jp (codh.rois.ac.jp)|136.187.88.58|:80... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 18165135 (17M)\nSaving to: \u2018train-images-idx3-ubyte.gz?raw=True\u2019\n\ntrain-images-idx3-u 100%[===================>]  17.32M  5.94MB/s    in 2.9s    \n\n2021-06-21 23:08:36 (5.94 MB/s) - \u2018train-images-idx3-ubyte.gz?raw=True\u2019 saved [18165135/18165135]\n\n-rw-rw-r-- 1 spark spark 45M Feb  4  2019 train-images-idx3-ubyte\n--2021-06-21 23:08:39--  http://codh.rois.ac.jp/kmnist/dataset/kmnist/train-labels-idx1-ubyte.gz?raw=True\nResolving codh.rois.ac.jp (codh.rois.ac.jp)... 136.187.88.58\nConnecting to codh.rois.ac.jp (codh.rois.ac.jp)|136.187.88.58|:80... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 29497 (29K)\nSaving to: \u2018train-labels-idx1-ubyte.gz?raw=True\u2019\n\ntrain-labels-idx1-u 100%[===================>]  28.81K  --.-KB/s    in 0.1s    \n\n2021-06-21 23:08:40 (209 KB/s) - \u2018train-labels-idx1-ubyte.gz?raw=True\u2019 saved [29497/29497]\n\n-rw-rw-r-- 1 spark spark 59K Feb  4  2019 train-labels-idx1-ubyte\n--2021-06-21 23:08:43--  http://codh.rois.ac.jp/kmnist/dataset/kmnist/t10k-images-idx3-ubyte.gz?raw=True\nResolving codh.rois.ac.jp (codh.rois.ac.jp)... 136.187.88.58\nConnecting to codh.rois.ac.jp (codh.rois.ac.jp)|136.187.88.58|:80... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 3041136 (2.9M)\nSaving to: \u2018t10k-images-idx3-ubyte.gz?raw=True\u2019\n\nt10k-images-idx3-ub 100%[===================>]   2.90M  1.78MB/s    in 1.6s    \n\n2021-06-21 23:08:45 (1.78 MB/s) - \u2018t10k-images-idx3-ubyte.gz?raw=True\u2019 saved [3041136/3041136]\n\n-rw-rw-r-- 1 spark spark 7.5M Feb  4  2019 t10k-images-idx3-ubyte\n--2021-06-21 23:08:47--  http://codh.rois.ac.jp/kmnist/dataset/kmnist/t10k-labels-idx1-ubyte.gz?raw=True\nResolving codh.rois.ac.jp (codh.rois.ac.jp)... 136.187.88.58\nConnecting to codh.rois.ac.jp (codh.rois.ac.jp)|136.187.88.58|:80... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 5120 (5.0K)\nSaving to: \u2018t10k-labels-idx1-ubyte.gz?raw=True\u2019\n\nt10k-labels-idx1-ub 100%[===================>]   5.00K  --.-KB/s    in 0s      \n\n2021-06-21 23:08:48 (143 MB/s) - \u2018t10k-labels-idx1-ubyte.gz?raw=True\u2019 saved [5120/5120]\n\n-rw-rw-r-- 1 spark spark 9.8K Feb  4  2019 t10k-labels-idx1-ubyte\n"
                }
            ],
            "source": "!wget http://codh.rois.ac.jp/kmnist/dataset/kmnist/train-images-idx3-ubyte.gz?raw=True\n!mv train-images-idx3-ubyte.gz?raw=True train-images-idx3-ubyte.gz\n!gunzip train-images-idx3-ubyte.gz\n!ls -lahr train-images-idx3-ubyte\n\n!wget http://codh.rois.ac.jp/kmnist/dataset/kmnist/train-labels-idx1-ubyte.gz?raw=True\n!mv train-labels-idx1-ubyte.gz?raw=True train-labels-idx1-ubyte.gz\n!gunzip train-labels-idx1-ubyte.gz\n!ls -lahr train-labels-idx1-ubyte\n\n!wget http://codh.rois.ac.jp/kmnist/dataset/kmnist/t10k-images-idx3-ubyte.gz?raw=True\n!mv t10k-images-idx3-ubyte.gz?raw=True t10k-images-idx3-ubyte.gz\n!gunzip t10k-images-idx3-ubyte.gz\n!ls -lahr t10k-images-idx3-ubyte\n\n!wget http://codh.rois.ac.jp/kmnist/dataset/kmnist/t10k-labels-idx1-ubyte.gz?raw=True\n!mv t10k-labels-idx1-ubyte.gz?raw=True t10k-labels-idx1-ubyte.gz\n!gunzip t10k-labels-idx1-ubyte.gz\n!ls -lahr t10k-labels-idx1-ubyte"
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "total 53908\r\ndrwxrwxr-x  2 spark spark     4096 Jun 21 23:08 .\r\ndrwxr-xr-x 11 spark  2000     4096 Jun 21 23:08 ..\r\n-rw-rw-r--  1 spark spark  7840016 Jun 21 23:08 t10k-images-idx3-ubyte\r\n-rw-rw-r--  1 spark spark    10008 Jun 21 23:08 t10k-labels-idx1-ubyte\r\n-rw-rw-r--  1 spark spark 47040016 Jun 21 23:08 train-images-idx3-ubyte\r\n-rw-rw-r--  1 spark spark    60008 Jun 21 23:08 train-labels-idx1-ubyte\r\n"
                }
            ],
            "source": "!mkdir kmnistdata\n!cp t10k-images-idx3-ubyte kmnistdata/t10k-images-idx3-ubyte\n!cp t10k-labels-idx1-ubyte kmnistdata/t10k-labels-idx1-ubyte\n!cp train-images-idx3-ubyte kmnistdata/train-images-idx3-ubyte\n!cp train-labels-idx1-ubyte kmnistdata/train-labels-idx1-ubyte\n!ls -al kmnistdata\n\ndata = MNIST('kmnistdata')\ntrain_images, train_labels = data.load_training()\ntest_images, test_labels = data.load_testing()\n\ntrain_images = np.array(train_images)\ntrain_labels = np.array(train_labels)\ntest_images = np.array(test_images)\ntest_labels = np.array(test_labels)\n\ntrain_images = train_images / 255\ntest_images = test_images / 255"
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [],
            "source": "# model_1 def\n\nmodel_1 = tf.keras.Sequential([\n    tf.keras.layers.Dense(128, activation=tf.nn.relu),\n    tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n])"
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [],
            "source": "# model_1 compile\nmodel_1.compile(optimizer='adam',\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])"
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Epoch 1/5\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.4048 - accuracy: 0.8803 - val_loss: 0.5066 - val_accuracy: 0.8456\nEpoch 2/5\n1875/1875 [==============================] - 3s 2ms/step - loss: 0.1947 - accuracy: 0.9426 - val_loss: 0.4295 - val_accuracy: 0.8718\nEpoch 3/5\n1875/1875 [==============================] - 3s 2ms/step - loss: 0.1385 - accuracy: 0.9597 - val_loss: 0.3975 - val_accuracy: 0.8907\nEpoch 4/5\n1875/1875 [==============================] - 3s 2ms/step - loss: 0.1047 - accuracy: 0.9696 - val_loss: 0.4275 - val_accuracy: 0.8801\nEpoch 5/5\n1875/1875 [==============================] - 3s 2ms/step - loss: 0.0809 - accuracy: 0.9761 - val_loss: 0.3971 - val_accuracy: 0.8982\n"
                },
                {
                    "data": {
                        "text/plain": "<tensorflow.python.keras.callbacks.History at 0x7fb78c0a2750>"
                    },
                    "execution_count": 10,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "# model_1 fit\n\n# began by using 5 epochs here but observed that \n# the validation accuracy is increasing even as we\n# stopped training, so consider using more epochs here next\n\nmodel_1.fit(train_images, train_labels, epochs=5, verbose=1, validation_data=(test_images, test_labels))"
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "model_1 test loss: 0.3971461057662964\nmodel_1 test accuracy 0.8981999754905701\n"
                }
            ],
            "source": "score = model_1.evaluate(test_images, test_labels, verbose=0)\nprint('model_1 test loss:', score[0])\nprint('model_1 test accuracy', score[1])"
        },
        {
            "cell_type": "code",
            "execution_count": 38,
            "metadata": {},
            "outputs": [],
            "source": "# model_1_1 def\n\nmodel_1_1 = tf.keras.Sequential([\n    tf.keras.layers.Dense(128, activation=tf.nn.relu),\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Dense(128, activation=tf.nn.relu),    \n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n])"
        },
        {
            "cell_type": "code",
            "execution_count": 39,
            "metadata": {},
            "outputs": [],
            "source": "# model_1_1 compile\nmodel_1_1.compile(optimizer='adam',\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])"
        },
        {
            "cell_type": "code",
            "execution_count": 40,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Epoch 1/5\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.4807 - accuracy: 0.8521 - val_loss: 0.5372 - val_accuracy: 0.8344\nEpoch 2/5\n1875/1875 [==============================] - 3s 2ms/step - loss: 0.2692 - accuracy: 0.9171 - val_loss: 0.4496 - val_accuracy: 0.8587\nEpoch 3/5\n1875/1875 [==============================] - 3s 2ms/step - loss: 0.2181 - accuracy: 0.9329 - val_loss: 0.4092 - val_accuracy: 0.8790\nEpoch 4/5\n1875/1875 [==============================] - 3s 2ms/step - loss: 0.1801 - accuracy: 0.9444 - val_loss: 0.3915 - val_accuracy: 0.8871\nEpoch 5/5\n1875/1875 [==============================] - 3s 2ms/step - loss: 0.1662 - accuracy: 0.9484 - val_loss: 0.3839 - val_accuracy: 0.8890\n"
                },
                {
                    "data": {
                        "text/plain": "<tensorflow.python.keras.callbacks.History at 0x7f3b8c3a7c50>"
                    },
                    "execution_count": 40,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "# model_1_1 fit\nmodel_1_1.fit(train_images, train_labels, epochs=5, verbose=1, validation_data=(test_images, test_labels))"
        },
        {
            "cell_type": "code",
            "execution_count": 41,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Model: \"sequential_10\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense_33 (Dense)             (None, 128)               100480    \n_________________________________________________________________\ndropout_9 (Dropout)          (None, 128)               0         \n_________________________________________________________________\ndense_34 (Dense)             (None, 128)               16512     \n_________________________________________________________________\ndropout_10 (Dropout)         (None, 128)               0         \n_________________________________________________________________\ndense_35 (Dense)             (None, 10)                1290      \n=================================================================\nTotal params: 118,282\nTrainable params: 118,282\nNon-trainable params: 0\n_________________________________________________________________\n"
                }
            ],
            "source": "model_1_1.summary()"
        },
        {
            "cell_type": "code",
            "execution_count": 52,
            "metadata": {},
            "outputs": [],
            "source": "# model_1_2 def\n\nmodel_1_2 = tf.keras.Sequential([\n    tf.keras.layers.Dense(128, activation=tf.nn.relu),\n    tf.keras.layers.Dense(128, activation=tf.nn.relu),    \n    tf.keras.layers.Dense(128, activation=tf.nn.relu),\n    tf.keras.layers.Dense(128, activation=tf.nn.relu),    \n    tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n])"
        },
        {
            "cell_type": "code",
            "execution_count": 53,
            "metadata": {},
            "outputs": [],
            "source": "# model_1_2 compile\nmodel_1_2.compile(optimizer='adam',\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])"
        },
        {
            "cell_type": "code",
            "execution_count": 54,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Epoch 1/20\n1875/1875 [==============================] - 5s 2ms/step - loss: 0.3722 - accuracy: 0.8857 - val_loss: 0.5198 - val_accuracy: 0.8383\nEpoch 2/20\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.1791 - accuracy: 0.9460 - val_loss: 0.4599 - val_accuracy: 0.8676\nEpoch 3/20\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.1261 - accuracy: 0.9617 - val_loss: 0.4123 - val_accuracy: 0.8814\nEpoch 4/20\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.1046 - accuracy: 0.9681 - val_loss: 0.4151 - val_accuracy: 0.8933\nEpoch 5/20\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.0839 - accuracy: 0.9747 - val_loss: 0.4230 - val_accuracy: 0.8908\nEpoch 6/20\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.0675 - accuracy: 0.9791 - val_loss: 0.4180 - val_accuracy: 0.8970\nEpoch 7/20\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.0614 - accuracy: 0.9805 - val_loss: 0.4154 - val_accuracy: 0.8987\nEpoch 8/20\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.0523 - accuracy: 0.9837 - val_loss: 0.4947 - val_accuracy: 0.8912\nEpoch 9/20\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.0468 - accuracy: 0.9852 - val_loss: 0.4445 - val_accuracy: 0.8974\nEpoch 10/20\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.0417 - accuracy: 0.9872 - val_loss: 0.5009 - val_accuracy: 0.8985\nEpoch 11/20\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.0375 - accuracy: 0.9882 - val_loss: 0.4986 - val_accuracy: 0.9005\nEpoch 12/20\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.0343 - accuracy: 0.9891 - val_loss: 0.5347 - val_accuracy: 0.8962\nEpoch 13/20\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.0330 - accuracy: 0.9898 - val_loss: 0.5516 - val_accuracy: 0.9018\nEpoch 14/20\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.0302 - accuracy: 0.9909 - val_loss: 0.5276 - val_accuracy: 0.9038\nEpoch 15/20\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.0294 - accuracy: 0.9913 - val_loss: 0.5073 - val_accuracy: 0.9056\nEpoch 16/20\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.0250 - accuracy: 0.9926 - val_loss: 0.5310 - val_accuracy: 0.9074\nEpoch 17/20\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.0256 - accuracy: 0.9925 - val_loss: 0.5616 - val_accuracy: 0.9026\nEpoch 18/20\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.0248 - accuracy: 0.9924 - val_loss: 0.5574 - val_accuracy: 0.9090\nEpoch 19/20\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.0238 - accuracy: 0.9928 - val_loss: 0.6296 - val_accuracy: 0.9029\nEpoch 20/20\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.0228 - accuracy: 0.9933 - val_loss: 0.6076 - val_accuracy: 0.9082\n"
                },
                {
                    "data": {
                        "text/plain": "<tensorflow.python.keras.callbacks.History at 0x7f3b6c554e10>"
                    },
                    "execution_count": 54,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "# model_1_2 fit\nmodel_1_2.fit(train_images, train_labels, epochs=20, verbose=1, validation_data=(test_images, test_labels))"
        },
        {
            "cell_type": "code",
            "execution_count": 45,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Model: \"sequential_11\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense_36 (Dense)             (None, 128)               100480    \n_________________________________________________________________\ndense_37 (Dense)             (None, 128)               16512     \n_________________________________________________________________\ndense_38 (Dense)             (None, 128)               16512     \n_________________________________________________________________\ndense_39 (Dense)             (None, 128)               16512     \n_________________________________________________________________\ndense_40 (Dense)             (None, 10)                1290      \n=================================================================\nTotal params: 151,306\nTrainable params: 151,306\nNon-trainable params: 0\n_________________________________________________________________\n"
                }
            ],
            "source": "model_1_2.summary()"
        },
        {
            "cell_type": "code",
            "execution_count": 49,
            "metadata": {},
            "outputs": [],
            "source": "# model_1_3 def\n\nmodel_1_3 = tf.keras.Sequential([\n    tf.keras.layers.Dense(128, activation=tf.nn.relu),\n    tf.keras.layers.Dense(128, activation=tf.nn.relu),    \n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Dense(128, activation=tf.nn.relu),\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Dense(128, activation=tf.nn.relu),    \n    tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n])"
        },
        {
            "cell_type": "code",
            "execution_count": 50,
            "metadata": {},
            "outputs": [],
            "source": "# model_1_3 compile\nmodel_1_3.compile(optimizer='adam',\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])"
        },
        {
            "cell_type": "code",
            "execution_count": 51,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Epoch 1/20\n1875/1875 [==============================] - 5s 2ms/step - loss: 0.4513 - accuracy: 0.8613 - val_loss: 0.5261 - val_accuracy: 0.8399\nEpoch 2/20\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.2188 - accuracy: 0.9348 - val_loss: 0.4477 - val_accuracy: 0.8650\nEpoch 3/20\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.1648 - accuracy: 0.9506 - val_loss: 0.4268 - val_accuracy: 0.8837\nEpoch 4/20\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.1326 - accuracy: 0.9604 - val_loss: 0.3916 - val_accuracy: 0.8929\nEpoch 5/20\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.1138 - accuracy: 0.9666 - val_loss: 0.3983 - val_accuracy: 0.8895\nEpoch 6/20\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.0943 - accuracy: 0.9714 - val_loss: 0.4388 - val_accuracy: 0.8899\nEpoch 7/20\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.0860 - accuracy: 0.9750 - val_loss: 0.4374 - val_accuracy: 0.8923\nEpoch 8/20\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.0774 - accuracy: 0.9767 - val_loss: 0.4261 - val_accuracy: 0.8929\nEpoch 9/20\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.0683 - accuracy: 0.9790 - val_loss: 0.4281 - val_accuracy: 0.8980\nEpoch 10/20\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.0612 - accuracy: 0.9820 - val_loss: 0.4724 - val_accuracy: 0.8949\nEpoch 11/20\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.0572 - accuracy: 0.9829 - val_loss: 0.4482 - val_accuracy: 0.9027\nEpoch 12/20\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.0510 - accuracy: 0.9846 - val_loss: 0.4692 - val_accuracy: 0.9042\nEpoch 13/20\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.0499 - accuracy: 0.9849 - val_loss: 0.5359 - val_accuracy: 0.8918\nEpoch 14/20\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.0467 - accuracy: 0.9859 - val_loss: 0.4440 - val_accuracy: 0.9007\nEpoch 15/20\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.0414 - accuracy: 0.9876 - val_loss: 0.5110 - val_accuracy: 0.9058\nEpoch 16/20\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.0405 - accuracy: 0.9877 - val_loss: 0.5444 - val_accuracy: 0.9010\nEpoch 17/20\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.0398 - accuracy: 0.9880 - val_loss: 0.5134 - val_accuracy: 0.9040\nEpoch 18/20\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.0363 - accuracy: 0.9894 - val_loss: 0.5336 - val_accuracy: 0.8990\nEpoch 19/20\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.0350 - accuracy: 0.9893 - val_loss: 0.5940 - val_accuracy: 0.8990\nEpoch 20/20\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.0346 - accuracy: 0.9898 - val_loss: 0.6163 - val_accuracy: 0.8973\n"
                },
                {
                    "data": {
                        "text/plain": "<tensorflow.python.keras.callbacks.History at 0x7f3b6c6dec90>"
                    },
                    "execution_count": 51,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "# model_1_3 fit\nmodel_1_3.fit(train_images, train_labels, epochs=20, verbose=1, validation_data=(test_images, test_labels))"
        },
        {
            "cell_type": "code",
            "execution_count": 55,
            "metadata": {},
            "outputs": [],
            "source": "# model_1_4 def\n\nmodel_1_4 = tf.keras.Sequential([\n    tf.keras.layers.Dense(128, activation=tf.nn.relu),\n    tf.keras.layers.Dense(128, activation=tf.nn.relu),    \n    tf.keras.layers.Dense(128, activation=tf.nn.relu),\n    tf.keras.layers.Dense(128, activation=tf.nn.relu),    \n    tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n])"
        },
        {
            "cell_type": "code",
            "execution_count": 56,
            "metadata": {},
            "outputs": [],
            "source": "# model_1_4 compile\n\nmodel_1_4.compile(optimizer=tf.keras.optimizers.Adadelta(),\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])"
        },
        {
            "cell_type": "code",
            "execution_count": 57,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Epoch 1/20\n1875/1875 [==============================] - 5s 3ms/step - loss: 2.2887 - accuracy: 0.1389 - val_loss: 2.2826 - val_accuracy: 0.1397\nEpoch 2/20\n1875/1875 [==============================] - 4s 2ms/step - loss: 2.2387 - accuracy: 0.2098 - val_loss: 2.2480 - val_accuracy: 0.1768\nEpoch 3/20\n1875/1875 [==============================] - 4s 2ms/step - loss: 2.1765 - accuracy: 0.2908 - val_loss: 2.2032 - val_accuracy: 0.2277\nEpoch 4/20\n1875/1875 [==============================] - 4s 2ms/step - loss: 2.0938 - accuracy: 0.3853 - val_loss: 2.1450 - val_accuracy: 0.2882\nEpoch 5/20\n1875/1875 [==============================] - 4s 2ms/step - loss: 1.9884 - accuracy: 0.4529 - val_loss: 2.0727 - val_accuracy: 0.3376\nEpoch 6/20\n1875/1875 [==============================] - 4s 2ms/step - loss: 1.8645 - accuracy: 0.5017 - val_loss: 1.9919 - val_accuracy: 0.3773\nEpoch 7/20\n1875/1875 [==============================] - 4s 2ms/step - loss: 1.7342 - accuracy: 0.5435 - val_loss: 1.9111 - val_accuracy: 0.4105\nEpoch 8/20\n1875/1875 [==============================] - 4s 2ms/step - loss: 1.6090 - accuracy: 0.5814 - val_loss: 1.8350 - val_accuracy: 0.4467\nEpoch 9/20\n1875/1875 [==============================] - 4s 2ms/step - loss: 1.4955 - accuracy: 0.6195 - val_loss: 1.7680 - val_accuracy: 0.4764\nEpoch 10/20\n1875/1875 [==============================] - 4s 2ms/step - loss: 1.3955 - accuracy: 0.6489 - val_loss: 1.7075 - val_accuracy: 0.4958\nEpoch 11/20\n1875/1875 [==============================] - 4s 2ms/step - loss: 1.3091 - accuracy: 0.6693 - val_loss: 1.6542 - val_accuracy: 0.5120\nEpoch 12/20\n1875/1875 [==============================] - 4s 2ms/step - loss: 1.2341 - accuracy: 0.6838 - val_loss: 1.6048 - val_accuracy: 0.5242\nEpoch 13/20\n1875/1875 [==============================] - 4s 2ms/step - loss: 1.1688 - accuracy: 0.6944 - val_loss: 1.5617 - val_accuracy: 0.5323\nEpoch 14/20\n1875/1875 [==============================] - 4s 2ms/step - loss: 1.1123 - accuracy: 0.7027 - val_loss: 1.5190 - val_accuracy: 0.5401\nEpoch 15/20\n1875/1875 [==============================] - 4s 2ms/step - loss: 1.0628 - accuracy: 0.7106 - val_loss: 1.4837 - val_accuracy: 0.5479\nEpoch 16/20\n1875/1875 [==============================] - 4s 2ms/step - loss: 1.0198 - accuracy: 0.7178 - val_loss: 1.4493 - val_accuracy: 0.5571\nEpoch 17/20\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.9816 - accuracy: 0.7244 - val_loss: 1.4182 - val_accuracy: 0.5667\nEpoch 18/20\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.9480 - accuracy: 0.7317 - val_loss: 1.3857 - val_accuracy: 0.5752\nEpoch 19/20\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.9178 - accuracy: 0.7390 - val_loss: 1.3616 - val_accuracy: 0.5834\nEpoch 20/20\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.8906 - accuracy: 0.7456 - val_loss: 1.3349 - val_accuracy: 0.5905\n"
                },
                {
                    "data": {
                        "text/plain": "<tensorflow.python.keras.callbacks.History at 0x7f3b6c3f87d0>"
                    },
                    "execution_count": 57,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "# model_1_4 fit\n\nmodel_1_4.fit(train_images, train_labels, epochs=20, verbose=1, validation_data=(test_images, test_labels))"
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [],
            "source": "# model_1_4_1 def (using the dropouts with adadelta)\n\nmodel_1_4_1 = tf.keras.Sequential([\n    tf.keras.layers.Dense(128, activation=tf.nn.relu),\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Dense(128, activation=tf.nn.relu),    \n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n])"
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [],
            "source": "# model_1_4_1 compile\n\nmodel_1_4_1.compile(optimizer=tf.keras.optimizers.Adadelta(learning_rate = 1.0),\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])"
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Epoch 1/20\n1875/1875 [==============================] - 11s 5ms/step - loss: 0.4941 - accuracy: 0.8462 - val_loss: 0.5475 - val_accuracy: 0.8331\nEpoch 2/20\n1875/1875 [==============================] - 10s 5ms/step - loss: 0.2808 - accuracy: 0.9155 - val_loss: 0.4634 - val_accuracy: 0.8607\nEpoch 3/20\n1875/1875 [==============================] - 10s 5ms/step - loss: 0.2289 - accuracy: 0.9313 - val_loss: 0.4422 - val_accuracy: 0.8734\nEpoch 4/20\n1875/1875 [==============================] - 10s 5ms/step - loss: 0.1979 - accuracy: 0.9408 - val_loss: 0.4326 - val_accuracy: 0.8782\nEpoch 5/20\n1875/1875 [==============================] - 10s 5ms/step - loss: 0.1789 - accuracy: 0.9456 - val_loss: 0.4243 - val_accuracy: 0.8819\nEpoch 6/20\n1875/1875 [==============================] - 10s 5ms/step - loss: 0.1636 - accuracy: 0.9510 - val_loss: 0.4209 - val_accuracy: 0.8848\nEpoch 7/20\n1875/1875 [==============================] - 10s 5ms/step - loss: 0.1542 - accuracy: 0.9545 - val_loss: 0.3964 - val_accuracy: 0.8936\nEpoch 8/20\n1875/1875 [==============================] - 10s 5ms/step - loss: 0.1447 - accuracy: 0.9556 - val_loss: 0.4173 - val_accuracy: 0.8922\nEpoch 9/20\n1875/1875 [==============================] - 10s 5ms/step - loss: 0.1362 - accuracy: 0.9597 - val_loss: 0.4295 - val_accuracy: 0.8891\nEpoch 10/20\n1875/1875 [==============================] - 10s 5ms/step - loss: 0.1293 - accuracy: 0.9609 - val_loss: 0.4238 - val_accuracy: 0.8954\nEpoch 11/20\n1875/1875 [==============================] - 10s 5ms/step - loss: 0.1231 - accuracy: 0.9637 - val_loss: 0.4679 - val_accuracy: 0.8877\nEpoch 12/20\n1875/1875 [==============================] - 10s 5ms/step - loss: 0.1186 - accuracy: 0.9649 - val_loss: 0.4539 - val_accuracy: 0.8894\nEpoch 13/20\n1875/1875 [==============================] - 10s 5ms/step - loss: 0.1153 - accuracy: 0.9664 - val_loss: 0.4661 - val_accuracy: 0.8929\nEpoch 14/20\n1875/1875 [==============================] - 10s 5ms/step - loss: 0.1087 - accuracy: 0.9673 - val_loss: 0.4636 - val_accuracy: 0.8960\nEpoch 15/20\n1875/1875 [==============================] - 10s 5ms/step - loss: 0.1086 - accuracy: 0.9675 - val_loss: 0.4513 - val_accuracy: 0.8941\nEpoch 16/20\n1875/1875 [==============================] - 10s 5ms/step - loss: 0.1083 - accuracy: 0.9685 - val_loss: 0.4880 - val_accuracy: 0.8916\nEpoch 17/20\n1875/1875 [==============================] - 10s 5ms/step - loss: 0.1003 - accuracy: 0.9703 - val_loss: 0.4925 - val_accuracy: 0.8889\nEpoch 18/20\n1875/1875 [==============================] - 10s 5ms/step - loss: 0.1022 - accuracy: 0.9703 - val_loss: 0.4843 - val_accuracy: 0.8965\nEpoch 19/20\n1875/1875 [==============================] - 10s 5ms/step - loss: 0.0969 - accuracy: 0.9719 - val_loss: 0.5149 - val_accuracy: 0.8913\nEpoch 20/20\n1875/1875 [==============================] - 10s 5ms/step - loss: 0.0938 - accuracy: 0.9726 - val_loss: 0.4912 - val_accuracy: 0.8949\n"
                },
                {
                    "data": {
                        "text/plain": "<tensorflow.python.keras.callbacks.History at 0x7f09a868ced0>"
                    },
                    "execution_count": 11,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "# model_1_4_1 fit\n\nmodel_1_4_1.fit(train_images, train_labels, epochs=20, verbose=1, validation_data=(test_images, test_labels))"
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": "# model_1_5 def\n\nmodel_1_5 = tf.keras.Sequential([\n    tf.keras.layers.Dense(128, activation=tf.nn.relu),\n    tf.keras.layers.Dense(128, activation=tf.nn.relu),    \n    tf.keras.layers.Dense(128, activation=tf.nn.relu),\n    tf.keras.layers.Dense(128, activation=tf.nn.relu),    \n    tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n])"
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": "# model_1_5 compile\n\nmodel_1_5.compile(optimizer='sgd',\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])"
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Epoch 1/20\n1875/1875 [==============================] - 5s 2ms/step - loss: 0.9446 - accuracy: 0.7149 - val_loss: 0.8656 - val_accuracy: 0.7227\nEpoch 2/20\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.4174 - accuracy: 0.8721 - val_loss: 0.6876 - val_accuracy: 0.7840\nEpoch 3/20\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.3243 - accuracy: 0.9013 - val_loss: 0.6232 - val_accuracy: 0.8074\nEpoch 4/20\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.2668 - accuracy: 0.9203 - val_loss: 0.5939 - val_accuracy: 0.8173\nEpoch 5/20\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.2259 - accuracy: 0.9320 - val_loss: 0.5436 - val_accuracy: 0.8390\nEpoch 6/20\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.1958 - accuracy: 0.9427 - val_loss: 0.5060 - val_accuracy: 0.8485\nEpoch 7/20\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.1698 - accuracy: 0.9505 - val_loss: 0.4889 - val_accuracy: 0.8544\nEpoch 8/20\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.1493 - accuracy: 0.9557 - val_loss: 0.4861 - val_accuracy: 0.8551\nEpoch 9/20\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.1316 - accuracy: 0.9611 - val_loss: 0.4982 - val_accuracy: 0.8576\nEpoch 10/20\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.1163 - accuracy: 0.9651 - val_loss: 0.4720 - val_accuracy: 0.8610\nEpoch 11/20\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.1039 - accuracy: 0.9694 - val_loss: 0.4492 - val_accuracy: 0.8764\nEpoch 12/20\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.0923 - accuracy: 0.9735 - val_loss: 0.4679 - val_accuracy: 0.8724\nEpoch 13/20\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.0813 - accuracy: 0.9767 - val_loss: 0.4561 - val_accuracy: 0.8786\nEpoch 14/20\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.0712 - accuracy: 0.9801 - val_loss: 0.4690 - val_accuracy: 0.8754\nEpoch 15/20\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.0625 - accuracy: 0.9822 - val_loss: 0.4710 - val_accuracy: 0.8786\nEpoch 16/20\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.0550 - accuracy: 0.9853 - val_loss: 0.4933 - val_accuracy: 0.8764\nEpoch 17/20\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.0479 - accuracy: 0.9873 - val_loss: 0.4819 - val_accuracy: 0.8811\nEpoch 18/20\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.0416 - accuracy: 0.9893 - val_loss: 0.5052 - val_accuracy: 0.8844\nEpoch 19/20\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.0362 - accuracy: 0.9910 - val_loss: 0.5249 - val_accuracy: 0.8793\nEpoch 20/20\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.0309 - accuracy: 0.9926 - val_loss: 0.5060 - val_accuracy: 0.8876\n"
                },
                {
                    "data": {
                        "text/plain": "<tensorflow.python.keras.callbacks.History at 0x7f292876d990>"
                    },
                    "execution_count": 7,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "# model_1_5 fit\n\nmodel_1_5.fit(train_images, train_labels, epochs=20, verbose=1, validation_data=(test_images, test_labels))"
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {},
            "outputs": [],
            "source": "# model_1_5_1 def\n\nmodel_1_5_1 = tf.keras.Sequential([\n    tf.keras.layers.Dense(128, activation=tf.nn.relu),\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Dense(128, activation=tf.nn.relu),    \n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n])"
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {},
            "outputs": [],
            "source": "# model_1_5_1 compile\n\nmodel_1_5_1.compile(optimizer='sgd',\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])"
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Epoch 1/20\n1875/1875 [==============================] - 9s 5ms/step - loss: 1.0603 - accuracy: 0.6719 - val_loss: 0.9947 - val_accuracy: 0.6860\nEpoch 2/20\n1875/1875 [==============================] - 8s 4ms/step - loss: 0.6080 - accuracy: 0.8130 - val_loss: 0.8254 - val_accuracy: 0.7384\nEpoch 3/20\n1875/1875 [==============================] - 8s 4ms/step - loss: 0.4905 - accuracy: 0.8513 - val_loss: 0.7144 - val_accuracy: 0.7748\nEpoch 4/20\n1875/1875 [==============================] - 8s 4ms/step - loss: 0.4224 - accuracy: 0.8715 - val_loss: 0.6567 - val_accuracy: 0.7926\nEpoch 5/20\n1875/1875 [==============================] - 8s 4ms/step - loss: 0.3763 - accuracy: 0.8852 - val_loss: 0.6121 - val_accuracy: 0.8071\nEpoch 6/20\n1875/1875 [==============================] - 8s 4ms/step - loss: 0.3469 - accuracy: 0.8936 - val_loss: 0.5718 - val_accuracy: 0.8230\nEpoch 7/20\n1875/1875 [==============================] - 8s 4ms/step - loss: 0.3212 - accuracy: 0.9036 - val_loss: 0.5500 - val_accuracy: 0.8311\nEpoch 8/20\n1875/1875 [==============================] - 8s 4ms/step - loss: 0.2959 - accuracy: 0.9097 - val_loss: 0.5295 - val_accuracy: 0.8357\nEpoch 9/20\n1875/1875 [==============================] - 8s 4ms/step - loss: 0.2794 - accuracy: 0.9142 - val_loss: 0.5106 - val_accuracy: 0.8435\nEpoch 10/20\n1875/1875 [==============================] - 9s 5ms/step - loss: 0.2658 - accuracy: 0.9189 - val_loss: 0.4877 - val_accuracy: 0.8524\nEpoch 11/20\n1875/1875 [==============================] - 8s 4ms/step - loss: 0.2510 - accuracy: 0.9240 - val_loss: 0.4828 - val_accuracy: 0.8541\nEpoch 12/20\n1875/1875 [==============================] - 8s 4ms/step - loss: 0.2429 - accuracy: 0.9262 - val_loss: 0.4670 - val_accuracy: 0.8604\nEpoch 13/20\n1875/1875 [==============================] - 8s 4ms/step - loss: 0.2320 - accuracy: 0.9297 - val_loss: 0.4575 - val_accuracy: 0.8632\nEpoch 14/20\n1875/1875 [==============================] - 8s 4ms/step - loss: 0.2228 - accuracy: 0.9322 - val_loss: 0.4611 - val_accuracy: 0.8650\nEpoch 15/20\n1875/1875 [==============================] - 8s 4ms/step - loss: 0.2132 - accuracy: 0.9349 - val_loss: 0.4471 - val_accuracy: 0.8698\nEpoch 16/20\n1875/1875 [==============================] - 8s 4ms/step - loss: 0.2066 - accuracy: 0.9368 - val_loss: 0.4404 - val_accuracy: 0.8699\nEpoch 17/20\n1875/1875 [==============================] - 8s 4ms/step - loss: 0.1998 - accuracy: 0.9385 - val_loss: 0.4384 - val_accuracy: 0.8742\nEpoch 18/20\n1875/1875 [==============================] - 9s 5ms/step - loss: 0.1924 - accuracy: 0.9409 - val_loss: 0.4326 - val_accuracy: 0.8723\nEpoch 19/20\n1875/1875 [==============================] - 8s 4ms/step - loss: 0.1882 - accuracy: 0.9423 - val_loss: 0.4210 - val_accuracy: 0.8777\nEpoch 20/20\n1875/1875 [==============================] - 8s 4ms/step - loss: 0.1818 - accuracy: 0.9448 - val_loss: 0.4250 - val_accuracy: 0.8776\n"
                },
                {
                    "data": {
                        "text/plain": "<tensorflow.python.keras.callbacks.History at 0x7fbfc2a95110>"
                    },
                    "execution_count": 19,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "# model_1_5_1 fit\n\nmodel_1_5_1.fit(train_images, train_labels, epochs=20, verbose=1, validation_data=(test_images, test_labels))"
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [],
            "source": "# model_2 def\n\n# optimizer is adam and loss is sparse_categorical_crossentropy\n\nbatch_size = 128\nnum_classes = 10\nepochs = 12\n\n# input image dimensions\nimg_rows, img_cols = 28, 28\n\nx_train = train_images\nx_test = test_images\ny_train = train_labels\ny_test = test_labels\n\nif K.image_data_format() == 'channels_first':\n    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n    input_shape = (1, img_rows, img_cols)\nelse:\n    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n    input_shape = (img_rows, img_cols, 1)\n\nmodel_2 = tf.keras.Sequential()\nmodel_2.add(Conv2D(32, kernel_size=(3, 3),\n                 activation='relu',\n                 input_shape=input_shape))\nmodel_2.add(Conv2D(64, (3, 3), activation='relu'))\nmodel_2.add(MaxPooling2D(pool_size=(2, 2)))\nmodel_2.add(Dropout(0.25))\nmodel_2.add(tf.keras.layers.Flatten())\nmodel_2.add(tf.keras.layers.Dense(128, activation='relu'))\nmodel_2.add(Dropout(0.5))\nmodel_2.add(tf.keras.layers.Dense(num_classes, activation='softmax'))\n\n# model_2 compile\n\nmodel_2.compile(optimizer='adam', \n          loss='sparse_categorical_crossentropy',\n          metrics=['accuracy'])"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Epoch 1/12\n469/469 [==============================] - 211s 450ms/step - loss: 0.4129 - accuracy: 0.8742 - val_loss: 0.3417 - val_accuracy: 0.9001\nEpoch 2/12\n469/469 [==============================] - 209s 445ms/step - loss: 0.1737 - accuracy: 0.9475 - val_loss: 0.2736 - val_accuracy: 0.9198\nEpoch 3/12\n469/469 [==============================] - 208s 443ms/step - loss: 0.1290 - accuracy: 0.9598 - val_loss: 0.2315 - val_accuracy: 0.9371\nEpoch 4/12\n469/469 [==============================] - 209s 446ms/step - loss: 0.1016 - accuracy: 0.9682 - val_loss: 0.2087 - val_accuracy: 0.9409\nEpoch 5/12\n469/469 [==============================] - 208s 443ms/step - loss: 0.0820 - accuracy: 0.9740 - val_loss: 0.2093 - val_accuracy: 0.9436\nEpoch 6/12\n469/469 [==============================] - 208s 443ms/step - loss: 0.0701 - accuracy: 0.9777 - val_loss: 0.2163 - val_accuracy: 0.9427\nEpoch 7/12\n469/469 [==============================] - 208s 443ms/step - loss: 0.0663 - accuracy: 0.9783 - val_loss: 0.2089 - val_accuracy: 0.9478\nEpoch 8/12\n469/469 [==============================] - 208s 444ms/step - loss: 0.0553 - accuracy: 0.9814 - val_loss: 0.2130 - val_accuracy: 0.9489\nEpoch 9/12\n469/469 [==============================] - 208s 443ms/step - loss: 0.0512 - accuracy: 0.9829 - val_loss: 0.1924 - val_accuracy: 0.9526\nEpoch 10/12\n469/469 [==============================] - 208s 443ms/step - loss: 0.0460 - accuracy: 0.9845 - val_loss: 0.2268 - val_accuracy: 0.9472\nEpoch 11/12\n469/469 [==============================] - 200s 426ms/step - loss: 0.0430 - accuracy: 0.9858 - val_loss: 0.2108 - val_accuracy: 0.9530\nEpoch 12/12\n469/469 [==============================] - 197s 421ms/step - loss: 0.0372 - accuracy: 0.9871 - val_loss: 0.2121 - val_accuracy: 0.9539\n"
                },
                {
                    "data": {
                        "text/plain": "<tensorflow.python.keras.callbacks.History at 0x7fe4603c49d0>"
                    },
                    "execution_count": 13,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "# fit model\n\nmodel_2.fit(x_train, y_train,\n         batch_size=batch_size,\n         epochs=epochs,\n         verbose=1,\n         validation_data=(x_test, y_test))"
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [],
            "source": "# model_2_1 def : change of optimizer to Adadelta and compile\n\nmodel_2_1 = tf.keras.Sequential()\nmodel_2_1.add(Conv2D(32, kernel_size=(3, 3),\n                 activation='relu',\n                 input_shape=input_shape))\nmodel_2_1.add(Conv2D(64, (3, 3), activation='relu'))\nmodel_2_1.add(MaxPooling2D(pool_size=(2, 2)))\nmodel_2_1.add(Dropout(0.25))\nmodel_2_1.add(tf.keras.layers.Flatten())\nmodel_2_1.add(tf.keras.layers.Dense(128, activation='relu'))\nmodel_2_1.add(Dropout(0.5))\nmodel_2_1.add(tf.keras.layers.Dense(num_classes, activation='softmax'))\n\nmodel_2_1.compile(loss='sparse_categorical_crossentropy',\n              optimizer=tf.keras.optimizers.Adadelta(learning_rate=1.0),\n              metrics=['accuracy'])"
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Epoch 1/12\n469/469 [==============================] - 209s 444ms/step - loss: 0.4418 - accuracy: 0.8609 - val_loss: 0.3767 - val_accuracy: 0.8808\nEpoch 2/12\n469/469 [==============================] - 206s 439ms/step - loss: 0.1749 - accuracy: 0.9479 - val_loss: 0.2798 - val_accuracy: 0.9196\nEpoch 3/12\n469/469 [==============================] - 206s 439ms/step - loss: 0.1318 - accuracy: 0.9610 - val_loss: 0.2514 - val_accuracy: 0.9303\nEpoch 4/12\n469/469 [==============================] - 205s 437ms/step - loss: 0.1067 - accuracy: 0.9678 - val_loss: 0.2469 - val_accuracy: 0.9308\nEpoch 5/12\n469/469 [==============================] - 205s 438ms/step - loss: 0.0929 - accuracy: 0.9725 - val_loss: 0.2341 - val_accuracy: 0.9393\nEpoch 6/12\n469/469 [==============================] - 205s 438ms/step - loss: 0.0788 - accuracy: 0.9766 - val_loss: 0.2202 - val_accuracy: 0.9429\nEpoch 7/12\n469/469 [==============================] - 205s 438ms/step - loss: 0.0723 - accuracy: 0.9777 - val_loss: 0.2165 - val_accuracy: 0.9387\nEpoch 8/12\n469/469 [==============================] - 204s 436ms/step - loss: 0.0669 - accuracy: 0.9795 - val_loss: 0.2265 - val_accuracy: 0.9465\nEpoch 9/12\n469/469 [==============================] - 204s 436ms/step - loss: 0.0630 - accuracy: 0.9810 - val_loss: 0.2334 - val_accuracy: 0.9471\nEpoch 10/12\n469/469 [==============================] - 205s 438ms/step - loss: 0.0575 - accuracy: 0.9815 - val_loss: 0.2249 - val_accuracy: 0.9502\nEpoch 11/12\n469/469 [==============================] - 205s 438ms/step - loss: 0.0532 - accuracy: 0.9835 - val_loss: 0.2498 - val_accuracy: 0.9475\nEpoch 12/12\n469/469 [==============================] - 205s 438ms/step - loss: 0.0520 - accuracy: 0.9841 - val_loss: 0.2111 - val_accuracy: 0.9489\n"
                },
                {
                    "data": {
                        "text/plain": "<tensorflow.python.keras.callbacks.History at 0x7f0994d1c9d0>"
                    },
                    "execution_count": 15,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "model_2_1.fit(x_train, y_train,\n           batch_size=batch_size,\n           epochs=epochs,\n           verbose=1,\n           validation_data=(x_test, y_test))"
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [],
            "source": "# optimizer = Adamax\n\n# model_2_2: change of optimizer to Adamax and compile\n\nmodel_2_2 = tf.keras.Sequential()\nmodel_2_2.add(Conv2D(32, kernel_size=(3, 3),\n                 activation='relu',\n                 input_shape=input_shape))\nmodel_2_2.add(Conv2D(64, (3, 3), activation='relu'))\nmodel_2_2.add(MaxPooling2D(pool_size=(2, 2)))\nmodel_2_2.add(Dropout(0.25))\nmodel_2_2.add(tf.keras.layers.Flatten())\nmodel_2_2.add(tf.keras.layers.Dense(128, activation='relu'))\nmodel_2_2.add(Dropout(0.5))\nmodel_2_2.add(tf.keras.layers.Dense(num_classes, activation='softmax'))\n\nmodel_2_2.compile(loss='sparse_categorical_crossentropy',\n              optimizer='Adamax',\n              metrics=['accuracy'])"
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Epoch 1/12\n469/469 [==============================] - 95s 202ms/step - loss: 0.5973 - accuracy: 0.8165 - val_loss: 0.5072 - val_accuracy: 0.8421\nEpoch 2/12\n469/469 [==============================] - 93s 199ms/step - loss: 0.2788 - accuracy: 0.9162 - val_loss: 0.3876 - val_accuracy: 0.8847\nEpoch 3/12\n469/469 [==============================] - 94s 200ms/step - loss: 0.2121 - accuracy: 0.9376 - val_loss: 0.3238 - val_accuracy: 0.9056\nEpoch 4/12\n469/469 [==============================] - 93s 199ms/step - loss: 0.1723 - accuracy: 0.9477 - val_loss: 0.3036 - val_accuracy: 0.9139\nEpoch 5/12\n469/469 [==============================] - 94s 200ms/step - loss: 0.1514 - accuracy: 0.9544 - val_loss: 0.2632 - val_accuracy: 0.9246\nEpoch 6/12\n469/469 [==============================] - 94s 200ms/step - loss: 0.1321 - accuracy: 0.9598 - val_loss: 0.2511 - val_accuracy: 0.9304\nEpoch 7/12\n469/469 [==============================] - 93s 198ms/step - loss: 0.1150 - accuracy: 0.9647 - val_loss: 0.2460 - val_accuracy: 0.9293\nEpoch 8/12\n469/469 [==============================] - 93s 199ms/step - loss: 0.1053 - accuracy: 0.9676 - val_loss: 0.2406 - val_accuracy: 0.9351\nEpoch 9/12\n469/469 [==============================] - 93s 199ms/step - loss: 0.0962 - accuracy: 0.9700 - val_loss: 0.2287 - val_accuracy: 0.9390\nEpoch 10/12\n469/469 [==============================] - 93s 198ms/step - loss: 0.0885 - accuracy: 0.9720 - val_loss: 0.2302 - val_accuracy: 0.9392\nEpoch 11/12\n469/469 [==============================] - 93s 199ms/step - loss: 0.0829 - accuracy: 0.9743 - val_loss: 0.2284 - val_accuracy: 0.9391\nEpoch 12/12\n469/469 [==============================] - 93s 197ms/step - loss: 0.0761 - accuracy: 0.9758 - val_loss: 0.2371 - val_accuracy: 0.9387\n"
                },
                {
                    "data": {
                        "text/plain": "<tensorflow.python.keras.callbacks.History at 0x7f618c5133d0>"
                    },
                    "execution_count": 12,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "model_2_2.fit(x_train, y_train,\n           batch_size=batch_size,\n           epochs=epochs,\n           verbose=1,\n           validation_data=(x_test, y_test))"
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [],
            "source": "# optimizer = Adagrad\n\n# model_2_3: change of optimizer to Adagrad and compile\n\nmodel_2_3 = tf.keras.Sequential()\nmodel_2_3.add(Conv2D(32, kernel_size=(3, 3),\n                 activation='relu',\n                 input_shape=input_shape))\nmodel_2_3.add(Conv2D(64, (3, 3), activation='relu'))\nmodel_2_3.add(MaxPooling2D(pool_size=(2, 2)))\nmodel_2_3.add(Dropout(0.25))\nmodel_2_3.add(tf.keras.layers.Flatten())\nmodel_2_3.add(tf.keras.layers.Dense(128, activation='relu'))\nmodel_2_3.add(Dropout(0.5))\nmodel_2_3.add(tf.keras.layers.Dense(num_classes, activation='softmax'))\n\nmodel_2_3.compile(loss='sparse_categorical_crossentropy',\n              optimizer='Adagrad',\n              metrics=['accuracy'])"
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Epoch 1/12\n469/469 [==============================] - 214s 455ms/step - loss: 2.0159 - accuracy: 0.3416 - val_loss: 1.6746 - val_accuracy: 0.5227\nEpoch 2/12\n469/469 [==============================] - 213s 454ms/step - loss: 1.1844 - accuracy: 0.6353 - val_loss: 1.2752 - val_accuracy: 0.6165\nEpoch 3/12\n469/469 [==============================] - 211s 450ms/step - loss: 0.9188 - accuracy: 0.7169 - val_loss: 1.1081 - val_accuracy: 0.6604\nEpoch 4/12\n469/469 [==============================] - 213s 454ms/step - loss: 0.8054 - accuracy: 0.7503 - val_loss: 1.0051 - val_accuracy: 0.6873\nEpoch 5/12\n469/469 [==============================] - 212s 452ms/step - loss: 0.7404 - accuracy: 0.7710 - val_loss: 0.9493 - val_accuracy: 0.7028\nEpoch 6/12\n469/469 [==============================] - 212s 451ms/step - loss: 0.6963 - accuracy: 0.7845 - val_loss: 0.8988 - val_accuracy: 0.7176\nEpoch 7/12\n469/469 [==============================] - 213s 453ms/step - loss: 0.6649 - accuracy: 0.7959 - val_loss: 0.8592 - val_accuracy: 0.7281\nEpoch 8/12\n469/469 [==============================] - 212s 452ms/step - loss: 0.6365 - accuracy: 0.8021 - val_loss: 0.8345 - val_accuracy: 0.7337\nEpoch 9/12\n469/469 [==============================] - 213s 453ms/step - loss: 0.6149 - accuracy: 0.8087 - val_loss: 0.8111 - val_accuracy: 0.7466\nEpoch 10/12\n469/469 [==============================] - 213s 455ms/step - loss: 0.6023 - accuracy: 0.8135 - val_loss: 0.7878 - val_accuracy: 0.7509\nEpoch 11/12\n469/469 [==============================] - 213s 455ms/step - loss: 0.5855 - accuracy: 0.8193 - val_loss: 0.7791 - val_accuracy: 0.7539\nEpoch 12/12\n469/469 [==============================] - 213s 455ms/step - loss: 0.5701 - accuracy: 0.8239 - val_loss: 0.7549 - val_accuracy: 0.7618\n"
                },
                {
                    "data": {
                        "text/plain": "<tensorflow.python.keras.callbacks.History at 0x7fbfc2bc2e90>"
                    },
                    "execution_count": 15,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "model_2_3.fit(x_train, y_train,\n           batch_size=batch_size,\n           epochs=epochs,\n           verbose=1,\n           validation_data=(x_test, y_test))"
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [],
            "source": "# optimizer = Nadam\n\n# model_2_4: change of optimizer to Nadam and compile\n\nmodel_2_4 = tf.keras.Sequential()\nmodel_2_4.add(Conv2D(32, kernel_size=(3, 3),\n                 activation='relu',\n                 input_shape=input_shape))\nmodel_2_4.add(Conv2D(64, (3, 3), activation='relu'))\nmodel_2_4.add(MaxPooling2D(pool_size=(2, 2)))\nmodel_2_4.add(Dropout(0.25))\nmodel_2_4.add(tf.keras.layers.Flatten())\nmodel_2_4.add(tf.keras.layers.Dense(128, activation='relu'))\nmodel_2_4.add(Dropout(0.5))\nmodel_2_4.add(tf.keras.layers.Dense(num_classes, activation='softmax'))\n\nmodel_2_4.compile(loss='sparse_categorical_crossentropy',\n              optimizer='Nadam',\n              metrics=['accuracy'])"
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Epoch 1/12\n469/469 [==============================] - 97s 206ms/step - loss: 0.4006 - accuracy: 0.8765 - val_loss: 0.3753 - val_accuracy: 0.8869\nEpoch 2/12\n469/469 [==============================] - 95s 203ms/step - loss: 0.1686 - accuracy: 0.9496 - val_loss: 0.2696 - val_accuracy: 0.9219\nEpoch 3/12\n469/469 [==============================] - 95s 204ms/step - loss: 0.1271 - accuracy: 0.9604 - val_loss: 0.2380 - val_accuracy: 0.9340\nEpoch 4/12\n469/469 [==============================] - 96s 204ms/step - loss: 0.0979 - accuracy: 0.9690 - val_loss: 0.2271 - val_accuracy: 0.9407\nEpoch 5/12\n469/469 [==============================] - 96s 204ms/step - loss: 0.0836 - accuracy: 0.9736 - val_loss: 0.2184 - val_accuracy: 0.9427\nEpoch 6/12\n469/469 [==============================] - 96s 204ms/step - loss: 0.0676 - accuracy: 0.9787 - val_loss: 0.1975 - val_accuracy: 0.9490\nEpoch 7/12\n469/469 [==============================] - 96s 204ms/step - loss: 0.0602 - accuracy: 0.9804 - val_loss: 0.2283 - val_accuracy: 0.9439\nEpoch 8/12\n469/469 [==============================] - 95s 203ms/step - loss: 0.0541 - accuracy: 0.9819 - val_loss: 0.1975 - val_accuracy: 0.9522\nEpoch 9/12\n469/469 [==============================] - 96s 204ms/step - loss: 0.0479 - accuracy: 0.9840 - val_loss: 0.2026 - val_accuracy: 0.9545\nEpoch 10/12\n469/469 [==============================] - 95s 203ms/step - loss: 0.0440 - accuracy: 0.9853 - val_loss: 0.2121 - val_accuracy: 0.9550\nEpoch 11/12\n469/469 [==============================] - 96s 204ms/step - loss: 0.0388 - accuracy: 0.9871 - val_loss: 0.2211 - val_accuracy: 0.9544\nEpoch 12/12\n469/469 [==============================] - 96s 205ms/step - loss: 0.0367 - accuracy: 0.9879 - val_loss: 0.2240 - val_accuracy: 0.9539\n"
                },
                {
                    "data": {
                        "text/plain": "<tensorflow.python.keras.callbacks.History at 0x7f61798bef50>"
                    },
                    "execution_count": 17,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "model_2_4.fit(x_train, y_train,\n           batch_size=batch_size,\n           epochs=epochs,\n           verbose=1,\n           validation_data=(x_test, y_test))"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "#### This is the first dataset in .npz format"
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "--2021-06-11 17:21:46--  http://codh.rois.ac.jp/kmnist/dataset/kmnist/kmnist-train-imgs.npz?raw=True\nResolving codh.rois.ac.jp (codh.rois.ac.jp)... 136.187.88.58\nConnecting to codh.rois.ac.jp (codh.rois.ac.jp)|136.187.88.58|:80... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 18384171 (18M)\nSaving to: \u2018kmnist-train-imgs.npz?raw=True\u2019\n\nkmnist-train-imgs.n 100%[===================>]  17.53M  4.25MB/s    in 4.9s    \n\n2021-06-11 17:21:52 (3.61 MB/s) - \u2018kmnist-train-imgs.npz?raw=True\u2019 saved [18384171/18384171]\n\n-rw-rw-r-- 1 spark spark 18M Feb  4  2019 kmnist-train-imgs.npz\n--2021-06-11 17:21:54--  http://codh.rois.ac.jp/kmnist/dataset/kmnist/kmnist-train-labels.npz?raw=True\nResolving codh.rois.ac.jp (codh.rois.ac.jp)... 136.187.88.58\nConnecting to codh.rois.ac.jp (codh.rois.ac.jp)|136.187.88.58|:80... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 29700 (29K)\nSaving to: \u2018kmnist-train-labels.npz?raw=True\u2019\n\nkmnist-train-labels 100%[===================>]  29.00K  --.-KB/s    in 0.1s    \n\n2021-06-11 17:21:54 (211 KB/s) - \u2018kmnist-train-labels.npz?raw=True\u2019 saved [29700/29700]\n\n-rw-rw-r-- 1 spark spark 30K Feb  4  2019 kmnist-train-labels.npz\n--2021-06-11 17:21:56--  http://codh.rois.ac.jp/kmnist/dataset/kmnist/kmnist-test-imgs.npz?raw=True\nResolving codh.rois.ac.jp (codh.rois.ac.jp)... 136.187.88.58\nConnecting to codh.rois.ac.jp (codh.rois.ac.jp)|136.187.88.58|:80... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 3079479 (2.9M)\nSaving to: \u2018kmnist-test-imgs.npz?raw=True\u2019\n\nkmnist-test-imgs.np 100%[===================>]   2.94M  2.30MB/s    in 1.3s    \n\n2021-06-11 17:21:58 (2.30 MB/s) - \u2018kmnist-test-imgs.npz?raw=True\u2019 saved [3079479/3079479]\n\n-rw-rw-r-- 1 spark spark 3.0M Feb  4  2019 kmnist-test-imgs.npz\n--2021-06-11 17:22:00--  http://codh.rois.ac.jp/kmnist/dataset/kmnist/kmnist-test-labels.npz?raw=True\nResolving codh.rois.ac.jp (codh.rois.ac.jp)... 136.187.88.58\nConnecting to codh.rois.ac.jp (codh.rois.ac.jp)|136.187.88.58|:80... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 5304 (5.2K)\nSaving to: \u2018kmnist-test-labels.npz?raw=True\u2019\n\nkmnist-test-labels. 100%[===================>]   5.18K  --.-KB/s    in 0s      \n\n2021-06-11 17:22:00 (137 MB/s) - \u2018kmnist-test-labels.npz?raw=True\u2019 saved [5304/5304]\n\n-rw-rw-r-- 1 spark spark 5.2K Feb  4  2019 kmnist-test-labels.npz\n"
                }
            ],
            "source": "!wget http://codh.rois.ac.jp/kmnist/dataset/kmnist/kmnist-train-imgs.npz?raw=True\n!mv kmnist-train-imgs.npz?raw=True kmnist-train-imgs.npz\n!ls -lahr kmnist-train-imgs.npz\n\n!wget http://codh.rois.ac.jp/kmnist/dataset/kmnist/kmnist-train-labels.npz?raw=True\n!mv kmnist-train-labels.npz?raw=True kmnist-train-labels.npz\n!ls -lahr kmnist-train-labels.npz\n\n!wget http://codh.rois.ac.jp/kmnist/dataset/kmnist/kmnist-test-imgs.npz?raw=True\n!mv kmnist-test-imgs.npz?raw=True kmnist-test-imgs.npz\n!ls -lahr kmnist-test-imgs.npz\n\n!wget http://codh.rois.ac.jp/kmnist/dataset/kmnist/kmnist-test-labels.npz?raw=True\n!mv kmnist-test-labels.npz?raw=True kmnist-test-labels.npz\n!ls -lahr kmnist-test-labels.npz"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### Model:  K Nearest Neighbors"
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [],
            "source": "def load(f):\n    return np.load(f)['arr_0']"
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {},
            "outputs": [],
            "source": "x_train = load('kmnist-train-imgs.npz')\ny_train = load('kmnist-train-labels.npz')\nx_test = load('kmnist-test-imgs.npz')\ny_test = load('kmnist-test-labels.npz')"
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {},
            "outputs": [],
            "source": "x_train = x_train.reshape(-1, 784)\nx_test = x_test.reshape(-1, 784)"
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Fit KNeighborsClassifier(n_jobs=-1, n_neighbors=4, weights='distance')\n"
                },
                {
                    "data": {
                        "text/plain": "KNeighborsClassifier(n_jobs=-1, n_neighbors=4, weights='distance')"
                    },
                    "execution_count": 19,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "model_3 = KNeighborsClassifier(n_neighbors=4, weights='distance', n_jobs=-1)\nprint('Fit', model_3)\nmodel_3.fit(x_train, y_train)"
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Evaluate KNeighborsClassifier(n_jobs=-1, n_neighbors=4, weights='distance')\nThe accuracy is: 0.921\n"
                }
            ],
            "source": "print('Evaluate', model_3)\nscore = model_3.score(x_test, y_test)\nprint('The accuracy is:', score)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### Model: K Nearest Neighbors with Principal Component Analysis dimensionality reduction"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# KNN with dim reduction:  the code is currently broken, skip this for now\n\npca = PCA(n_components=60, random_state=0)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "x_train = pca.fit_transform(x_train)\nx_test = pca.transform"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "model_4 = KNeighborsClassifier(n_neighbors=4, weights='distance', n_jobs=-1)\nprint('Fit', model_4)\nmodel_4.fit(x_train, y_train)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "print('Evaluate', model_4)\nscore = model_4.score(x_test, y_test)\nprint('The accuracy is:', score)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Second dataset\n\n#### 232,365 images, 49 classes, each image is 28 x 28 or represented by a 794 element array"
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "--2021-06-21 23:59:46--  http://codh.rois.ac.jp/kmnist/dataset/k49/k49-train-imgs.npz?raw=True\nResolving codh.rois.ac.jp (codh.rois.ac.jp)... 136.187.88.58\nConnecting to codh.rois.ac.jp (codh.rois.ac.jp)|136.187.88.58|:80... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 66117696 (63M)\nSaving to: \u2018k49-train-imgs.npz?raw=True\u2019\n\nk49-train-imgs.npz? 100%[===================>]  63.05M  3.18MB/s    in 20s     \n\n2021-06-22 00:00:07 (3.14 MB/s) - \u2018k49-train-imgs.npz?raw=True\u2019 saved [66117696/66117696]\n\n-rw-rw-r-- 1 spark spark 64M Feb  4  2019 k49-train-imgs.npz\n--2021-06-22 00:00:09--  http://codh.rois.ac.jp/kmnist/dataset/k49/k49-train-labels.npz?raw=True\nResolving codh.rois.ac.jp (codh.rois.ac.jp)... 136.187.88.58\nConnecting to codh.rois.ac.jp (codh.rois.ac.jp)|136.187.88.58|:80... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 164485 (161K)\nSaving to: \u2018k49-train-labels.npz?raw=True\u2019\n\nk49-train-labels.np 100%[===================>] 160.63K   383KB/s    in 0.4s    \n\n2021-06-22 00:00:10 (383 KB/s) - \u2018k49-train-labels.npz?raw=True\u2019 saved [164485/164485]\n\n-rw-rw-r-- 1 spark spark 161K Feb  4  2019 k49-train-labels.npz\n--2021-06-22 00:00:12--  http://codh.rois.ac.jp/kmnist/dataset/k49/k49-test-imgs.npz?raw=True\nResolving codh.rois.ac.jp (codh.rois.ac.jp)... 136.187.88.58\nConnecting to codh.rois.ac.jp (codh.rois.ac.jp)|136.187.88.58|:80... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 10971201 (10M)\nSaving to: \u2018k49-test-imgs.npz?raw=True\u2019\n\nk49-test-imgs.npz?r 100%[===================>]  10.46M  2.67MB/s    in 4.4s    \n\n2021-06-22 00:00:17 (2.38 MB/s) - \u2018k49-test-imgs.npz?raw=True\u2019 saved [10971201/10971201]\n\n-rw-rw-r-- 1 spark spark 11M Feb  4  2019 k49-test-imgs.npz\n--2021-06-22 00:00:19--  http://codh.rois.ac.jp/kmnist/dataset/k49/k49-test-labels.npz?raw=True\nResolving codh.rois.ac.jp (codh.rois.ac.jp)... 136.187.88.58\nConnecting to codh.rois.ac.jp (codh.rois.ac.jp)|136.187.88.58|:80... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 27450 (27K)\nSaving to: \u2018k49-test-labels.npz?raw=True\u2019\n\nk49-test-labels.npz 100%[===================>]  26.81K  --.-KB/s    in 0.1s    \n\n2021-06-22 00:00:20 (203 KB/s) - \u2018k49-test-labels.npz?raw=True\u2019 saved [27450/27450]\n\n-rw-rw-r-- 1 spark spark 27K Feb  4  2019 k49-test-labels.npz\n"
                }
            ],
            "source": "# download the train and test labels and images, \n# which are available onlyh in .npz format \n\n!wget http://codh.rois.ac.jp/kmnist/dataset/k49/k49-train-imgs.npz?raw=True\n!mv k49-train-imgs.npz?raw=True k49-train-imgs.npz\n!ls -lahr k49-train-imgs.npz\n\n!wget http://codh.rois.ac.jp/kmnist/dataset/k49/k49-train-labels.npz?raw=True\n!mv k49-train-labels.npz?raw=True k49-train-labels.npz\n!ls -lahr k49-train-labels.npz\n\n!wget http://codh.rois.ac.jp/kmnist/dataset/k49/k49-test-imgs.npz?raw=True\n!mv k49-test-imgs.npz?raw=True k49-test-imgs.npz\n!ls -lahr k49-test-imgs.npz\n\n!wget http://codh.rois.ac.jp/kmnist/dataset/k49/k49-test-labels.npz?raw=True\n!mv k49-test-labels.npz?raw=True k49-test-labels.npz\n!ls -lahr k49-test-labels.npz"
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {},
            "outputs": [],
            "source": "def load(f):\n    return np.load(f)['arr_0']"
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "metadata": {},
            "outputs": [],
            "source": "x_train_k49 = load('k49-train-imgs.npz')\ny_train_k49 = load('k49-train-labels.npz')\nx_test_k49 = load('k49-test-imgs.npz')\ny_test_k49 = load('k49-test-labels.npz')\n\nx_train_k49 = x_train_k49.reshape(-1, 784)\nx_test_k49 = x_test_k49.reshape(-1, 784)\n\n\n# the numpy arrays used in the keras models need to be between 0 and 1\n# we keep x_train_k49 and x_test_k49 in the format of between 0 and 255\n# for use in the K-nearest Neighbors model below\n\ntrain_images_k49 = x_train_k49 / 255\ntest_images_k49 = x_test_k49 / 255\n\n# and the labels can be the same\n\ntrain_labels_k49 = y_train_k49\ntest_labels_k49 = y_test_k49"
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [],
            "source": "# model_1_k49 def\n# we have to change the last layer due to the 49 categories\n\nmodel_1_k49 = tf.keras.Sequential([\n    tf.keras.layers.Dense(128, activation=tf.nn.relu),\n    tf.keras.layers.Dense(49, activation=tf.nn.softmax)\n])"
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [],
            "source": "# model_1_k49 compile\nmodel_1_k49.compile(optimizer='adam',\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])"
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Epoch 1/5\n7262/7262 [==============================] - 14s 2ms/step - loss: 0.9142 - accuracy: 0.7662 - val_loss: 1.0799 - val_accuracy: 0.7278\nEpoch 2/5\n7262/7262 [==============================] - 13s 2ms/step - loss: 0.5710 - accuracy: 0.8474 - val_loss: 0.9618 - val_accuracy: 0.7581\nEpoch 3/5\n7262/7262 [==============================] - 13s 2ms/step - loss: 0.4910 - accuracy: 0.8670 - val_loss: 0.9151 - val_accuracy: 0.7734\nEpoch 4/5\n7262/7262 [==============================] - 13s 2ms/step - loss: 0.4495 - accuracy: 0.8786 - val_loss: 0.9089 - val_accuracy: 0.7800\nEpoch 5/5\n7262/7262 [==============================] - 13s 2ms/step - loss: 0.4226 - accuracy: 0.8843 - val_loss: 0.9129 - val_accuracy: 0.7817\n"
                },
                {
                    "data": {
                        "text/plain": "<tensorflow.python.keras.callbacks.History at 0x7f57e44ea090>"
                    },
                    "execution_count": 11,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "# model_1_k49 fit\n# as in the first data set, this model could probably use more epochs\nmodel_1_k49.fit(train_images_k49, train_labels_k49, epochs=5, verbose=1, validation_data=(test_images_k49, test_labels_k49))"
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [],
            "source": "# model_1_2_k49 def\n\nmodel_1_2_k49 = tf.keras.Sequential([\n    tf.keras.layers.Dense(128, activation=tf.nn.relu),\n    tf.keras.layers.Dense(128, activation=tf.nn.relu),    \n    tf.keras.layers.Dense(128, activation=tf.nn.relu),\n    tf.keras.layers.Dense(128, activation=tf.nn.relu),    \n    tf.keras.layers.Dense(49, activation=tf.nn.softmax)\n])"
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [],
            "source": "# model_1_2_k49 compile\nmodel_1_2_k49.compile(optimizer='adam',\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])"
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Epoch 1/20\n7262/7262 [==============================] - 17s 2ms/step - loss: 0.8359 - accuracy: 0.7735 - val_loss: 0.9507 - val_accuracy: 0.7440\nEpoch 2/20\n7262/7262 [==============================] - 16s 2ms/step - loss: 0.5021 - accuracy: 0.8600 - val_loss: 0.8355 - val_accuracy: 0.7785\nEpoch 3/20\n7262/7262 [==============================] - 16s 2ms/step - loss: 0.4239 - accuracy: 0.8810 - val_loss: 0.7762 - val_accuracy: 0.7992\nEpoch 4/20\n7262/7262 [==============================] - 16s 2ms/step - loss: 0.3812 - accuracy: 0.8923 - val_loss: 0.7687 - val_accuracy: 0.8008\nEpoch 5/20\n7262/7262 [==============================] - 16s 2ms/step - loss: 0.3527 - accuracy: 0.8997 - val_loss: 0.7548 - val_accuracy: 0.8121\nEpoch 6/20\n7262/7262 [==============================] - 17s 2ms/step - loss: 0.3314 - accuracy: 0.9058 - val_loss: 0.7723 - val_accuracy: 0.8113\nEpoch 7/20\n7262/7262 [==============================] - 16s 2ms/step - loss: 0.3148 - accuracy: 0.9100 - val_loss: 0.7624 - val_accuracy: 0.8165\nEpoch 8/20\n7262/7262 [==============================] - 16s 2ms/step - loss: 0.3027 - accuracy: 0.9132 - val_loss: 0.7623 - val_accuracy: 0.8148\nEpoch 9/20\n7262/7262 [==============================] - 16s 2ms/step - loss: 0.2923 - accuracy: 0.9156 - val_loss: 0.7801 - val_accuracy: 0.8182\nEpoch 10/20\n7262/7262 [==============================] - 16s 2ms/step - loss: 0.2821 - accuracy: 0.9185 - val_loss: 0.7877 - val_accuracy: 0.8138\nEpoch 11/20\n7262/7262 [==============================] - 16s 2ms/step - loss: 0.2739 - accuracy: 0.9213 - val_loss: 0.8045 - val_accuracy: 0.8211\nEpoch 12/20\n7262/7262 [==============================] - 16s 2ms/step - loss: 0.2675 - accuracy: 0.9230 - val_loss: 0.8105 - val_accuracy: 0.8241\nEpoch 13/20\n7262/7262 [==============================] - 16s 2ms/step - loss: 0.2633 - accuracy: 0.9245 - val_loss: 0.8031 - val_accuracy: 0.8240\nEpoch 14/20\n7262/7262 [==============================] - 16s 2ms/step - loss: 0.2584 - accuracy: 0.9253 - val_loss: 0.8457 - val_accuracy: 0.8230\nEpoch 15/20\n7262/7262 [==============================] - 17s 2ms/step - loss: 0.2520 - accuracy: 0.9272 - val_loss: 0.8306 - val_accuracy: 0.8210\nEpoch 16/20\n7262/7262 [==============================] - 16s 2ms/step - loss: 0.2513 - accuracy: 0.9280 - val_loss: 0.9010 - val_accuracy: 0.8178\nEpoch 17/20\n7262/7262 [==============================] - 16s 2ms/step - loss: 0.2468 - accuracy: 0.9295 - val_loss: 0.8909 - val_accuracy: 0.8162\nEpoch 18/20\n7262/7262 [==============================] - 16s 2ms/step - loss: 0.2425 - accuracy: 0.9305 - val_loss: 0.8849 - val_accuracy: 0.8216\nEpoch 19/20\n7262/7262 [==============================] - 16s 2ms/step - loss: 0.2381 - accuracy: 0.9321 - val_loss: 0.9081 - val_accuracy: 0.8242\nEpoch 20/20\n7262/7262 [==============================] - 16s 2ms/step - loss: 0.2347 - accuracy: 0.9336 - val_loss: 0.9067 - val_accuracy: 0.8215\n"
                },
                {
                    "data": {
                        "text/plain": "<tensorflow.python.keras.callbacks.History at 0x7f65fd302650>"
                    },
                    "execution_count": 12,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "# model_1_2_k49 fit\nmodel_1_2_k49.fit(train_images_k49, train_labels_k49, epochs=20, verbose=1, validation_data=(test_images_k49, test_labels_k49))"
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense (Dense)                (None, 128)               100480    \n_________________________________________________________________\ndense_1 (Dense)              (None, 128)               16512     \n_________________________________________________________________\ndense_2 (Dense)              (None, 128)               16512     \n_________________________________________________________________\ndense_3 (Dense)              (None, 128)               16512     \n_________________________________________________________________\ndense_4 (Dense)              (None, 49)                6321      \n=================================================================\nTotal params: 156,337\nTrainable params: 156,337\nNon-trainable params: 0\n_________________________________________________________________\n"
                }
            ],
            "source": "model_1_2_k49.summary()"
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [],
            "source": "# model_1_5_k49 def\n\nmodel_1_5_k49 = tf.keras.Sequential([\n    tf.keras.layers.Dense(128, activation=tf.nn.relu),\n    tf.keras.layers.Dense(128, activation=tf.nn.relu),    \n    tf.keras.layers.Dense(128, activation=tf.nn.relu),\n    tf.keras.layers.Dense(128, activation=tf.nn.relu),    \n    tf.keras.layers.Dense(49, activation=tf.nn.softmax)\n])"
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [],
            "source": "# model_1_5_k49 compile\n\nmodel_1_5_k49.compile(optimizer='sgd',\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])"
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Epoch 1/20\n7262/7262 [==============================] - 15s 2ms/step - loss: 1.7135 - accuracy: 0.5690 - val_loss: 1.5775 - val_accuracy: 0.6086\nEpoch 2/20\n7262/7262 [==============================] - 14s 2ms/step - loss: 0.8724 - accuracy: 0.7705 - val_loss: 1.2330 - val_accuracy: 0.6849\nEpoch 3/20\n7262/7262 [==============================] - 14s 2ms/step - loss: 0.6760 - accuracy: 0.8188 - val_loss: 1.0666 - val_accuracy: 0.7251\nEpoch 4/20\n7262/7262 [==============================] - 14s 2ms/step - loss: 0.5714 - accuracy: 0.8449 - val_loss: 0.9663 - val_accuracy: 0.7480\nEpoch 5/20\n7262/7262 [==============================] - 14s 2ms/step - loss: 0.5047 - accuracy: 0.8612 - val_loss: 0.8925 - val_accuracy: 0.7698\nEpoch 6/20\n7262/7262 [==============================] - 14s 2ms/step - loss: 0.4553 - accuracy: 0.8747 - val_loss: 0.8496 - val_accuracy: 0.7793\nEpoch 7/20\n7262/7262 [==============================] - 14s 2ms/step - loss: 0.4175 - accuracy: 0.8841 - val_loss: 0.8400 - val_accuracy: 0.7818\nEpoch 8/20\n7262/7262 [==============================] - 14s 2ms/step - loss: 0.3871 - accuracy: 0.8919 - val_loss: 0.8328 - val_accuracy: 0.7877\nEpoch 9/20\n7262/7262 [==============================] - 14s 2ms/step - loss: 0.3613 - accuracy: 0.8989 - val_loss: 0.7757 - val_accuracy: 0.7993\nEpoch 10/20\n7262/7262 [==============================] - 14s 2ms/step - loss: 0.3407 - accuracy: 0.9042 - val_loss: 0.7588 - val_accuracy: 0.8054\nEpoch 11/20\n7262/7262 [==============================] - 14s 2ms/step - loss: 0.3219 - accuracy: 0.9098 - val_loss: 0.7452 - val_accuracy: 0.8108\nEpoch 12/20\n7262/7262 [==============================] - 14s 2ms/step - loss: 0.3041 - accuracy: 0.9138 - val_loss: 0.7314 - val_accuracy: 0.8141\nEpoch 13/20\n7262/7262 [==============================] - 14s 2ms/step - loss: 0.2898 - accuracy: 0.9179 - val_loss: 0.7356 - val_accuracy: 0.8138\nEpoch 14/20\n7262/7262 [==============================] - 14s 2ms/step - loss: 0.2767 - accuracy: 0.9211 - val_loss: 0.7151 - val_accuracy: 0.8225\nEpoch 15/20\n7262/7262 [==============================] - 14s 2ms/step - loss: 0.2646 - accuracy: 0.9247 - val_loss: 0.7086 - val_accuracy: 0.8245\nEpoch 16/20\n7262/7262 [==============================] - 14s 2ms/step - loss: 0.2540 - accuracy: 0.9278 - val_loss: 0.7011 - val_accuracy: 0.8257\nEpoch 17/20\n7262/7262 [==============================] - 14s 2ms/step - loss: 0.2440 - accuracy: 0.9301 - val_loss: 0.7067 - val_accuracy: 0.8260\nEpoch 18/20\n7262/7262 [==============================] - 14s 2ms/step - loss: 0.2343 - accuracy: 0.9324 - val_loss: 0.7352 - val_accuracy: 0.8214\nEpoch 19/20\n7262/7262 [==============================] - 14s 2ms/step - loss: 0.2254 - accuracy: 0.9346 - val_loss: 0.6968 - val_accuracy: 0.8311\nEpoch 20/20\n7262/7262 [==============================] - 14s 2ms/step - loss: 0.2177 - accuracy: 0.9369 - val_loss: 0.7056 - val_accuracy: 0.8291\n"
                },
                {
                    "data": {
                        "text/plain": "<tensorflow.python.keras.callbacks.History at 0x7ff0000e0dd0>"
                    },
                    "execution_count": 11,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "# model_1_5_k49 fit\n\nmodel_1_5_k49.fit(train_images_k49, train_labels_k49, epochs=20, verbose=1, validation_data=(test_images_k49, test_labels_k49))"
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense (Dense)                (None, 128)               100480    \n_________________________________________________________________\ndense_1 (Dense)              (None, 128)               16512     \n_________________________________________________________________\ndense_2 (Dense)              (None, 128)               16512     \n_________________________________________________________________\ndense_3 (Dense)              (None, 128)               16512     \n_________________________________________________________________\ndense_4 (Dense)              (None, 49)                6321      \n=================================================================\nTotal params: 156,337\nTrainable params: 156,337\nNon-trainable params: 0\n_________________________________________________________________\n"
                }
            ],
            "source": "# model_1_5_k49 fit\n\nmodel_1_5_k49.summary()"
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "metadata": {},
            "outputs": [],
            "source": "# model_2_k49 def\n\nbatch_size = 128\nnum_classes = 49       #  here we specify the number of classes as 49 instead of 10\nepochs = 12\n\n# input image dimensions\nimg_rows, img_cols = 28, 28\n\nx_train = train_images_k49\nx_test = test_images_k49\ny_train = train_labels_k49\ny_test = test_labels_k49\n\nif K.image_data_format() == 'channels_first':\n    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n    input_shape = (1, img_rows, img_cols)\nelse:\n    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n    input_shape = (img_rows, img_cols, 1)\n\nmodel_2_k49 = tf.keras.Sequential()\nmodel_2_k49.add(Conv2D(32, kernel_size=(3, 3),\n                 activation='relu',\n                 input_shape=input_shape))\nmodel_2_k49.add(Conv2D(64, (3, 3), activation='relu'))\nmodel_2_k49.add(MaxPooling2D(pool_size=(2, 2)))\nmodel_2_k49.add(Dropout(0.25))\nmodel_2_k49.add(tf.keras.layers.Flatten())\nmodel_2_k49.add(tf.keras.layers.Dense(128, activation='relu'))\nmodel_2_k49.add(Dropout(0.5))\nmodel_2_k49.add(tf.keras.layers.Dense(num_classes, activation='softmax'))\n\n# model_2 compile\n\nmodel_2_k49.compile(optimizer='adam', \n          loss='sparse_categorical_crossentropy',\n          metrics=['accuracy'])"
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Epoch 1/12\n1816/1816 [==============================] - 342s 188ms/step - loss: 0.9383 - accuracy: 0.7530 - val_loss: 0.6622 - val_accuracy: 0.8259\nEpoch 2/12\n1816/1816 [==============================] - 340s 187ms/step - loss: 0.5146 - accuracy: 0.8584 - val_loss: 0.5186 - val_accuracy: 0.8644\nEpoch 3/12\n1816/1816 [==============================] - 338s 186ms/step - loss: 0.4178 - accuracy: 0.8831 - val_loss: 0.4378 - val_accuracy: 0.8841\nEpoch 4/12\n1816/1816 [==============================] - 338s 186ms/step - loss: 0.3619 - accuracy: 0.8981 - val_loss: 0.3942 - val_accuracy: 0.8978\nEpoch 5/12\n1816/1816 [==============================] - 336s 185ms/step - loss: 0.3248 - accuracy: 0.9068 - val_loss: 0.3849 - val_accuracy: 0.9000\nEpoch 6/12\n1816/1816 [==============================] - 336s 185ms/step - loss: 0.2997 - accuracy: 0.9139 - val_loss: 0.3704 - val_accuracy: 0.9067\nEpoch 7/12\n1816/1816 [==============================] - 335s 185ms/step - loss: 0.2788 - accuracy: 0.9189 - val_loss: 0.3439 - val_accuracy: 0.9128\nEpoch 8/12\n1816/1816 [==============================] - 337s 185ms/step - loss: 0.2621 - accuracy: 0.9229 - val_loss: 0.3515 - val_accuracy: 0.9116\nEpoch 9/12\n1816/1816 [==============================] - 336s 185ms/step - loss: 0.2471 - accuracy: 0.9263 - val_loss: 0.3394 - val_accuracy: 0.9164\nEpoch 10/12\n1816/1816 [==============================] - 336s 185ms/step - loss: 0.2368 - accuracy: 0.9292 - val_loss: 0.3411 - val_accuracy: 0.9184\nEpoch 11/12\n1816/1816 [==============================] - 337s 186ms/step - loss: 0.2258 - accuracy: 0.9320 - val_loss: 0.3343 - val_accuracy: 0.9181\nEpoch 12/12\n1816/1816 [==============================] - 337s 186ms/step - loss: 0.2165 - accuracy: 0.9347 - val_loss: 0.3265 - val_accuracy: 0.9195\n"
                },
                {
                    "data": {
                        "text/plain": "<tensorflow.python.keras.callbacks.History at 0x7f57cc4be750>"
                    },
                    "execution_count": 13,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "# fit model2_k49\n\nmodel_2_k49.fit(x_train, y_train,\n         batch_size=batch_size,\n         epochs=epochs,\n         verbose=1,\n         validation_data=(x_test, y_test))"
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "metadata": {},
            "outputs": [],
            "source": "# model_2_1_k49 def\n\nbatch_size = 128\nnum_classes = 49       #  here we specify the number of classes as 49 instead of 10\nepochs = 12\n\n# input image dimensions\nimg_rows, img_cols = 28, 28\n\nx_train = train_images_k49\nx_test = test_images_k49\ny_train = train_labels_k49\ny_test = test_labels_k49\n\nif K.image_data_format() == 'channels_first':\n    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n    input_shape = (1, img_rows, img_cols)\nelse:\n    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n    input_shape = (img_rows, img_cols, 1)\n\nmodel_2_1_k49 = tf.keras.Sequential()\nmodel_2_1_k49.add(Conv2D(32, kernel_size=(3, 3),\n                 activation='relu',\n                 input_shape=input_shape))\nmodel_2_1_k49.add(Conv2D(64, (3, 3), activation='relu'))\nmodel_2_1_k49.add(MaxPooling2D(pool_size=(2, 2)))\nmodel_2_1_k49.add(Dropout(0.25))\nmodel_2_1_k49.add(tf.keras.layers.Flatten())\nmodel_2_1_k49.add(tf.keras.layers.Dense(128, activation='relu'))\nmodel_2_1_k49.add(Dropout(0.5))\nmodel_2_1_k49.add(tf.keras.layers.Dense(num_classes, activation='softmax'))"
        },
        {
            "cell_type": "code",
            "execution_count": 22,
            "metadata": {},
            "outputs": [],
            "source": "# change of optimizer and compile model_2_k49\n\nmodel_2_1_k49.compile(loss='sparse_categorical_crossentropy',\n              optimizer=tf.keras.optimizers.Adadelta(learning_rate=1.0),\n              metrics=['accuracy'])"
        },
        {
            "cell_type": "code",
            "execution_count": 23,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Epoch 1/12\n1816/1816 [==============================] - 802s 441ms/step - loss: 0.9429 - accuracy: 0.7522 - val_loss: 0.6710 - val_accuracy: 0.8271\nEpoch 2/12\n1816/1816 [==============================] - 797s 439ms/step - loss: 0.5117 - accuracy: 0.8620 - val_loss: 0.5405 - val_accuracy: 0.8605\nEpoch 3/12\n1816/1816 [==============================] - 798s 439ms/step - loss: 0.4224 - accuracy: 0.8848 - val_loss: 0.4838 - val_accuracy: 0.8779\nEpoch 4/12\n1816/1816 [==============================] - 794s 437ms/step - loss: 0.3805 - accuracy: 0.8967 - val_loss: 0.4518 - val_accuracy: 0.8836\nEpoch 5/12\n1816/1816 [==============================] - 796s 438ms/step - loss: 0.3546 - accuracy: 0.9041 - val_loss: 0.4339 - val_accuracy: 0.8913\nEpoch 6/12\n1816/1816 [==============================] - 796s 438ms/step - loss: 0.3443 - accuracy: 0.9083 - val_loss: 0.4129 - val_accuracy: 0.8940\nEpoch 7/12\n1816/1816 [==============================] - 798s 439ms/step - loss: 0.3346 - accuracy: 0.9110 - val_loss: 0.4546 - val_accuracy: 0.8959\nEpoch 8/12\n1816/1816 [==============================] - 796s 439ms/step - loss: 0.3263 - accuracy: 0.9134 - val_loss: 0.4427 - val_accuracy: 0.8971\nEpoch 9/12\n1816/1816 [==============================] - 794s 437ms/step - loss: 0.3234 - accuracy: 0.9152 - val_loss: 0.3828 - val_accuracy: 0.9045\nEpoch 10/12\n1816/1816 [==============================] - 800s 440ms/step - loss: 0.3172 - accuracy: 0.9168 - val_loss: 0.3768 - val_accuracy: 0.9062\nEpoch 11/12\n1816/1816 [==============================] - 797s 439ms/step - loss: 0.3154 - accuracy: 0.9176 - val_loss: 0.3982 - val_accuracy: 0.8994\nEpoch 12/12\n1816/1816 [==============================] - 795s 438ms/step - loss: 0.3099 - accuracy: 0.9188 - val_loss: 0.3830 - val_accuracy: 0.9075\n"
                },
                {
                    "data": {
                        "text/plain": "<tensorflow.python.keras.callbacks.History at 0x7f0994b9f290>"
                    },
                    "execution_count": 23,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "# fit model_2_k49 after change of optmizer\n\nmodel_2_1_k49.fit(x_train, y_train,\n           batch_size=batch_size,\n           epochs=epochs,\n           verbose=1,\n           validation_data=(x_test, y_test))"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### Model:  K Nearest Neighbors - k49"
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [],
            "source": "# recall that the two numpy image arrays used below, namely x_train_k49 and x_test_k49\n# are arrays of integers, which works fine for KNN"
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Fit KNeighborsClassifier(n_jobs=-1, n_neighbors=4, weights='distance')\n"
                },
                {
                    "data": {
                        "text/plain": "KNeighborsClassifier(n_jobs=-1, n_neighbors=4, weights='distance')"
                    },
                    "execution_count": 17,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "model_3_k49 = KNeighborsClassifier(n_neighbors=4, weights='distance', n_jobs=-1)\nprint('Fit', model_3_k49)\nmodel_3_k49.fit(x_train_k49, y_train_k49)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Evaluate KNeighborsClassifier(n_jobs=-1, n_neighbors=4, weights='distance')\n"
                }
            ],
            "source": "print('Evaluate', model_3_k49)\nscore = model_3_k49.score(x_test_k49, y_test_k49)\nprint('The accuracy is:', score)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### The Third Dataset\n\n#### Includes Kanji characters, this data has 3832 different classes and consists of 140,426 images, each image is 64 X 64 pixels\n#### This dataset is not as processed as the other two. It is just a bunch of png images in a directory inside an archive file."
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "--2021-06-24 02:32:15--  http://codh.rois.ac.jp/kmnist/dataset/kkanji/kkanji.tar?raw=True\nResolving codh.rois.ac.jp (codh.rois.ac.jp)... 136.187.88.58\nConnecting to codh.rois.ac.jp (codh.rois.ac.jp)|136.187.88.58|:80... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 324290560 (309M) [application/x-tar]\nSaving to: \u2018kkanji.tar?raw=True\u2019\n\nkkanji.tar?raw=True 100%[===================>] 309.27M  6.87MB/s    in 46s     \n\n2021-06-24 02:33:03 (6.79 MB/s) - \u2018kkanji.tar?raw=True\u2019 saved [324290560/324290560]\n\n-rw-rw-r-- 1 spark spark 310M Dec  8  2018 kkanji.tar\nkkanji2/\nkkanji2/U+5B87/\nkkanji2/U+5B87/72d56fcb33d10fe0.png\nkkanji2/U+5B87/75f7923797777c74.png\nkkanji2/U+5B87/69d6becd4f8f2d61.png\nkkanji2/U+5B87/522dd01c5f9573f5.png\nkkanji2/U+5B87/36aadd8d92c64049.png\nkkanji2/U+5B87/58f75629e53b9e63.png\nkkanji2/U+5B87/557950fbb39b019b.png\nkkanji2/U+5B87/c4ca643dbc0299b6.png\nkkanji2/U+5B87/02f161e7e7a3c364.png\nkkanji2/U+5B87/c45553bb4a35c8d4.png\nkkanji2/U+5B87/ffa955bd6cb43af8.png\nkkanji2/U+5B87/d0387f14448f2a95.png\nkkanji2/U+5B87/d8c4a0dbd99fc02f.png\nkkanji2/U+5B87/419beef7f2c593da.png\nkkanji2/U+5B87/f2308287339f973d.png\nkkanji2/U+5B87/f885e3a957b99e8e.png\nkkanji2/U+5B87/f5e7a417e3d831d8.png\nkkanji2/U+5B87/17681a9b66db23e0.png\nkkanji2/U+5B87/cc2b8cd01c984262.png\nkkanji2/U+5B87/3e76b512ef42c8f2.png\nkkanji2/U+5B87/49276561d0ca08e0.png\nkkanji2/U+5B87/a169cf27462e020d.png\nkkanji2/U+5B87/61b85600b3fdbea9.png\nkkanji2/U+5B87/112b17b885fbfa61.png\nkkanji2/U+5B87/99654e0e2e597032.png\nkkanji2/U+5B87/7af7faf202121801.png\nkkanji2/U+5B87/8a6b7ea64d025e6d.png\nkkanji2/U+5B87/3c965132ab259d2b.png\nkkanji2/U+5B87/e5e120e4a9d70b6b.png\nkkanji2/U+5B87/a83b76ab3c9ab31c.png\nkkanji2/U+5B87/5a3cf89898fb6100.png\nkkanji2/U+5B87/d84ddc102e8425b6.png\nkkanji2/U+5B87/bdf7cd68b0df8857.png\nkkanji2/U+5B87/a0a4815c3bfe9f15.png\nkkanji2/U+5B87/b2e74a3d92ef78ac.png\nkkanji2/U+5B87/25e9d668d9b64914.png\nkkanji2/U+5B87/5b6ee9b9d03fc3ad.png\nkkanji2/U+583A/\nkkanji2/U+583A/2adaddcb1db613b1.png\nkkanji2/U+583A/3bd7379e66373395.png\nkkanji2/U+583A/ea8dd8f0d7375e9c.png\nkkanji2/U+583A/f738049a58a97177.png\nkkanji2/U+583A/b3a041d6510720b2.png\nkkanji2/U+583A/3921ca3a71dcaaff.png\nkkanji2/U+583A/21d02bce1618fd05.png\nkkanji2/U+583A/df87ce26b03e824d.png\nkkanji2/U+583A/5e5b7454b683e6b0.png\nkkanji2/U+583A/10063d313e1474aa.png\nkkanji2/U+583A/84e528c1a87102b4.png\nkkanji2/U+583A/f8db9187baf4b352.png\nkkanji2/U+583A/e1eb722d60ac207c.png\nkkanji2/U+583A/ab464d2670b260bc.png\nkkanji2/U+583A/596e410f8ae20a07.png\nkkanji2/U+583A/805fa5f10779d104.png\nkkanji2/U+583A/66e27da7a955f8ea.png\nkkanji2/U+583A/d43d5197995e7345.png\nkkanji2/U+583A/0ceea4b4033e6c66.png\nkkanji2/U+5ECA/\nkkanji2/U+5ECA/47d36e0f7df9b8ab.png\nkkanji2/U+5ECA/ea6adb0abe33425c.png\nkkanji2/U+5ECA/542e3b1950722456.png\nkkanji2/U+5ECA/3b16e7bcc8a3a717.png\nkkanji2/U+5ECA/7f899bc5008c5824.png\nkkanji2/U+9CF6/\nkkanji2/U+9CF6/e52af171049817f9.png\nkkanji2/U+9CF6/3a09e6488fd81fcd.png\nkkanji2/U+9CF6/ae5f75bd30a73375.png\nkkanji2/U+5A9B/\ntar: write error\n"
                }
            ],
            "source": "# we download the archive:\n\n!wget http://codh.rois.ac.jp/kmnist/dataset/kkanji/kkanji.tar?raw=True\n!mv kkanji.tar?raw=True kkanji.tar\n!ls -lahr kkanji.tar\n\n# list the contents of the archive\n# limit output to the first 70 files\n\n!tar -tf kkanji.tar | head -70"
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": "# extract the archive:\n!tar -xf kkanji.tar"
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [],
            "source": "# put the codepoints in a file\n# let the first line of the file be name\n# of the column in the dataframe we are creating\n!echo codepoint > codepoints.csv\n!ls kkanji2 >> codepoints.csv"
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "/home/spark/shared/codepoints.csv\n"
                }
            ],
            "source": "# verify the file's path:\nprint (os.path.abspath(\"codepoints.csv\"))"
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>codepoint</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>U+241C6</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>U+24FA3</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>U+25DA1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>U+27752</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>U+29780</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>3827</th>\n      <td>U+FA38</td>\n    </tr>\n    <tr>\n      <th>3828</th>\n      <td>U+FA45</td>\n    </tr>\n    <tr>\n      <th>3829</th>\n      <td>U+FA4A</td>\n    </tr>\n    <tr>\n      <th>3830</th>\n      <td>U+FA55</td>\n    </tr>\n    <tr>\n      <th>3831</th>\n      <td>U+FA5C</td>\n    </tr>\n  </tbody>\n</table>\n<p>3832 rows \u00d7 1 columns</p>\n</div>",
                        "text/plain": "     codepoint\n0      U+241C6\n1      U+24FA3\n2      U+25DA1\n3      U+27752\n4      U+29780\n...        ...\n3827    U+FA38\n3828    U+FA45\n3829    U+FA4A\n3830    U+FA55\n3831    U+FA5C\n\n[3832 rows x 1 columns]"
                    },
                    "execution_count": 10,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "# read all the lines of the file into pandas dataframe\n# including the column header which is already in the file\n# display new dataframe\n# here we can confirm that the data has 3832 classes\n\ndf_kanji_classmap = pd.read_csv(\"codepoints.csv\")\ndf_kanji_classmap"
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "image/png": "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\n",
                        "text/plain": "<Figure size 432x288 with 1 Axes>"
                    },
                    "metadata": {
                        "needs_background": "light"
                    },
                    "output_type": "display_data"
                }
            ],
            "source": "# we'll need to read the images in to transform them\n#read the first listed image in the first folder and display it\nimg = mping.imread('kkanji2/U+5B87/72d56fcb33d10fe0.png')\nplt.imshow(img)\nplt.show()\n# note that this image in only \"first\" as listed from the tar arcive above\n# it is not \"first\" in terms of the codepoint listing we created directly above"
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [],
            "source": "# create a pandas dataframe that contains the codepoint for each image, \n# and its full path in the os and display that dataframe\n\n#  ** reminder:  makes sure to set the path line correctly using the fact that\n# the directory kkanji2 is in the same directory as the file codepoints.csv\n\ndata = []\ndir = os.path.realpath('/home/spark/shared/kkanji2')\nfor r, d, f in os.walk(dir):\n    for file in f:\n        if \".png\" in file:\n            data.append((r.split('/')[-1],os.path.join(r,file)))\ndf_kanji2 = pd.DataFrame(data, columns=['codepoint', 'image_file_path']).sort_values(by=['codepoint'], ignore_index = True)"
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>codepoint</th>\n      <th>image_file_path</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>U+241C6</td>\n      <td>/home/spark/shared/kkanji2/U+241C6/c0d603c6ce4a4538.png</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>U+241C6</td>\n      <td>/home/spark/shared/kkanji2/U+241C6/689fa55040ec4f03.png</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>U+24FA3</td>\n      <td>/home/spark/shared/kkanji2/U+24FA3/4190e728bfc948e0.png</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>U+24FA3</td>\n      <td>/home/spark/shared/kkanji2/U+24FA3/80582798ed70ce7c.png</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>U+25DA1</td>\n      <td>/home/spark/shared/kkanji2/U+25DA1/512d7fcacddd25fd.png</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>140419</th>\n      <td>U+FA55</td>\n      <td>/home/spark/shared/kkanji2/U+FA55/f03dee341d017a83.png</td>\n    </tr>\n    <tr>\n      <th>140420</th>\n      <td>U+FA55</td>\n      <td>/home/spark/shared/kkanji2/U+FA55/9bb329908332c929.png</td>\n    </tr>\n    <tr>\n      <th>140421</th>\n      <td>U+FA55</td>\n      <td>/home/spark/shared/kkanji2/U+FA55/9158bbb254b2ec88.png</td>\n    </tr>\n    <tr>\n      <th>140422</th>\n      <td>U+FA5C</td>\n      <td>/home/spark/shared/kkanji2/U+FA5C/15e2060396eba2b3.png</td>\n    </tr>\n    <tr>\n      <th>140423</th>\n      <td>U+FA5C</td>\n      <td>/home/spark/shared/kkanji2/U+FA5C/679e4b2f026f6297.png</td>\n    </tr>\n  </tbody>\n</table>\n<p>140424 rows \u00d7 2 columns</p>\n</div>",
                        "text/plain": "       codepoint                                          image_file_path\n0        U+241C6  /home/spark/shared/kkanji2/U+241C6/c0d603c6ce4a4538.png\n1        U+241C6  /home/spark/shared/kkanji2/U+241C6/689fa55040ec4f03.png\n2        U+24FA3  /home/spark/shared/kkanji2/U+24FA3/4190e728bfc948e0.png\n3        U+24FA3  /home/spark/shared/kkanji2/U+24FA3/80582798ed70ce7c.png\n4        U+25DA1  /home/spark/shared/kkanji2/U+25DA1/512d7fcacddd25fd.png\n...          ...                                                      ...\n140419    U+FA55   /home/spark/shared/kkanji2/U+FA55/f03dee341d017a83.png\n140420    U+FA55   /home/spark/shared/kkanji2/U+FA55/9bb329908332c929.png\n140421    U+FA55   /home/spark/shared/kkanji2/U+FA55/9158bbb254b2ec88.png\n140422    U+FA5C   /home/spark/shared/kkanji2/U+FA5C/15e2060396eba2b3.png\n140423    U+FA5C   /home/spark/shared/kkanji2/U+FA5C/679e4b2f026f6297.png\n\n[140424 rows x 2 columns]"
                    },
                    "execution_count": 14,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": ""
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "'/home/spark/shared/kkanji2/U+241C6/c0d603c6ce4a4538.png'"
                    },
                    "execution_count": 15,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": ""
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "PNG\n(64, 64)\nL\n"
                }
            ],
            "source": ""
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "float32\n"
                },
                {
                    "data": {
                        "text/plain": "<matplotlib.image.AxesImage at 0x7f64ab064fd0>"
                    },
                    "execution_count": 17,
                    "metadata": {},
                    "output_type": "execute_result"
                },
                {
                    "data": {
                        "image/png": "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\n",
                        "text/plain": "<Figure size 432x288 with 1 Axes>"
                    },
                    "metadata": {
                        "needs_background": "light"
                    },
                    "output_type": "display_data"
                }
            ],
            "source": ""
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "<class 'numpy.ndarray'>\n(64, 64)\n"
                }
            ],
            "source": ""
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "array([[0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n        0.        ],\n       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n        0.        ],\n       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n        0.        ],\n       ...,\n       [0.        , 0.        , 0.        , ..., 0.6156863 , 0.07058824,\n        0.        ],\n       [0.        , 0.        , 0.        , ..., 0.0627451 , 0.        ,\n        0.        ],\n       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n        0.        ]], dtype=float32)"
                    },
                    "execution_count": 19,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": ""
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "(64, 64)"
                    },
                    "execution_count": 20,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": ""
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>codepoint</th>\n      <th>image_file_path</th>\n      <th>np_array</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>U+241C6</td>\n      <td>/home/spark/shared/kkanji2/U+241C6/c0d603c6ce4...</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>U+241C6</td>\n      <td>/home/spark/shared/kkanji2/U+241C6/689fa55040e...</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>U+24FA3</td>\n      <td>/home/spark/shared/kkanji2/U+24FA3/4190e728bfc...</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>U+24FA3</td>\n      <td>/home/spark/shared/kkanji2/U+24FA3/80582798ed7...</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>U+25DA1</td>\n      <td>/home/spark/shared/kkanji2/U+25DA1/512d7fcacdd...</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>140419</th>\n      <td>U+FA55</td>\n      <td>/home/spark/shared/kkanji2/U+FA55/f03dee341d01...</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>140420</th>\n      <td>U+FA55</td>\n      <td>/home/spark/shared/kkanji2/U+FA55/9bb329908332...</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>140421</th>\n      <td>U+FA55</td>\n      <td>/home/spark/shared/kkanji2/U+FA55/9158bbb254b2...</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>140422</th>\n      <td>U+FA5C</td>\n      <td>/home/spark/shared/kkanji2/U+FA5C/15e2060396eb...</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>140423</th>\n      <td>U+FA5C</td>\n      <td>/home/spark/shared/kkanji2/U+FA5C/679e4b2f026f...</td>\n      <td></td>\n    </tr>\n  </tbody>\n</table>\n<p>140424 rows \u00d7 3 columns</p>\n</div>",
                        "text/plain": "       codepoint                                    image_file_path np_array\n0        U+241C6  /home/spark/shared/kkanji2/U+241C6/c0d603c6ce4...         \n1        U+241C6  /home/spark/shared/kkanji2/U+241C6/689fa55040e...         \n2        U+24FA3  /home/spark/shared/kkanji2/U+24FA3/4190e728bfc...         \n3        U+24FA3  /home/spark/shared/kkanji2/U+24FA3/80582798ed7...         \n4        U+25DA1  /home/spark/shared/kkanji2/U+25DA1/512d7fcacdd...         \n...          ...                                                ...      ...\n140419    U+FA55  /home/spark/shared/kkanji2/U+FA55/f03dee341d01...         \n140420    U+FA55  /home/spark/shared/kkanji2/U+FA55/9bb329908332...         \n140421    U+FA55  /home/spark/shared/kkanji2/U+FA55/9158bbb254b2...         \n140422    U+FA5C  /home/spark/shared/kkanji2/U+FA5C/15e2060396eb...         \n140423    U+FA5C  /home/spark/shared/kkanji2/U+FA5C/679e4b2f026f...         \n\n[140424 rows x 3 columns]"
                    },
                    "execution_count": 13,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "# add a blank column to the dataframe with column name 'np_array'\n\ndf_kanji2['np_array'] = \"\"\ndf_kanji2"
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [],
            "source": "# convert the pandas dataframe into a pyspark dataframe\ndf_kanji2_pyspk = spark.createDataFrame(df_kanji2)"
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "+---------+--------------------+--------+----------+\n|codepoint|     image_file_path|np_array|classIndex|\n+---------+--------------------+--------+----------+\n|  U+241C6|/home/spark/share...|        |    2606.0|\n|  U+241C6|/home/spark/share...|        |    2606.0|\n|  U+24FA3|/home/spark/share...|        |    2607.0|\n|  U+24FA3|/home/spark/share...|        |    2607.0|\n|  U+25DA1|/home/spark/share...|        |    3017.0|\n|  U+27752|/home/spark/share...|        |    1966.0|\n|  U+27752|/home/spark/share...|        |    1966.0|\n|  U+27752|/home/spark/share...|        |    1966.0|\n|  U+27752|/home/spark/share...|        |    1966.0|\n|  U+27752|/home/spark/share...|        |    1966.0|\n|  U+29780|/home/spark/share...|        |    3018.0|\n|  U+29DDA|/home/spark/share...|        |    3019.0|\n|  U+29E75|/home/spark/share...|        |    3020.0|\n|   U+4093|/home/spark/share...|        |    3021.0|\n|   U+4453|/home/spark/share...|        |     680.0|\n|   U+4453|/home/spark/share...|        |     680.0|\n|   U+4453|/home/spark/share...|        |     680.0|\n|   U+4453|/home/spark/share...|        |     680.0|\n|   U+4453|/home/spark/share...|        |     680.0|\n|   U+4453|/home/spark/share...|        |     680.0|\n+---------+--------------------+--------+----------+\nonly showing top 20 rows\n\n"
                }
            ],
            "source": "# our data has 3,831 different classes each with unique string names\n# which is based on their character codepoints\n# but we want simple numeric class index\n# so we instantiate a StringIndexer in spark:\n\nindexer = StringIndexer(inputCol=\"codepoint\",outputCol=\"classIndex\")\nindexed_df = indexer.fit(df_kanji2_pyspk).transform(df_kanji2_pyspk)\nindexed_df.show()"
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>codepoint</th>\n      <th>image_file_path</th>\n      <th>np_array</th>\n      <th>classIndex</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>U+241C6</td>\n      <td>/home/spark/shared/kkanji2/U+241C6/c0d603c6ce4...</td>\n      <td></td>\n      <td>2606.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>U+241C6</td>\n      <td>/home/spark/shared/kkanji2/U+241C6/689fa55040e...</td>\n      <td></td>\n      <td>2606.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>U+24FA3</td>\n      <td>/home/spark/shared/kkanji2/U+24FA3/4190e728bfc...</td>\n      <td></td>\n      <td>2607.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>U+24FA3</td>\n      <td>/home/spark/shared/kkanji2/U+24FA3/80582798ed7...</td>\n      <td></td>\n      <td>2607.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>U+25DA1</td>\n      <td>/home/spark/shared/kkanji2/U+25DA1/512d7fcacdd...</td>\n      <td></td>\n      <td>3017.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>140419</th>\n      <td>U+FA55</td>\n      <td>/home/spark/shared/kkanji2/U+FA55/f03dee341d01...</td>\n      <td></td>\n      <td>1041.0</td>\n    </tr>\n    <tr>\n      <th>140420</th>\n      <td>U+FA55</td>\n      <td>/home/spark/shared/kkanji2/U+FA55/9bb329908332...</td>\n      <td></td>\n      <td>1041.0</td>\n    </tr>\n    <tr>\n      <th>140421</th>\n      <td>U+FA55</td>\n      <td>/home/spark/shared/kkanji2/U+FA55/9158bbb254b2...</td>\n      <td></td>\n      <td>1041.0</td>\n    </tr>\n    <tr>\n      <th>140422</th>\n      <td>U+FA5C</td>\n      <td>/home/spark/shared/kkanji2/U+FA5C/15e2060396eb...</td>\n      <td></td>\n      <td>3016.0</td>\n    </tr>\n    <tr>\n      <th>140423</th>\n      <td>U+FA5C</td>\n      <td>/home/spark/shared/kkanji2/U+FA5C/679e4b2f026f...</td>\n      <td></td>\n      <td>3016.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>140424 rows \u00d7 4 columns</p>\n</div>",
                        "text/plain": "       codepoint                                    image_file_path np_array  \\\n0        U+241C6  /home/spark/shared/kkanji2/U+241C6/c0d603c6ce4...            \n1        U+241C6  /home/spark/shared/kkanji2/U+241C6/689fa55040e...            \n2        U+24FA3  /home/spark/shared/kkanji2/U+24FA3/4190e728bfc...            \n3        U+24FA3  /home/spark/shared/kkanji2/U+24FA3/80582798ed7...            \n4        U+25DA1  /home/spark/shared/kkanji2/U+25DA1/512d7fcacdd...            \n...          ...                                                ...      ...   \n140419    U+FA55  /home/spark/shared/kkanji2/U+FA55/f03dee341d01...            \n140420    U+FA55  /home/spark/shared/kkanji2/U+FA55/9bb329908332...            \n140421    U+FA55  /home/spark/shared/kkanji2/U+FA55/9158bbb254b2...            \n140422    U+FA5C  /home/spark/shared/kkanji2/U+FA5C/15e2060396eb...            \n140423    U+FA5C  /home/spark/shared/kkanji2/U+FA5C/679e4b2f026f...            \n\n        classIndex  \n0           2606.0  \n1           2606.0  \n2           2607.0  \n3           2607.0  \n4           3017.0  \n...            ...  \n140419      1041.0  \n140420      1041.0  \n140421      1041.0  \n140422      3016.0  \n140423      3016.0  \n\n[140424 rows x 4 columns]"
                    },
                    "execution_count": 16,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "# transform back to pandas dataframe:\ndf_kanji2 = indexed_df.toPandas()\ndf_kanji2"
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {},
            "outputs": [],
            "source": "# add a column containing and numpy array of the image indicated in the path in image_file_path\n# and at the same time, flatten each image from a 64 x 64 numpy array to a single\n# dimension 4096 element long numpy array, and normalize it\ndf_kanji2['np_array'] = df_kanji2['image_file_path'].apply(lambda x: np.asarray(Image.open(x))).apply(lambda y: np.reshape(y,(4096,)))"
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>codepoint</th>\n      <th>image_file_path</th>\n      <th>np_array</th>\n      <th>classIndex</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>U+241C6</td>\n      <td>/home/spark/shared/kkanji2/U+241C6/c0d603c6ce4...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>2606.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>U+241C6</td>\n      <td>/home/spark/shared/kkanji2/U+241C6/689fa55040e...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>2606.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>U+24FA3</td>\n      <td>/home/spark/shared/kkanji2/U+24FA3/4190e728bfc...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>2607.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>U+24FA3</td>\n      <td>/home/spark/shared/kkanji2/U+24FA3/80582798ed7...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>2607.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>U+25DA1</td>\n      <td>/home/spark/shared/kkanji2/U+25DA1/512d7fcacdd...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>3017.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>140419</th>\n      <td>U+FA55</td>\n      <td>/home/spark/shared/kkanji2/U+FA55/f03dee341d01...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>1041.0</td>\n    </tr>\n    <tr>\n      <th>140420</th>\n      <td>U+FA55</td>\n      <td>/home/spark/shared/kkanji2/U+FA55/9bb329908332...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 20, 2,...</td>\n      <td>1041.0</td>\n    </tr>\n    <tr>\n      <th>140421</th>\n      <td>U+FA55</td>\n      <td>/home/spark/shared/kkanji2/U+FA55/9158bbb254b2...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>1041.0</td>\n    </tr>\n    <tr>\n      <th>140422</th>\n      <td>U+FA5C</td>\n      <td>/home/spark/shared/kkanji2/U+FA5C/15e2060396eb...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>3016.0</td>\n    </tr>\n    <tr>\n      <th>140423</th>\n      <td>U+FA5C</td>\n      <td>/home/spark/shared/kkanji2/U+FA5C/679e4b2f026f...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>3016.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>140424 rows \u00d7 4 columns</p>\n</div>",
                        "text/plain": "       codepoint                                    image_file_path  \\\n0        U+241C6  /home/spark/shared/kkanji2/U+241C6/c0d603c6ce4...   \n1        U+241C6  /home/spark/shared/kkanji2/U+241C6/689fa55040e...   \n2        U+24FA3  /home/spark/shared/kkanji2/U+24FA3/4190e728bfc...   \n3        U+24FA3  /home/spark/shared/kkanji2/U+24FA3/80582798ed7...   \n4        U+25DA1  /home/spark/shared/kkanji2/U+25DA1/512d7fcacdd...   \n...          ...                                                ...   \n140419    U+FA55  /home/spark/shared/kkanji2/U+FA55/f03dee341d01...   \n140420    U+FA55  /home/spark/shared/kkanji2/U+FA55/9bb329908332...   \n140421    U+FA55  /home/spark/shared/kkanji2/U+FA55/9158bbb254b2...   \n140422    U+FA5C  /home/spark/shared/kkanji2/U+FA5C/15e2060396eb...   \n140423    U+FA5C  /home/spark/shared/kkanji2/U+FA5C/679e4b2f026f...   \n\n                                                 np_array  classIndex  \n0       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...      2606.0  \n1       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...      2606.0  \n2       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...      2607.0  \n3       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...      2607.0  \n4       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...      3017.0  \n...                                                   ...         ...  \n140419  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...      1041.0  \n140420  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 20, 2,...      1041.0  \n140421  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...      1041.0  \n140422  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...      3016.0  \n140423  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...      3016.0  \n\n[140424 rows x 4 columns]"
                    },
                    "execution_count": 18,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "df_kanji2"
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "metadata": {},
            "outputs": [],
            "source": "images_kanji = df_kanji2[\"np_array\"].to_numpy()\nlabels_kanji = df_kanji2[\"classIndex\"].to_numpy()"
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "metadata": {},
            "outputs": [],
            "source": "images_kanji = np.stack(images_kanji)"
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "(140424, 4096)"
                    },
                    "execution_count": 21,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "images_kanji.shape"
        },
        {
            "cell_type": "code",
            "execution_count": 22,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "(140424,)"
                    },
                    "execution_count": 22,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "labels_kanji.shape"
        },
        {
            "cell_type": "code",
            "execution_count": 23,
            "metadata": {},
            "outputs": [],
            "source": "x_train_kanji, x_test_kanji, y_train_kanji, y_test_kanji = train_test_split(images_kanji, labels_kanji, test_size = 0.2, random_state=42)"
        },
        {
            "cell_type": "code",
            "execution_count": 24,
            "metadata": {},
            "outputs": [],
            "source": "# we need to normalize the numpy arrays so that each number in the numpy\n# array is between 0 and 1:\n\nx_train_kanji =  x_train_kanji / 255\nx_test_kanji = x_test_kanji / 255"
        },
        {
            "cell_type": "code",
            "execution_count": 25,
            "metadata": {},
            "outputs": [],
            "source": "# model_1_2_k def\n\nmodel_1_2_k = tf.keras.Sequential([\n    tf.keras.layers.Dense(1024, activation=tf.nn.relu),\n    tf.keras.layers.Dense(1024, activation=tf.nn.relu),    \n    tf.keras.layers.Dense(1024, activation=tf.nn.relu),\n    tf.keras.layers.Dense(1024, activation=tf.nn.relu),    \n    tf.keras.layers.Dense(3832, activation=tf.nn.softmax)\n])"
        },
        {
            "cell_type": "code",
            "execution_count": 26,
            "metadata": {},
            "outputs": [],
            "source": "# model_1_2_k compile\nmodel_1_2_k.compile(optimizer='adam',\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# model_1_2_k fit\nmodel_1_2_k.fit(x_train_kanji, y_train_kanji, epochs=12, verbose=1, validation_data=(x_test_kanji, y_test_kanji))"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": ""
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.7 with Spark",
            "language": "python3",
            "name": "python37"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.7.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}