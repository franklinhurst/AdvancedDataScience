{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Waiting for a Spark session to start...\nSpark Initialization Done! ApplicationId = app-20210429221125-0004\nKERNEL_ID = 35f4cb92-a731-405a-ab83-e153512d77b1\n"
                }
            ],
            "source": "#average"
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": "rdd = sc.parallelize(range(100))"
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "4950\n100\n49.5\n"
                }
            ],
            "source": "sum = rdd.sum()\nn = rdd.count()\nprint(sum)\nprint(n)\nmean = sum/n\nprint(mean)"
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": "# if the code above is written with an implicit cast to int\n# the precision will be lost, and the output will be '49'\n# instead of 49.5"
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": "# the first statistical moment is the mean\n# the mean and median give you an idea of the central tendency of the data set"
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": "# the very same program can run on a very small data set or\n# multiple terabytes of data without changing the code"
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "[0,\n 1,\n 2,\n 3,\n 4,\n 5,\n 6,\n 7,\n 8,\n 9,\n 10,\n 11,\n 12,\n 13,\n 14,\n 15,\n 16,\n 17,\n 18,\n 19,\n 20,\n 21,\n 22,\n 23,\n 24,\n 25,\n 26,\n 27,\n 28,\n 29,\n 30,\n 31,\n 32,\n 33,\n 34,\n 35,\n 36,\n 37,\n 38,\n 39,\n 40,\n 41,\n 42,\n 43,\n 44,\n 45,\n 46,\n 47,\n 48,\n 49,\n 50,\n 51,\n 52,\n 53,\n 54,\n 55,\n 56,\n 57,\n 58,\n 59,\n 60,\n 61,\n 62,\n 63,\n 64,\n 65,\n 66,\n 67,\n 68,\n 69,\n 70,\n 71,\n 72,\n 73,\n 74,\n 75,\n 76,\n 77,\n 78,\n 79,\n 80,\n 81,\n 82,\n 83,\n 84,\n 85,\n 86,\n 87,\n 88,\n 89,\n 90,\n 91,\n 92,\n 93,\n 94,\n 95,\n 96,\n 97,\n 98,\n 99]"
                    },
                    "execution_count": 7,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "rdd.collect()"
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "[0,\n 1,\n 2,\n 3,\n 4,\n 5,\n 6,\n 7,\n 8,\n 9,\n 10,\n 11,\n 12,\n 13,\n 14,\n 15,\n 16,\n 17,\n 18,\n 19,\n 20,\n 21,\n 22,\n 23,\n 24,\n 25,\n 26,\n 27,\n 28,\n 29,\n 30,\n 31,\n 32,\n 33,\n 34,\n 35,\n 36,\n 37,\n 38,\n 39,\n 40,\n 41,\n 42,\n 43,\n 44,\n 45,\n 46,\n 47,\n 48,\n 49,\n 50,\n 51,\n 52,\n 53,\n 54,\n 55,\n 56,\n 57,\n 58,\n 59,\n 60,\n 61,\n 62,\n 63,\n 64,\n 65,\n 66,\n 67,\n 68,\n 69,\n 70,\n 71,\n 72,\n 73,\n 74,\n 75,\n 76,\n 77,\n 78,\n 79,\n 80,\n 81,\n 82,\n 83,\n 84,\n 85,\n 86,\n 87,\n 88,\n 89,\n 90,\n 91,\n 92,\n 93,\n 94,\n 95,\n 96,\n 97,\n 98,\n 99]"
                    },
                    "execution_count": 8,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "rdd.sortBy(lambda x : x).collect()"
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [],
            "source": "rdd2 = sc.parallelize([101] + list(range(100)))"
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "5051\n101\n50.00990099009901\n"
                }
            ],
            "source": "sum = rdd2.sum()\nn = rdd2.count()\nprint(sum)\nprint(n)\nmean = sum/float(n)\nprint(mean)"
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "PythonRDD[20] at RDD at PythonRDD.scala:53"
                    },
                    "execution_count": 11,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "rdd2.sortBy(lambda x : x)"
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "pyspark.rdd.RDD"
                    },
                    "execution_count": 12,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "type(rdd2)"
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "PythonRDD[28] at RDD at PythonRDD.scala:53"
                    },
                    "execution_count": 13,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "# but the index is on the right, which we want to change, so we\n# use the map function and exchange the value and key order\n\nrdd2.sortBy(lambda x : x).zipWithIndex()"
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "pyspark.rdd.RDD"
                    },
                    "execution_count": 14,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "type(rdd2)"
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "PythonRDD[36] at RDD at PythonRDD.scala:53"
                    },
                    "execution_count": 15,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "# the following code given in the course lecture did not work\n\n#rdd2.sortBy(lambda x : x).zipWithIndex().map(lambda (value,key) : (key,value) ).collect()\n\n# this code worked instead to switch the order of each tuple:\n\nrdd2 = rdd2.sortBy(lambda x : x).zipWithIndex().map (lambda t: (t[1], t[0]))\nrdd2"
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "101"
                    },
                    "execution_count": 16,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "rdd2.count()"
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "metadata": {},
            "outputs": [],
            "source": "sortedAndIndexed = rdd2.sortBy(lambda x : x).zipWithIndex().map (lambda t: (t[1], t[0]))"
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "odd case\n"
                }
            ],
            "source": "nn = sortedAndIndexed.count()\nif (nn % 2 == 1):\n    print(\"odd case\")\nelse:\n    print(\"even case\")"
        },
        {
            "cell_type": "code",
            "execution_count": 22,
            "metadata": {},
            "outputs": [],
            "source": "rdd3 = sc.parallelize([101] + list(range(100)) +  [102,103])"
        },
        {
            "cell_type": "code",
            "execution_count": 23,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "5256\n103\n51.029126213592235\n"
                }
            ],
            "source": "sum = rdd3.sum()\nn = rdd3.count()\nprint(sum)\nprint(n)\nmean = sum/float(n)\nprint(mean)"
        },
        {
            "cell_type": "code",
            "execution_count": 24,
            "metadata": {},
            "outputs": [
                {
                    "ename": "SyntaxError",
                    "evalue": "invalid syntax (<ipython-input-24-508f200cc74f>, line 5)",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-24-508f200cc74f>\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    print sortedAndIndexed.lookup(index)\u001b[0m\n\u001b[0m                         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
                    ]
                }
            ],
            "source": "# the point of this unnecessarily complicated example is just to show that when you have outliers\n# then the median is a more useful measure of central tendency than the mean, since large\n# outliers have can have little or no influence on it\n\nsortedAndIndexed = rdd2.sortBy(lambda x : x).zipWithIndex().map (lambda t: (t[1], t[0]))\nn = sortedAndIndexed.count()\nif (n % 2 == 1):\n    index = (n-1)/2\n    print sortedAndIndexed.lookup(index)\nelse: \n    index1 = (n/2)-1\n    index2 = n/2\n    value1 = sortedAndIndexed.lookup(index1)[0]\n    value2 = sortedAndIndexed.lookup(index2)[0]\n    print (value1+value2)/2"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": ""
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.7 with Spark",
            "language": "python3",
            "name": "python37"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.7.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}