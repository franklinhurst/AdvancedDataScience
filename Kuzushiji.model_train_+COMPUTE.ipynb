{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "collapsed": true
            },
            "source": "### Model Training\n\nOnce your model is defined, it can be trained. This can happen on a single thread or on a parallel framework like Watson Machine Learning or Apache Spark. In the most simple case Model Definition and Model Training is just a couple of LOCs (lines of code) away. In the case of Watson Machine Learning or Apache Spark models might need to get serialized and transferred to another technology / framework.\n\nPlease specify and justify the technologies used for model definition and training in the ADD. \n\nOnce you think you have achieved a descent model performance save the notebook according to the process model\u2019s naming convention and proceed to the model evaluation task."
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "/opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages/secretstorage/dhcrypto.py:16: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n  from cryptography.utils import int_from_bytes\n/opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages/secretstorage/util.py:25: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n  from cryptography.utils import int_from_bytes\nRequirement already satisfied: tensorflow in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (2.1.0)\nRequirement already satisfied: google-pasta>=0.1.6 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from tensorflow) (0.2.0)\nRequirement already satisfied: protobuf>=3.8.0 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from tensorflow) (3.12.3)\nRequirement already satisfied: tensorflow-estimator<2.2.0,>=2.1.0rc0 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from tensorflow) (2.1.0)\nRequirement already satisfied: numpy<2.0,>=1.16.0 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from tensorflow) (1.18.5)\nRequirement already satisfied: keras-preprocessing>=1.1.0 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from tensorflow) (1.1.0)\nRequirement already satisfied: tensorboard<2.2.0,>=2.1.0 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from tensorflow) (2.1.0)\nRequirement already satisfied: wheel>=0.26; python_version >= \"3\" in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from tensorflow) (0.34.2)\nRequirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from tensorflow) (3.1.0)\nRequirement already satisfied: astor>=0.6.0 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from tensorflow) (0.8.0)\nRequirement already satisfied: wrapt>=1.11.1 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from tensorflow) (1.12.1)\nRequirement already satisfied: keras-applications>=1.0.8 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from tensorflow) (1.0.8)\nRequirement already satisfied: grpcio>=1.8.6 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from tensorflow) (1.27.2)\nCollecting scipy==1.4.1; python_version >= \"3\"\n  Downloading scipy-1.4.1-cp37-cp37m-manylinux1_x86_64.whl (26.1 MB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 26.1 MB 16.1 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: absl-py>=0.7.0 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from tensorflow) (0.9.0)\nRequirement already satisfied: gast==0.2.2 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from tensorflow) (0.2.2)\nRequirement already satisfied: termcolor>=1.1.0 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from tensorflow) (1.1.0)\nRequirement already satisfied: six>=1.12.0 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from tensorflow) (1.15.0)\nRequirement already satisfied: setuptools in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from protobuf>=3.8.0->tensorflow) (47.3.1.post20200622)\nRequirement already satisfied: google-auth<2,>=1.6.3 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (1.22.0)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (3.1.1)\nRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (2.24.0)\nRequirement already satisfied: werkzeug>=0.11.15 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (1.0.1)\nRequirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (0.4.1)\nRequirement already satisfied: h5py in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from keras-applications>=1.0.8->tensorflow) (2.10.0)\nRequirement already satisfied: aiohttp<4.0.0dev,>=3.6.2; python_version >= \"3.6\" in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow) (3.6.2)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow) (0.2.8)\nRequirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow) (4.1.1)\nRequirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.5\" in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow) (4.6)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow) (1.25.9)\nRequirement already satisfied: idna<3,>=2.5 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow) (2.9)\nRequirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow) (3.0.4)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow) (2021.5.30)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow) (1.3.0)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from aiohttp<4.0.0dev,>=3.6.2; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow) (19.3.0)\nRequirement already satisfied: multidict<5.0,>=4.5 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from aiohttp<4.0.0dev,>=3.6.2; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow) (4.7.6)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from aiohttp<4.0.0dev,>=3.6.2; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow) (1.5.1)\nRequirement already satisfied: async-timeout<4.0,>=3.0 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from aiohttp<4.0.0dev,>=3.6.2; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow) (3.0.1)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow) (0.4.8)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow) (3.1.0)\nRequirement already satisfied: typing-extensions>=3.7.4; python_version < \"3.8\" in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from yarl<2.0,>=1.0->aiohttp<4.0.0dev,>=3.6.2; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow) (3.7.4.2)\nInstalling collected packages: scipy\n  Attempting uninstall: scipy\n    Found existing installation: scipy 1.5.0\n    Uninstalling scipy-1.5.0:\n      Successfully uninstalled scipy-1.5.0\nSuccessfully installed scipy-1.4.1\n/opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages/secretstorage/dhcrypto.py:16: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n  from cryptography.utils import int_from_bytes\n/opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages/secretstorage/util.py:25: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n  from cryptography.utils import int_from_bytes\nCollecting python-mnist\n  Downloading python_mnist-0.7-py2.py3-none-any.whl (9.6 kB)\nInstalling collected packages: python-mnist\nSuccessfully installed python-mnist-0.7\n/opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages/secretstorage/dhcrypto.py:16: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n  from cryptography.utils import int_from_bytes\n/opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages/secretstorage/util.py:25: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n  from cryptography.utils import int_from_bytes\nRequirement already satisfied: Pillow in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (7.2.0)\n/opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages/secretstorage/dhcrypto.py:16: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n  from cryptography.utils import int_from_bytes\n/opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages/secretstorage/util.py:25: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n  from cryptography.utils import int_from_bytes\nCollecting pyspark\n  Downloading pyspark-3.1.2.tar.gz (212.4 MB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 212.4 MB 114 kB/s  eta 0:00:01\n\u001b[?25hCollecting py4j==0.10.9\n  Downloading py4j-0.10.9-py2.py3-none-any.whl (198 kB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 198 kB 37.0 MB/s eta 0:00:01\n\u001b[?25hBuilding wheels for collected packages: pyspark\n  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.1.2-py2.py3-none-any.whl size=212880770 sha256=c1fb498fddef3fa4d4733ae78722a239feb28716cd7aac9b17bb07e2ae800359\n  Stored in directory: /tmp/wsuser/.cache/pip/wheels/a5/0a/c1/9561f6fecb759579a7d863dcd846daaa95f598744e71b02c77\nSuccessfully built pyspark\nInstalling collected packages: py4j, pyspark\nSuccessfully installed py4j-0.10.9 pyspark-3.1.2\n"
                }
            ],
            "source": "!pip install tensorflow\n!pip install python-mnist\n!pip install Pillow\n!pip install pyspark"
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "'2.1.0'"
                    },
                    "execution_count": 2,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "%matplotlib inline\nimport matplotlib.pyplot as plt\nfrom matplotlib import image\nimport tensorflow as tf\nimport seaborn as sns\nfrom mnist import MNIST\nimport numpy as np\nimport PIL\nfrom PIL import Image\nimport os\nimport matplotlib.image as mping\nfrom pyspark import SparkContext, SparkConf\nfrom pyspark.sql import SparkSession\nfrom pyspark.ml.feature import StringIndexer\nfrom numpy import asarray\nimport pandas as pd\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout\nfrom tensorflow.keras import backend as K\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.decomposition import PCA\n\ntf.__version__"
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "'7.2.0'"
                    },
                    "execution_count": 3,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "PIL.__version__"
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": "# fire up the spark session\nsc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\"))\n\nspark = SparkSession \\\n    .builder \\\n    .getOrCreate()"
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": "# enable arrow which lets us transfrom a pandas dataframe into a pyspark dataframe\nspark.conf.set(\"spark.sql.execution.arrow.enabled\",\"true\")"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## First dataset\n\n#### 60,000 images and 10 classes, each image is 28 x 28 or represented by a 794 element array"
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "--2021-06-10 16:22:42--  http://codh.rois.ac.jp/kmnist/dataset/kmnist/train-images-idx3-ubyte.gz?raw=True\nResolving codh.rois.ac.jp (codh.rois.ac.jp)... 136.187.88.58\nConnecting to codh.rois.ac.jp (codh.rois.ac.jp)|136.187.88.58|:80... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 18165135 (17M)\nSaving to: \u2018train-images-idx3-ubyte.gz?raw=True\u2019\n\ntrain-images-idx3-u 100%[===================>]  17.32M  6.40MB/s    in 2.7s    \n\n2021-06-10 16:22:45 (6.40 MB/s) - \u2018train-images-idx3-ubyte.gz?raw=True\u2019 saved [18165135/18165135]\n\n-rw-rw---- 1 wsuser watsonstudio 45M Feb  4  2019 train-images-idx3-ubyte\n--2021-06-10 16:22:49--  http://codh.rois.ac.jp/kmnist/dataset/kmnist/train-labels-idx1-ubyte.gz?raw=True\nResolving codh.rois.ac.jp (codh.rois.ac.jp)... 136.187.88.58\nConnecting to codh.rois.ac.jp (codh.rois.ac.jp)|136.187.88.58|:80... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 29497 (29K)\nSaving to: \u2018train-labels-idx1-ubyte.gz?raw=True\u2019\n\ntrain-labels-idx1-u 100%[===================>]  28.81K  --.-KB/s    in 0.1s    \n\n2021-06-10 16:22:50 (205 KB/s) - \u2018train-labels-idx1-ubyte.gz?raw=True\u2019 saved [29497/29497]\n\n-rw-rw---- 1 wsuser watsonstudio 59K Feb  4  2019 train-labels-idx1-ubyte\n--2021-06-10 16:22:53--  http://codh.rois.ac.jp/kmnist/dataset/kmnist/t10k-images-idx3-ubyte.gz?raw=True\nResolving codh.rois.ac.jp (codh.rois.ac.jp)... 136.187.88.58\nConnecting to codh.rois.ac.jp (codh.rois.ac.jp)|136.187.88.58|:80... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 3041136 (2.9M)\nSaving to: \u2018t10k-images-idx3-ubyte.gz?raw=True\u2019\n\nt10k-images-idx3-ub 100%[===================>]   2.90M  2.85MB/s    in 1.0s    \n\n2021-06-10 16:22:55 (2.85 MB/s) - \u2018t10k-images-idx3-ubyte.gz?raw=True\u2019 saved [3041136/3041136]\n\n-rw-rw---- 1 wsuser watsonstudio 7.5M Feb  4  2019 t10k-images-idx3-ubyte\n--2021-06-10 16:22:58--  http://codh.rois.ac.jp/kmnist/dataset/kmnist/t10k-labels-idx1-ubyte.gz?raw=True\nResolving codh.rois.ac.jp (codh.rois.ac.jp)... 136.187.88.58\nConnecting to codh.rois.ac.jp (codh.rois.ac.jp)|136.187.88.58|:80... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 5120 (5.0K)\nSaving to: \u2018t10k-labels-idx1-ubyte.gz?raw=True\u2019\n\nt10k-labels-idx1-ub 100%[===================>]   5.00K  --.-KB/s    in 0s      \n\n2021-06-10 16:22:59 (180 MB/s) - \u2018t10k-labels-idx1-ubyte.gz?raw=True\u2019 saved [5120/5120]\n\n-rw-rw---- 1 wsuser watsonstudio 9.8K Feb  4  2019 t10k-labels-idx1-ubyte\n"
                }
            ],
            "source": "!wget http://codh.rois.ac.jp/kmnist/dataset/kmnist/train-images-idx3-ubyte.gz?raw=True\n!mv train-images-idx3-ubyte.gz?raw=True train-images-idx3-ubyte.gz\n!gunzip train-images-idx3-ubyte.gz\n!ls -lahr train-images-idx3-ubyte\n\n!wget http://codh.rois.ac.jp/kmnist/dataset/kmnist/train-labels-idx1-ubyte.gz?raw=True\n!mv train-labels-idx1-ubyte.gz?raw=True train-labels-idx1-ubyte.gz\n!gunzip train-labels-idx1-ubyte.gz\n!ls -lahr train-labels-idx1-ubyte\n\n!wget http://codh.rois.ac.jp/kmnist/dataset/kmnist/t10k-images-idx3-ubyte.gz?raw=True\n!mv t10k-images-idx3-ubyte.gz?raw=True t10k-images-idx3-ubyte.gz\n!gunzip t10k-images-idx3-ubyte.gz\n!ls -lahr t10k-images-idx3-ubyte\n\n!wget http://codh.rois.ac.jp/kmnist/dataset/kmnist/t10k-labels-idx1-ubyte.gz?raw=True\n!mv t10k-labels-idx1-ubyte.gz?raw=True t10k-labels-idx1-ubyte.gz\n!gunzip t10k-labels-idx1-ubyte.gz\n!ls -lahr t10k-labels-idx1-ubyte"
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "total 53680\r\ndrwxrwx--- 2 wsuser watsonstudio     4096 Jun 10 16:23 .\r\ndrwxr-x--- 4 wsuser watsonstudio     4096 Jun 10 16:23 ..\r\n-rw-rw---- 1 wsuser watsonstudio  7840016 Jun 10 16:23 t10k-images-idx3-ubyte\r\n-rw-rw---- 1 wsuser watsonstudio    10008 Jun 10 16:23 t10k-labels-idx1-ubyte\r\n-rw-rw---- 1 wsuser watsonstudio 47040016 Jun 10 16:23 train-images-idx3-ubyte\r\n-rw-rw---- 1 wsuser watsonstudio    60008 Jun 10 16:23 train-labels-idx1-ubyte\r\n"
                }
            ],
            "source": "!mkdir kmnistdata\n!cp t10k-images-idx3-ubyte kmnistdata/t10k-images-idx3-ubyte\n!cp t10k-labels-idx1-ubyte kmnistdata/t10k-labels-idx1-ubyte\n!cp train-images-idx3-ubyte kmnistdata/train-images-idx3-ubyte\n!cp train-labels-idx1-ubyte kmnistdata/train-labels-idx1-ubyte\n!ls -al kmnistdata\n\ndata = MNIST('kmnistdata')\ntrain_images, train_labels = data.load_training()\ntest_images, test_labels = data.load_testing()\n\ntrain_images = np.array(train_images)\ntrain_labels = np.array(train_labels)\ntest_images = np.array(test_images)\ntest_labels = np.array(test_labels)\n\ntrain_images = train_images / 255\ntest_images = test_images / 255"
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [],
            "source": "# model_1 def\n\nmodel_1 = tf.keras.Sequential([\n    tf.keras.layers.Dense(128, activation=tf.nn.relu),\n    tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n])"
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [],
            "source": "# model_1 compile\nmodel_1.compile(optimizer='adam',\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])"
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Train on 60000 samples\nEpoch 1/5\n60000/60000 [==============================] - 557s 9ms/sample - loss: 0.4007 - accuracy: 0.8808\nEpoch 2/5\n60000/60000 [==============================] - 557s 9ms/sample - loss: 0.1926 - accuracy: 0.9446\nEpoch 3/5\n60000/60000 [==============================] - 554s 9ms/sample - loss: 0.1375 - accuracy: 0.9593\nEpoch 4/5\n60000/60000 [==============================] - 562s 9ms/sample - loss: 0.1021 - accuracy: 0.9694\nEpoch 5/5\n60000/60000 [==============================] - 550s 9ms/sample - loss: 0.0795 - accuracy: 0.9762\n"
                },
                {
                    "data": {
                        "text/plain": "<tensorflow.python.keras.callbacks.History at 0x7f9c592c8350>"
                    },
                    "execution_count": 10,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "# model_1 fit\nmodel_1.fit(train_images, train_labels, epochs=5)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# note: need to rerun the fit about and include the validation data setp as is done in model_2 below"
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [],
            "source": "# model_2 def\n\nbatch_size = 128\nnum_classes = 10\nepochs = 12\n\n# input image dimensions\nimg_rows, img_cols = 28, 28\n\nx_train = train_images\nx_test = test_images\ny_train = train_labels\ny_test = test_labels\n\nif K.image_data_format() == 'channels_first':\n    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n    input_shape = (1, img_rows, img_cols)\nelse:\n    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n    input_shape = (img_rows, img_cols, 1)\n\nmodel_2 = tf.keras.Sequential()\nmodel_2.add(Conv2D(32, kernel_size=(3, 3),\n                 activation='relu',\n                 input_shape=input_shape))\nmodel_2.add(Conv2D(64, (3, 3), activation='relu'))\nmodel_2.add(MaxPooling2D(pool_size=(2, 2)))\nmodel_2.add(Dropout(0.25))\nmodel_2.add(tf.keras.layers.Flatten())\nmodel_2.add(tf.keras.layers.Dense(128, activation='relu'))\nmodel_2.add(Dropout(0.5))\nmodel_2.add(tf.keras.layers.Dense(num_classes, activation='softmax'))\n\n# model_2 compile\n\nmodel_2.compile(optimizer='adam', \n          loss='sparse_categorical_crossentropy',\n          metrics=['accuracy'])"
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Train on 60000 samples, validate on 10000 samples\nEpoch 1/12\n60000/60000 [==============================] - 1600s 27ms/sample - loss: 0.4046 - accuracy: 0.8769 - val_loss: 0.3375 - val_accuracy: 0.9013\nEpoch 2/12\n60000/60000 [==============================] - 1507s 25ms/sample - loss: 0.1685 - accuracy: 0.9482 - val_loss: 0.2603 - val_accuracy: 0.9240\nEpoch 3/12\n60000/60000 [==============================] - 1588s 26ms/sample - loss: 0.1220 - accuracy: 0.9626 - val_loss: 0.2323 - val_accuracy: 0.9336\nEpoch 4/12\n60000/60000 [==============================] - 1586s 26ms/sample - loss: 0.1003 - accuracy: 0.9693 - val_loss: 0.2272 - val_accuracy: 0.9370\nEpoch 5/12\n38272/60000 [==================>...........] - ETA: 9:09 - loss: 0.0803 - accuracy: 0.9741"
                },
                {
                    "ename": "KeyboardInterrupt",
                    "evalue": "",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
                        "\u001b[0;32m<ipython-input-12-e851dfc9ca9f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m          \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m          \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m          validation_data=(x_test, y_test))\n\u001b[0m",
                        "\u001b[0;32m/opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m   def evaluate(self,\n",
                        "\u001b[0;32m/opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    340\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m                 total_epochs=epochs)\n\u001b[0m\u001b[1;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    126\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[1;32m    127\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 98\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    597\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    600\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2361\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2363\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2365\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1609\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1611\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1613\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1690\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1692\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
                        "\u001b[0;32m/opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
                    ]
                }
            ],
            "source": "# fit model\n\nmodel_2.fit(x_train, y_train,\n         batch_size=batch_size,\n         epochs=epochs,\n         verbose=1,\n         validation_data=(x_test, y_test))"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# change of optimizer and compile\nmodel_2.compile(loss='sparse_categorical_crossentropy',\n              optimizer=tf.keras.optimizers.Adadelta(),\n              metrics=['accuracy'])"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "model_2.fit(x_train, y_train,\n           batch_size=batch_size,\n           epochs=epochs,\n           verbose=1,\n           validation_data=(x_test, y_test))"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "#### This is the first dataset in .npz format"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "!wget http://codh.rois.ac.jp/kmnist/dataset/kmnist/kmnist-train-imgs.npz?raw=True\n!mv kmnist-train-imgs.npz?raw=True kmnist-train-imgs.npz\n!ls -lahr kmnist-train-imgs.npz\n\n!wget http://codh.rois.ac.jp/kmnist/dataset/kmnist/kmnist-train-labels.npz?raw=True\n!mv kmnist-train-labels.npz?raw=True kmnist-train-labels.npz\n!ls -lahr kmnist-train-labels.npz\n\n!wget http://codh.rois.ac.jp/kmnist/dataset/kmnist/kmnist-test-imgs.npz?raw=True\n!mv kmnist-test-imgs.npz?raw=True kmnist-test-imgs.npz\n!ls -lahr kmnist-test-imgs.npz\n\n!wget http://codh.rois.ac.jp/kmnist/dataset/kmnist/kmnist-test-labels.npz?raw=True\n!mv kmnist-test-labels.npz?raw=True kmnist-test-labels.npz\n!ls -lahr kmnist-test-labels.npz"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### Model:  K Nearest Neighbors"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "def load(f):\n    return np.load(f)['arr_0']"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "x_train = load('kmnist-train-imgs.npz')\ny_train = load('kmnist-train-labels.npz')\nx_test = load('kmnist-test-imgs.npz')\ny_test = load('kmnist-test-labels.npz')"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "x_train = x_train.reshape(-1, 784)\nx_test = x_test.reshape(-1, 784)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "model_3 = KNeighborsClassifier(n_neighbors=4, weights='distance', n_jobs=-1)\nprint('Fit', model_3)\nmodel_3.fit(x_train, y_train)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "print('Evaluate', model_3)\nscore = model_3.score(x_test, y_test)\nprint('The accuracy is:', score)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### Model: K Nearest Neighbors with Principal Component Analysis dimensionality reduction"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "pca = PCA(n_components=60, random_state=0)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "x_train = pca.fit_transform(x_train)\nx_test = pca.transform"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "model_4 = KNeighborsClassifier(n_neighbors=4, weights='distance', n_jobs=-1)\nprint('Fit', model_4)\nmodel_4.fit(x_train, y_train)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "print('Evaluate', model_4)\nscore = model_4.score(x_test, y_test)\nprint('The accuracy is:', score)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Second dataset\n\n#### 232,365 images, 49 classes, each image is 28 x 28 or represented by a 794 element array"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# download the train and test labels and images, \n# which are available onlyh in .npz format \n\n!wget http://codh.rois.ac.jp/kmnist/dataset/k49/k49-train-imgs.npz?raw=True\n!mv k49-train-imgs.npz?raw=True k49-train-imgs.npz\n!ls -lahr k49-train-imgs.npz\n\n!wget http://codh.rois.ac.jp/kmnist/dataset/k49/k49-train-labels.npz?raw=True\n!mv k49-train-labels.npz?raw=True k49-train-labels.npz\n!ls -lahr k49-train-labels.npz\n\n!wget http://codh.rois.ac.jp/kmnist/dataset/k49/k49-test-imgs.npz?raw=True\n!mv k49-test-imgs.npz?raw=True k49-test-imgs.npz\n!ls -lahr k49-test-imgs.npz\n\n!wget http://codh.rois.ac.jp/kmnist/dataset/k49/k49-test-labels.npz?raw=True\n!mv k49-test-labels.npz?raw=True k49-test-labels.npz\n!ls -lahr k49-test-labels.npz"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "def load(f):\n    return np.load(f)['arr_0']"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "x_train_k49 = load('k49-train-imgs.npz')\ny_train_k49 = load('k49-train-labels.npz')\nx_test_k49 = load('k49-test-imgs.npz')\ny_test_k49 = load('k49-test-labels.npz')\n\nx_train_k49 = x_train_k49.reshape(-1, 784)\nx_test_k49 = x_test_k49.reshape(-1, 784)\n\n\n# the numpy arrays used in the keras models need to be between 0 and 1\n# we keep x_train_k49 and x_test_k49 in the format of between 0 and 255\n# for use in the K-nearest Neighbors model below\n\ntrain_images_k49 = x_train_k49 / 255\ntest_images_k49 = x_test_k49 / 255\n\n# and the labels can be the same\n\ntrain_labels_k49 = y_train_k49\ntest_labels_k49 = y_test_k49"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# model_1_k49 def\n# we have to change the last layer due to the 49 categories\n\nmodel_1_k49 = tf.keras.Sequential([\n    tf.keras.layers.Dense(128, activation=tf.nn.relu),\n    tf.keras.layers.Dense(49, activation=tf.nn.softmax)\n])"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# model_1_k49 compile\nmodel_1_k49.compile(optimizer='adam',\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# model_1_k49 fit\nmodel_1_k49.fit(train_images_k49, train_labels_k49, epochs=5)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# model_2_k49 def\n\nbatch_size = 128\nnum_classes = 49       #  here we specify the number of classes as 49 instead of 10\nepochs = 12\n\n# input image dimensions\nimg_rows, img_cols = 28, 28\n\nx_train = train_images_k49\nx_test = test_images_k49\ny_train = train_labels_k49\ny_test = test_labels_k49\n\nif K.image_data_format() == 'channels_first':\n    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n    input_shape = (1, img_rows, img_cols)\nelse:\n    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n    input_shape = (img_rows, img_cols, 1)\n\nmodel_2_k49 = tf.keras.Sequential()\nmodel_2_k49.add(Conv2D(32, kernel_size=(3, 3),\n                 activation='relu',\n                 input_shape=input_shape))\nmodel_2_k49.add(Conv2D(64, (3, 3), activation='relu'))\nmodel_2_k49.add(MaxPooling2D(pool_size=(2, 2)))\nmodel_2_k49.add(Dropout(0.25))\nmodel_2_k49.add(tf.keras.layers.Flatten())\nmodel_2_k49.add(tf.keras.layers.Dense(128, activation='relu'))\nmodel_2_k49.add(Dropout(0.5))\nmodel_2_k49.add(tf.keras.layers.Dense(num_classes, activation='softmax'))\n\n# model_2 compile\n\nmodel_2_k49.compile(optimizer='adam', \n          loss='sparse_categorical_crossentropy',\n          metrics=['accuracy'])"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# fit model2_k49\n\nmodel_2_k49.fit(x_train, y_train,\n         batch_size=batch_size,\n         epochs=epochs,\n         verbose=1,\n         validation_data=(x_test, y_test))"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# change of optimizer and compile model_2_k49\n\nmodel_2_k49.compile(loss='sparse_categorical_crossentropy',\n              optimizer=tf.keras.optimizers.Adadelta(),\n              metrics=['accuracy'])"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# fit model_2_k49 after change of optmizer\n\nmodel_2_k49.fit(x_train, y_train,\n           batch_size=batch_size,\n           epochs=epochs,\n           verbose=1,\n           validation_data=(x_test, y_test))"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### Model:  K Nearest Neighbors - k49"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# recall that the two numpy image arrays used below, namely x_train_k49 and x_test_k49\n# are arrays of integers, which works fine for KNN"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "model_3_k49 = KNeighborsClassifier(n_neighbors=4, weights='distance', n_jobs=-1)\nprint('Fit', model_3_k49)\nmodel_3_k49.fit(x_train_k49, y_train_k49)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "print('Evaluate', model_3_k49)\nscore = model_3_k49.score(x_test_k49, y_test_k49)\nprint('The accuracy is:', score)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": ""
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.7",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.7.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}