{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "###\u00a0Welcome to the Exercise on TensorFlow 2.x. \n\nFirst we make us familiar with linear algebra operations. Then we implement a linear regression model. After that we implement a neural network using the low level TensorFlow API. Finally, we conclude with a neural network implemented in Keras.\n\nPlease replace all ###your_code_here### sections with the correct code.\n\nNow let's make sure we are on the latest version of TensorFlow"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "!pip install --upgrade tensorflow"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "import tensorflow as tf\ntf.__version__"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Let's play around with some linear algebra examples"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "a = tf.constant([1., 2., 2., 3.])\nprint(a)\n\n\nb = tf.constant([4., 5., 5., 6.])\nprint(b)\n\n\nc =  ###your_code_here### => Please add element-wise addition, you should get tf.Tensor([5. 7. 7. 9.]\nprint(c)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "a = tf.constant([1., 2., 2., 3.])\nprint(a)\n\n\nb = tf.constant([4., 5., 5., 6.])\nprint(b)\n\n\nc = ###your_code_here### => Please compute the dot product, you should get 42\nprint(c)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### Linear Regression"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "import tensorflow as tf\ntf.__version__\n\nimport numpy as np\ndata = np.array(\n    [\n        [100,35,35,12,0.32],\n        [101,46,35,21,0.34],\n        [130,56,46,3412,12.42],\n        [131,58,48,3542,13.43]\n    ]\n)\n\nx = data[:,1:-1]\ny_target = data[:,-1]\n\nb = tf.Variable(1,dtype=tf.float64)\nw = tf.Variable([1,1,1],dtype=tf.float64)\n\n\ndef linear_model(x):\n    return ###your_code_here### => Please express the linear model using the TensorFlow low level API\n\noptimizer = tf.keras.optimizers.Adam()\nloss_object = tf.keras.losses.MeanSquaredLogarithmicError()\n\ndef train_step(x, y):\n    with tf.GradientTape() as tape:\n        predicted = linear_model(x)   \n        loss_value = loss_object(y, predicted)\n        print ('Loss {} '.format(loss_value))\n        grads = tape.gradient(loss_value, [b,w])\n        optimizer.apply_gradients(zip(grads, [b,w]))\n    \ndef train(epochs):\n    for epoch in range(epochs):\n            train_step(x, y_target)\n    print ('Epoch {} finished'.format(epoch))\n    \ntrain(epochs = 1000)\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "b"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "w"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "linear_model(x)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "import tensorflow as tf\ntf.__version__\n\nimport numpy as np\n\n\ndata = np.array(\n    [\n        [100,35,35,12,0.],\n        [101,46,35,21,0.],\n        [130,56,46,3412,1.],\n        [131,58,48,3542,1.]\n    ]\n)\n\n\n\nx = data[:,1:-1]\ny_target = data[:,-1]\n\n\n\nx = x / np.linalg.norm(x)\n\nb = tf.Variable(1,dtype=tf.float64)\nw = tf.Variable([1,1,1],dtype=tf.float64)\n\n\ndef logstic_model(x):\n    return ###your_code_here### => Please express the logistic regression model using the TensorFlow low level API\n\noptimizer = tf.keras.optimizers.SGD(learning_rate=10)\nloss_object = tf.keras.losses.BinaryCrossentropy()\n\ndef train_step(x, y):\n    with tf.GradientTape() as tape:\n        predicted = logstic_model(x) \n        loss_value = loss_object(y, predicted)\n        print(loss_value)\n        grads = tape.gradient(loss_value, [b,w])\n        optimizer.apply_gradients(zip(grads, [b,w]))\n    \ndef train(epochs):\n    for epoch in range(epochs):\n            train_step(x, y_target)\n    \ntrain(epochs = 1000)\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "logstic_model(x)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Neural Network"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "import tensorflow as tf\ntf.__version__\n\nimport numpy as np\ndata = np.array(\n    [\n        [100,35,35,12,0.32],\n        [101,46,35,21,0.34],\n        [130,56,46,3412,12.42],\n        [131,58,48,3542,13.43]\n    ]\n)\n\nx = data[:,1:-1]\nx = x / np.linalg.norm(x)\ny_target = data[:,-1]\ny_target = y_target / np.linalg.norm(y_target)\n\n\nw1 = tf.Variable([[1,1,1],[1,1,1],[1,1,1]],dtype=tf.float64)\nw2 = tf.Variable([1,1,1],dtype=tf.float64)\n\ndef layer1(x):\n    return ###your_code_here### => Please express fully connected layer1 with sigmoid activation using the TensorFlow low level API\n\nprint(layer1(x))\n\ndef layer2(x):\n    return ###your_code_here### => Please express fully connected layer2 with sigmoid activation using the TensorFlow low level API\n\n\n\noptimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\nloss_object = tf.keras.losses.MeanSquaredLogarithmicError()\n\ndef train_step(x, y):\n    with tf.GradientTape() as tape:\n        predicted = layer2(x)   \n        loss_value = loss_object(y, predicted)\n        print(loss_value)\n        grads = tape.gradient(loss_value, [w1,w2])\n        optimizer.apply_gradients(zip(grads, [w1,w2]))\n    \ndef train(epochs):\n    for epoch in range(epochs):\n            train_step(x, y_target)\n    print ('Epoch {} finished'.format(epoch))\n    \ntrain(epochs = 1000)\n\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "w1 "
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "layer1(x)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "x"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "###\u00a0Keras"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "import numpy as np\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.optimizers import SGD\nfrom tensorflow.keras.utils import to_categorical\n\ndata = np.array(\n    [\n        [100,35,35,12,0],\n        [101,46,35,21,0],\n        [130,56,46,3412,1],\n        [131,58,48,3542,1]\n    ]\n)\n\nx = data[:,1:-1]\ny_target = data[:,-1]\n\nx = x / np.linalg.norm(x)\n\nmodel = Sequential()\n###your_code_here### => Please express two fully connected layes with sigmoid activation using the Keras high level API\n\nmodel.compile(optimizer=SGD(learning_rate=0.1),\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\n\nmodel.fit(x, y_target, epochs=1000,\n          verbose=1)\n\n\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "model.predict(x)"
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.7",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.7.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}