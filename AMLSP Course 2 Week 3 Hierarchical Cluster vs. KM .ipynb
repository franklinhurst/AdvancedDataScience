{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "collapsed": true
            },
            "source": "### K-Means Clustering vs. Hierarchical Clustering\n\n#### K-Means clustering \n\nK-Means is the Hello World clustering algorithm.  We can use K-Means in any number of dimensions.  The K-Means algorithm has to be initialized with a number of clusters, meaning the analyst has to specify a number of clusters before the algorithm begins.  It is often not clear how many clusters should be chosen, particularly if the data analyst does not know a lot about the data and/or is not a subject matter expert.  So this is one of the challenges of K-Means, knowing how many different clusters we need to look for.  Keep in mind that one of the benfits of unsupervised machine learning is that the algorithm itself is pointing out to use the useful attributes of the data.  But with K-Means clustering, the number of clusters is not determined by the algorithm since it is a requirement for algorithm initalization.  Once the number of clusters is defined, the locations of the centers of the clusters are called centroids and the centroids are randomly assigned within the space.  This space can be any number of dimensions, from one dimension to hyperspace.  The number of centroids chosen is k centroids.  The algorithm then calculates the sum of the squred distances, using the appropriate distance formula for the appropriate number of dimensions, between every point in the datasets and the k different centroids.  The algorithm then assigns each data point to exactly one centroid such that the distance between each data point and its centroid is minimized.  Now at this point in the algorithm we have k clusters, but the centroids are not necessarily at the center of each cluster.  The algorithm now determines new centroids based on the average values of each cluster.  Next the algorithm goes again to the step of measuring the distances between every data point and the centroids and assigns the datapoints to each centroid to minimize the sum of the (squared) distances between each point and its centroid.  Next the new centroids are again calculated.  The algoritm proceeds in this mannter until the centroids meet a standard of not moving very much so that they are essentially stationary as the algoritm proceeds. Essentially this means that when we reach the appropriate set of centroids we are optimizing a distance measure, since as the algorithm proceeds, the new centroids allow a smaller sum of squared distances than the earlier centroids did.  Using the squred distances in the algorithm instead of just the distances has the effect of increasing the \"penality\" for long distanccs and increasing the \"bonus\" for short distances within the optimization.  Inherent in this algoritm is that the clusters will tend to be essentially spherical around the final centroids.  We can think of this as spheres in Euclidean spaces or we can think of them as hyperspace spheres for sitautions with dimensions greater than three.  So you can already see two of the limitations of K-Means algorithm:  1.)  The data analyst must choose the number of clusters (by chosing the number of intiial centroids) beforehand and 2.) The clusters will be roughly spherical.  You might think that spherical clusters make sense but there are situations where we do not want to restrict our clusters to be necessarily spherical.  Suppose we are trying to use clustering to determine the number of globs in a biological application or one involving physics.  Imagine a nebula or cancer application where the globs we want to identify resember in the side of a lava lamp.  Some of hte clusters will be spherical and some will be shaped abnormally.  In a situation like this we do not want to restrict the clusters to be sphereical and/or based purely on the distance between molecules.  Therefore we will want a slightly more sophisticated algorithm than K-means to determine our clusters.  Of course, when spherical clustering is desired and/or the number of expected clusters is predetermined based on subject matter knowledge, the K-Means can work fine.  Or if we have an idea of the nubmer of clusters expected:  for example, we know that appropriate nubmer of clusters will be between 3 and 6, then we can just run K-Means 4 times, letting k = 3, 4, 5, and 6, in turn.  This is one solution in certain appropriate situations."
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "#### Hierarchical clustering\n\nHierarchical clustering has two basic approaches:  bottom up and top down.\n\nIn bottom up hierarchical clustering, each point in the dataset is assigned to its own cluster, so that we begin with as many clusters as points in the dataset.  We measure the distance between each point and every other point.  We then group the points in to groups of two such that we minimize the distances between points within a cluster.  Next we compare the distances between the clusters and form clusters made up of two two-point clusters in such a way that minimizes the distances between the two-point clusters within the same cluster.  Now we have clusters with 4 points.  We continue this part of the algorithm until we end up with one cluster.  Now from this point, we can analyze the different number of clusters explored in the algorithm and we can determine what number of clusters divides the dataset the most appropriately given the data science question that we are attempting to answer or the problem to be solved.  Again, like K-Means, the question will be guided based on the context and the subject matter knowledge that we have.  But unlike K-Means, we are not required to specify the number of clusters before we begin.\n\nWith top down hierarchical clustering, we start with all the points in the same cluster and then divide the points into two clusters, based on a measure that minimizes distance.  We continue until we get to the point where each point forms its own cluster. \n\nNo matter whether we begin  with top down or bottom up hierarchical clustering, we will end up with a hierarchy that shows the point relationships at each level of the hierarchy, and of course the point in the hierarchy that we choose will determine the number of clusters that the points are divided into.  So hierarchical clustering has two advanages over K-Means:  it can learn nonspherical boundaries and it does not require the analyst to supply the number of clusters in advance and in fact it can provide guidance after the analysis is done as to what the most appropriate number of clusters might be, based on the data."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": ""
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.7",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.7.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}